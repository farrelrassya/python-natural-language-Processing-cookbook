{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farrelrassya/python-natural-language-Processing-cookbook/blob/main/chapter%2008%20-%20Transformers%20%20/%20Chapter_08_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a753e455",
      "metadata": {
        "id": "a753e455"
      },
      "source": [
        "# Chapter 8 — Transformers and Their Applications\n",
        "\n",
        "The **transformer architecture** has fundamentally reshaped NLP. Unlike the feature-engineering pipelines of earlier chapters (bag-of-words, TF-IDF, even static word embeddings), transformers learn contextual representations end-to-end, enabling a single pre-trained model to be adapted to dozens of downstream tasks.\n",
        "\n",
        "This chapter walks through the practical workflow of using transformers via the Hugging Face ecosystem:\n",
        "\n",
        "| # | Recipe | Key Concept |\n",
        "|---|--------|-------------|\n",
        "| 1 | **Loading a Dataset** | Hugging Face `datasets` library |\n",
        "| 2 | **Tokenizing Text** | Sub-word tokenization, input IDs, attention masks |\n",
        "| 3 | **Classification** | Sentiment analysis with a fine-tuned RoBERTa model |\n",
        "| 4 | **Zero-Shot Classification** | Classify without task-specific training data (BART-MNLI) |\n",
        "| 5 | **Text Generation** | Autoregressive decoding with GPT-2 |\n",
        "| 6 | **Language Translation** | Encoder-decoder generation with Google T5 |\n",
        "\n",
        "We move from the **encoder** side of the transformer (recipes 1--4) to the **decoder** side (recipe 5) and finally to the full **encoder-decoder** architecture (recipe 6)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f2ca59e",
      "metadata": {
        "id": "1f2ca59e"
      },
      "source": [
        "## 0 — Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7653bc35",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7653bc35",
        "outputId": "4888f0ff-057e-491e-9b71-c3dc20d7f76a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "\n",
        "# 0.1  Install packages\n",
        "\n",
        "import os\n",
        "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"]       = \"false\"\n",
        "\n",
        "!pip install -q \\\n",
        "    datasets \\\n",
        "    evaluate \\\n",
        "    transformers \\\n",
        "    accelerate \\\n",
        "    sentencepiece \\\n",
        "    protobuf \\\n",
        "    torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1d8e4b4d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d8e4b4d",
        "outputId": "e0ab376b-dd77-4b2b-bf84-2dcc88bae7c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compute device: cuda\n",
            "  GPU: Tesla T4\n",
            "  Memory: 15.6 GB\n",
            "Setup complete.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 0.2  Core imports & configuration\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# Patch jupyter_client to silence datetime.utcnow() spam\n",
        "from datetime import datetime, timezone\n",
        "try:\n",
        "    import jupyter_client.session as _jcs\n",
        "    _jcs.utcnow = lambda: datetime.now(timezone.utc)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Compute device: {device}\")\n",
        "if device.type == \"cuda\":\n",
        "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"  (Models will run on CPU — inference will be slower)\")\n",
        "\n",
        "print(\"Setup complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a06af6b",
      "metadata": {
        "id": "9a06af6b"
      },
      "source": [
        "We set `HF_HUB_DISABLE_PROGRESS_BARS=1` to prevent Hugging Face progress bars from polluting the notebook metadata (a known issue with GitHub rendering). The `jupyter_client.session` patch silences the `datetime.utcnow()` deprecation warnings that flood Colab output when transformer models run inference.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69f8ac7d",
      "metadata": {
        "id": "69f8ac7d"
      },
      "source": [
        "## Recipe 1 — Loading a Dataset\n",
        "\n",
        "The Hugging Face `datasets` library provides a unified API to thousands of public datasets. It handles downloading, caching, memory-mapping (so large datasets do not need to fit in RAM), and standardized train/validation/test splits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f1a50573",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425,
          "referenced_widgets": [
            "55204c0bf1584163984eda7a97aa2701",
            "aa07764a4a424736a2705957130cd09b",
            "89b682ec2fe949779f2dd57d2f22fba9",
            "1d2e94d7d7ea4d52937814e8154992d3",
            "6523473472794bcaa77da4caeff8fff6",
            "23d750b0274d43ab9e3f73b918e68001",
            "2ab552b9b79e4df4bfabc948adc9ce2a",
            "f3e4a59339354e86a9b704e086f11d54",
            "8176e7eed915429ba8b02dd788924142",
            "a86441c4fee34916b31e637ba37e4243",
            "ee5a414395fe4a46be99139ad5b84625",
            "210ad4c8d4c04df9aab9f3e2a3442ef0",
            "6595a92182ad4fbd838593ab68cebf93",
            "af91d99d18cd498383c8c76b57be37ef",
            "f9c08280a84c4e4e91182d94e72c3d55",
            "d1116a98bada4434bdd977404fa23f18",
            "e879246a70d24e538c5b9de04d12cdb0",
            "73e4324ddc1e4bbdb273f54e5db5c459",
            "0f272b987d6e40648efd49f19b2e06d7",
            "da2633fa9b314748b5e8443261b06439",
            "638514996d094ff1a1102e4378d56c8d",
            "ef0bf5ebdf7f4331bd37fca4927708b1",
            "cae1b7e14d2b48efa7cda968a8f96c90",
            "ce1659b9d8cd49d98fa7cbaff21f310e",
            "b8b44eaf83fe48b284b131769684bc19",
            "a9330d57a4eb4be78019260913dc507a",
            "14700cfc4e8b42ae980c87f3f6556836",
            "bb37a574fc72476890a6fb729c950318",
            "b34b0156d3a44fe891fdd9957ba1311c",
            "00e46277e573450399b50bb5952abe06",
            "4b33dbd9d6354d73a189913abee74b54",
            "50b5afc41cc448d9a95f3b0326e0ffe1",
            "1a35811a070445488d039e943a39284c"
          ]
        },
        "id": "f1a50573",
        "outputId": "f50e0365-d5e0-42e5-adbf-4a951ecd5ea8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/8530 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "55204c0bf1584163984eda7a97aa2701"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/1066 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "210ad4c8d4c04df9aab9f3e2a3442ef0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/1066 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cae1b7e14d2b48efa7cda968a8f96c90"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available splits: ['train', 'validation', 'test']\n",
            "\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 8530\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 1066\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 1066\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 1.1  Load the Rotten Tomatoes dataset\n",
        "\n",
        "from datasets import load_dataset, get_dataset_split_names\n",
        "\n",
        "dataset = load_dataset(\"rotten_tomatoes\")\n",
        "\n",
        "print(\"Available splits:\", get_dataset_split_names(\"rotten_tomatoes\"))\n",
        "print()\n",
        "print(dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c21137b",
      "metadata": {
        "id": "4c21137b"
      },
      "source": [
        "The Rotten Tomatoes dataset contains $5{,}331$ positive and $5{,}331$ negative movie review sentences, split into train, validation, and test partitions. It is a **binary sentiment classification** benchmark -- simple enough to demonstrate transformer workflows without lengthy training times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "162c8f56",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "162c8f56",
        "outputId": "9adbf79e-9594-4c41-aadf-a1afc0babfd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Description:\n",
            " ...\n",
            "\n",
            "Features: {'text': Value('string'), 'label': ClassLabel(names=['neg', 'pos'])}\n",
            "Number of examples: 8,530\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 1.2  Inspect the training split\n",
        "\n",
        "training_data = dataset[\"train\"]\n",
        "\n",
        "print(\"Description:\")\n",
        "print(training_data.description[:200], \"...\")\n",
        "print()\n",
        "print(\"Features:\", training_data.features)\n",
        "print(f\"Number of examples: {len(training_data):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "51c18637",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51c18637",
        "outputId": "f228a51a-e028-42b5-e1ad-bc47bb1af631"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0] the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\n",
            "[1] the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .\n",
            "[2] effective but too-tepid biopic\n",
            "[3] if you sometimes like to go to the movies to have fun , wasabi is a good place to start .\n",
            "[4] emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one .\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 1.3  Sample first 5 sentences\n",
        "\n",
        "sentences = training_data[\"text\"][:5]\n",
        "\n",
        "for i, sentence in enumerate(sentences):\n",
        "    print(f\"[{i}] {sentence}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a727ff6",
      "metadata": {
        "id": "3a727ff6"
      },
      "source": [
        "Each example has two fields: `text` (the review string) and `label` (0 for negative, 1 for positive). The reviews are single sentences -- short and punchy, typical of critics' one-liners. This brevity makes the dataset ideal for testing whether a model can capture sentiment from limited context.\n",
        "\n",
        "**Production note:** In real projects you rarely work with clean, pre-split datasets. The `datasets` library also supports loading from CSV, JSON, Parquet, and SQL -- use `load_dataset(\"csv\", data_files=\"path/to/file.csv\")`.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59c62691",
      "metadata": {
        "id": "59c62691"
      },
      "source": [
        "## Recipe 2 — Tokenizing Text\n",
        "\n",
        "Transformers do not see words -- they see **token IDs**. The tokenizer converts raw text into a sequence of integer IDs that index into the model's vocabulary. BERT-family models use **WordPiece** tokenization: common words get a single token, while rare words are split into sub-word units.\n",
        "\n",
        "The tokenizer also adds special tokens:\n",
        "\n",
        "$$\\text{[CLS]} \\;\\; w_1 \\;\\; w_2 \\;\\; \\cdots \\;\\; w_n \\;\\; \\text{[SEP]}$$\n",
        "\n",
        "where `[CLS]` (classification) marks the start and `[SEP]` (separator) marks the end. For BERT, the `[CLS]` token's representation is typically used as the sentence embedding for classification tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "09ef41f1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09ef41f1",
        "outputId": "9a6fb5d7-caad-43cc-c012-5276dc7ec86a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys: ['input_ids', 'token_type_ids', 'attention_mask']\n",
            "\n",
            "Sentence 0: \"The first sentence, which is the longest one in the list.\"\n",
            "  Token IDs (15 tokens): [101, 1109, 1148, 5650, 117, 1134, 1110, 1103, 6119, 1141, 1107, 1103, 2190, 119, 102]\n",
            "  Attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "\n",
            "Sentence 1: \"The second sentence is not that long.\"\n",
            "  Token IDs (10 tokens): [101, 1109, 1248, 5650, 1110, 1136, 1115, 1263, 119, 102]\n",
            "  Attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "\n",
            "Sentence 2: \"A very short sentence.\"\n",
            "  Token IDs (7 tokens): [101, 138, 1304, 1603, 5650, 119, 102]\n",
            "  Attention mask: [1, 1, 1, 1, 1, 1, 1]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 2.1  Initialize a BERT tokenizer\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "sentences = [\n",
        "    \"The first sentence, which is the longest one in the list.\",\n",
        "    \"The second sentence is not that long.\",\n",
        "    \"A very short sentence.\"\n",
        "]\n",
        "\n",
        "tokenized_input = tokenizer(sentences)\n",
        "\n",
        "print(\"Keys:\", list(tokenized_input.keys()))\n",
        "print()\n",
        "for i, sent in enumerate(sentences):\n",
        "    ids = tokenized_input[\"input_ids\"][i]\n",
        "    print(f'Sentence {i}: \"{sent}\"')\n",
        "    print(f\"  Token IDs ({len(ids)} tokens): {ids}\")\n",
        "    print(f\"  Attention mask: {tokenized_input['attention_mask'][i]}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71e1d08a",
      "metadata": {
        "id": "71e1d08a"
      },
      "source": [
        "The tokenizer returns three parallel lists:\n",
        "\n",
        "**`input_ids`** — Integer IDs mapping each token to the model's vocabulary. Token 101 is `[CLS]` and 102 is `[SEP]`.\n",
        "\n",
        "**`token_type_ids`** — All zeros for single-sentence input. For sentence-pair tasks (e.g., natural language inference), the second sentence gets type ID 1.\n",
        "\n",
        "**`attention_mask`** — Binary mask indicating real tokens (1) vs. padding (0). Since we have not padded yet, all values are 1.\n",
        "\n",
        "Notice the sentences have **different lengths** (15, 10, and 7 tokens). When batching, we would need to pad the shorter sentences to match the longest one -- the attention mask then tells the model to ignore the padding positions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "376078e9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "376078e9",
        "outputId": "c0a70334-dbb3-40a9-d935-40707dfe918b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs: [101, 1109, 1148, 5650, 117, 1134, 1110, 1103, 6119, 1141, 1107, 1103, 2190, 119, 102]\n",
            "Tokens:    ['[CLS]', 'The', 'first', 'sentence', ',', 'which', 'is', 'the', 'longest', 'one', 'in', 'the', 'list', '.', '[SEP]']\n",
            "\n",
            "Vocabulary size: 28,996 tokens\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 2.2  Convert IDs back to tokens\n",
        "\n",
        "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"][0])\n",
        "\n",
        "print(\"Token IDs:\", tokenized_input[\"input_ids\"][0])\n",
        "print(\"Tokens:   \", tokens)\n",
        "print()\n",
        "print(f\"Vocabulary size: {tokenizer.vocab_size:,} tokens\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0c2201b",
      "metadata": {
        "id": "d0c2201b"
      },
      "source": [
        "Converting IDs back to tokens confirms the round-trip: every word maps to its original form, with `[CLS]` and `[SEP]` bookending the sequence. The `bert-base-cased` vocabulary contains approximately $28{,}996$ tokens -- a mix of whole words and sub-word pieces (prefixed with `##`).\n",
        "\n",
        "**Sub-word tokenization** is the key insight that makes transformers handle open vocabularies: even words never seen during pre-training can be represented as a sequence of known sub-word units. For example, \"unbelievable\" might become `[\"un\", \"##bel\", \"##ie\", \"##va\", \"##ble\"]`.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2a521fc",
      "metadata": {
        "id": "e2a521fc"
      },
      "source": [
        "## Recipe 3 — Classifying Text with Transformers\n",
        "\n",
        "The Hugging Face `pipeline` abstraction wraps tokenization, model inference, and post-processing into a single callable. For sentiment analysis, we use a RoBERTa model fine-tuned on the Rotten Tomatoes dataset itself.\n",
        "\n",
        "The architecture is:\n",
        "\n",
        "$$\\text{Input} \\xrightarrow{\\text{Tokenizer}} \\text{Token IDs} \\xrightarrow{\\text{RoBERTa Encoder}} \\mathbf{h}_{\\text{[CLS]}} \\xrightarrow{\\text{Linear + Softmax}} P(\\text{pos}), P(\\text{neg})$$\n",
        "\n",
        "The encoder produces a contextual representation $\\mathbf{h}_{\\text{[CLS]}} \\in \\mathbb{R}^{768}$ for the `[CLS]` token, which a linear classification head maps to class probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8acd987f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8acd987f",
        "outputId": "69b3ff62-0280-49b4-936a-41a4fbd97b2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "RobertaForSequenceClassification LOAD REPORT from: textattack/roberta-base-rotten-tomatoes\n",
            "Key                         | Status     |  | \n",
            "----------------------------+------------+--+-\n",
            "roberta.pooler.dense.weight | UNEXPECTED |  | \n",
            "roberta.pooler.dense.bias   | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: textattack/roberta-base-rotten-tomatoes\n",
            "Running on: cuda\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 3.1  Initialize sentiment analysis pipeline\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "roberta_pipe = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=\"textattack/roberta-base-rotten-tomatoes\",\n",
        "    device=device)\n",
        "\n",
        "print(f\"Model: textattack/roberta-base-rotten-tomatoes\")\n",
        "print(f\"Running on: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "63570ec9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "e30bd40f516a48fca43a21598a1f16b2",
            "5a0a0e3099964e32aa3f91ce558e8a46",
            "96733a179a614f03b6a7c866981e8c4a",
            "3011f5c61f09411dad63862270418c5b",
            "726d405fbc4640d28a29b3a1b0fb0deb",
            "618782ccd1154101b7b1b0445ec14cac",
            "de6ea12a6c6f45a591db095b0849560f",
            "9c9c8cc718d6406588378ac57ed26c54",
            "d24a8caa6cdc4853975251525fe65cad",
            "7ad134a9ba1a4e37ab10b03168dfd572",
            "b6ab609163c24537aecb7a0e723880da"
          ]
        },
        "id": "63570ec9",
        "outputId": "4632cf0e-1d70-47d7-9924-679b14383e3a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e30bd40f516a48fca43a21598a1f16b2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Y] actual=1  pred=1  conf=0.965  \"lovingly photographed in the manner of a golden book sprung to life , ...\"\n",
            "[Y] actual=1  pred=1  conf=0.996  \"consistently clever and suspenseful ....\"\n",
            "[X] actual=1  pred=0  conf=0.916  \"it's like a \" big chill \" reunion of the baader-meinhof gang , only th...\"\n",
            "[Y] actual=1  pred=1  conf=0.996  \"the story gives ample opportunity for large-scale action and suspense ...\"\n",
            "[Y] actual=1  pred=1  conf=0.879  \"red dragon \" never cuts corners ....\"\n"
          ]
        }
      ],
      "source": [
        "# 3.2  Predict on a small sample\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers.pipelines.pt_utils import KeyDataset\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Load dataset\n",
        "sample = load_dataset(\"rotten_tomatoes\", split=\"test\").select(range(5))\n",
        "\n",
        "# KeyDataset otomatis mengekstrak kolom \"text\" sebagai generator string murni\n",
        "predictions = []\n",
        "for out in tqdm(roberta_pipe(KeyDataset(sample, \"text\"), batch_size=8)):\n",
        "    predictions.append(out)\n",
        "\n",
        "# Print hasilnya\n",
        "for idx, text in enumerate(sample[\"text\"]):\n",
        "    actual = sample[\"label\"][idx]\n",
        "    pred_label = 1 if predictions[idx][\"label\"] == \"LABEL_1\" else 0\n",
        "    score = predictions[idx][\"score\"]\n",
        "    match = \"Y\" if actual == pred_label else \"X\"\n",
        "\n",
        "    print(f'[{match}] actual={actual}  pred={pred_label}  conf={score:.3f}  \"{text[:70]}...\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3017ad68",
      "metadata": {
        "id": "3017ad68"
      },
      "source": [
        "The pipeline correctly classifies most of the sample. Each prediction comes with a confidence score — the softmax probability of the chosen label. A score near 1.0 indicates high confidence; scores around 0.5 suggest the model is uncertain.\n",
        "\n",
        "Notice that even for the misclassified example(s), the confidence is lower — the model \"knows\" it is less sure. This calibration property is valuable in production: you can set a **confidence threshold** below which predictions are routed to human review."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "a02f9ddf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188,
          "referenced_widgets": [
            "5078a8d507914fd98fdc63432da9e178",
            "5a1a7c0757964a49b65e753bb152dee4",
            "d62f79fd60a745a4b0f49ec28bc1a89c",
            "24cada3780f345628ce4aea6f11f6e53",
            "f89dc13545564ba4ac99a643ca3acd67",
            "bd578bd608284f8ea09ccac15c301e23",
            "c545bfafc348453dbf530491652e2430",
            "1939de511f7f4de9ab01f339895cceb3",
            "3f0baa87dee84fd5942f6e493451ca47",
            "e74c35f3a6574756b083d395554952b1",
            "45095e915e0d4daab1a48085d82be832"
          ]
        },
        "id": "a02f9ddf",
        "outputId": "c84c75d2-eb9c-4e26-d138-01f1ea2b5519"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running inference on test set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/67 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5078a8d507914fd98fdc63432da9e178"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Full test set evaluation:\n",
            "    accuracy: 0.8874\n",
            "   precision: 0.9223\n",
            "      recall: 0.8462\n",
            "          f1: 0.8826\n",
            "  throughput: 316.1 samples/sec\n"
          ]
        }
      ],
      "source": [
        "# 3.3  Evaluate on the full test set (Robust Approach)\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers.pipelines.pt_utils import KeyDataset\n",
        "from evaluate import combine\n",
        "from tqdm.auto import tqdm\n",
        "import time\n",
        "\n",
        "test_data = load_dataset(\"rotten_tomatoes\", split=\"test\")\n",
        "\n",
        "# 1. Load metrics\n",
        "metrics = combine([\"accuracy\", \"precision\", \"recall\", \"f1\"])\n",
        "\n",
        "# 2. Run batched inference\n",
        "print(\"Running inference on test set...\")\n",
        "start_time = time.time()\n",
        "predictions = []\n",
        "\n",
        "# Gunakan batch_size yang sesuai dengan sisa memori GPU T4 kamu (misal: 16 atau 32)\n",
        "for out in tqdm(roberta_pipe(KeyDataset(test_data, \"text\"), batch_size=16, truncation=True)):\n",
        "    # Mapping label string dari model kembali ke integer 0/1\n",
        "    pred_label = 1 if out[\"label\"] == \"LABEL_1\" else 0\n",
        "    predictions.append(pred_label)\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "# 3. Compute metrics\n",
        "references = test_data[\"label\"]\n",
        "eval_results = metrics.compute(predictions=predictions, references=references)\n",
        "\n",
        "# Kalkulasi throughput manual\n",
        "total_samples = len(test_data)\n",
        "throughput = total_samples / (end_time - start_time)\n",
        "\n",
        "# 4. Print results\n",
        "print(\"\\nFull test set evaluation:\")\n",
        "for metric in [\"accuracy\", \"precision\", \"recall\", \"f1\"]:\n",
        "    print(f\"  {metric:>10}: {eval_results[metric]:.4f}\")\n",
        "print(f\"  {'throughput':>10}: {throughput:.1f} samples/sec\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0305a525",
      "metadata": {
        "id": "0305a525"
      },
      "source": [
        "The RoBERTa model achieves strong performance on the Rotten Tomatoes test set. The precision-recall balance tells us whether the model is biased toward positive or negative predictions:\n",
        "\n",
        "$$F_1 = 2 \\cdot \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$\n",
        "\n",
        "**Precision** measures \"of all reviews predicted positive, how many actually are?\" **Recall** measures \"of all actually positive reviews, how many did we catch?\" An $F_1$ near 0.88 is strong for single-sentence sentiment analysis, where context is limited and sarcasm is common.\n",
        "\n",
        "**Throughput** matters in production: on a GPU this model processes hundreds of reviews per second; on CPU it drops to tens. For batch inference on millions of reviews, GPU acceleration is essential.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9a2812b",
      "metadata": {
        "id": "d9a2812b"
      },
      "source": [
        "## Recipe 4 — Zero-Shot Classification\n",
        "\n",
        "**Zero-shot classification** lets you classify text into categories the model was never explicitly trained on. The model (here, `facebook/bart-large-mnli`) was trained on **Natural Language Inference (NLI)** — given a premise and hypothesis, predict whether the hypothesis is entailed, contradicted, or neutral.\n",
        "\n",
        "The trick: for each candidate label, the pipeline constructs the hypothesis *\"This text is about {label}\"* and checks whether the input text entails it. The label with the highest entailment score wins:\n",
        "\n",
        "$$\\hat{y} = \\arg\\max_{c \\in \\mathcal{C}} P(\\text{entailment} \\mid \\text{premise}=\\mathbf{x}, \\; \\text{hypothesis}=\\texttt{\"This is about } c\\texttt{\"})$$\n",
        "\n",
        "This means you can define **any set of labels at inference time** — no retraining required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "bd339f28",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd339f28",
        "outputId": "88358432-0c25-426b-a99d-17d1a0fc5d38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: facebook/bart-large-mnli\n",
            "Running on: cuda\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 4.1  Initialize zero-shot pipeline\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "zero_shot_pipe = pipeline(\n",
        "    \"zero-shot-classification\",\n",
        "    model=\"facebook/bart-large-mnli\",\n",
        "    device=device)\n",
        "\n",
        "print(\"Model: facebook/bart-large-mnli\")\n",
        "print(f\"Running on: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "83009ee9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83009ee9",
        "outputId": "c1516499-7b07-4978-c018-5512114e6547"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: \"I am so hooked to video games as I cannot get any work done!\"\n",
            "\n",
            "  gaming       0.847  #################################\n",
            "  hobby        0.082  ###\n",
            "  technology   0.068  ##\n",
            "  computer     0.002  \n",
            "  art          0.001  \n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 4.2  Classify with custom labels\n",
        "\n",
        "result1 = zero_shot_pipe(\n",
        "    \"I am so hooked to video games as I cannot get any work done!\",\n",
        "    candidate_labels=[\"technology\", \"gaming\", \"hobby\", \"art\", \"computer\"])\n",
        "\n",
        "print(f'Text: \"{result1['sequence']}\"')\n",
        "print()\n",
        "for label, score in zip(result1[\"labels\"], result1[\"scores\"]):\n",
        "    bar = \"#\" * int(score * 40)\n",
        "    print(f\"  {label:<12} {score:.3f}  {bar}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6bff8ef",
      "metadata": {
        "id": "f6bff8ef"
      },
      "source": [
        "The model assigns **gaming** the highest probability, well above the other candidates. Notice that \"hobby\" and \"technology\" have modest scores — they are plausible but less specific. The key advantage: we defined these labels ourselves, with zero training data. The model generalizes from its NLI training to reason about label relevance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "8ac7b491",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ac7b491",
        "outputId": "6065d51a-0913-4584-ea90-185dae431112"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: \"A early morning exercise regimen can drive many diseases away!\"\n",
            "\n",
            "  health       0.907  ####################################\n",
            "  medical      0.069  ##\n",
            "  weather      0.011  \n",
            "  geography    0.009  \n",
            "  politics     0.005  \n",
            "\n",
            "Predicted class: health (confidence: 0.91)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 4.3  Second example — health domain\n",
        "\n",
        "result2 = zero_shot_pipe(\n",
        "    \"A early morning exercise regimen can drive many diseases away!\",\n",
        "    candidate_labels=[\"health\", \"medical\", \"weather\", \"geography\", \"politics\"])\n",
        "\n",
        "print(f'Text: \"{result2['sequence']}\"')\n",
        "print()\n",
        "for label, score in zip(result2[\"labels\"], result2[\"scores\"]):\n",
        "    bar = \"#\" * int(score * 40)\n",
        "    print(f\"  {label:<12} {score:.3f}  {bar}\")\n",
        "print(f\"\\nPredicted class: {result2['labels'][0]} \"\n",
        "      f\"(confidence: {result2['scores'][0]:.2f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f97b3c5",
      "metadata": {
        "id": "7f97b3c5"
      },
      "source": [
        "The model confidently classifies the exercise sentence as **health** with high probability, clearly distinguishing it from the semantically adjacent \"medical\" label. This distinction is impressive: \"health\" implies wellness and prevention, while \"medical\" implies diagnosis and treatment. The model captures this nuance without any labeled examples.\n",
        "\n",
        "**When to use zero-shot classification:**\n",
        "- **Prototyping:** Test whether a classification approach works before investing in labeled data\n",
        "- **Low-resource domains:** When you have fewer than ~100 labeled examples per class\n",
        "- **Dynamic label sets:** When categories change frequently (e.g., customer support routing)\n",
        "\n",
        "**When NOT to use it:** When you have abundant labeled data and need maximum accuracy, a fine-tuned model will always outperform zero-shot.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "565cc3de",
      "metadata": {
        "id": "565cc3de"
      },
      "source": [
        "## Recipe 5 — Generating Text with GPT-2\n",
        "\n",
        "GPT-2 is a **decoder-only** transformer that generates text autoregressively — predicting one token at a time, each conditioned on all previous tokens:\n",
        "\n",
        "$$P(w_1, w_2, \\ldots, w_T) = \\prod_{t=1}^{T} P(w_t \\mid w_1, \\ldots, w_{t-1})$$\n",
        "\n",
        "At each step, the model produces a probability distribution over its entire vocabulary ($\\sim 50{,}257$ tokens). The **decoding strategy** determines how we sample from this distribution, which dramatically affects output quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "4ced256a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ced256a",
        "outputId": "a8f5f90d-5cb6-4038-e0ee-d1090ef06dfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "GPT2LMHeadModel LOAD REPORT from: gpt2\n",
            "Key                  | Status     |  | \n",
            "---------------------+------------+--+-\n",
            "h.{0...11}.attn.bias | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
            "Passing `generation_config` together with generation-related arguments=({'max_length', 'pad_token_id', 'do_sample', 'num_beams', 'num_return_sequences'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Basic beam search ===\n",
            "[1] The cat had no business entering the neighbors garage, but when he got there, he saw a man with a gun.\n",
            "\n",
            "\"He said, 'I'm going to kill you,' and I said, 'I'm going to kill you,' and he said, 'I'm going to kill you,' and I said, 'I'm going to kill you,' and he said, 'I'm going to kill you,' and I said, 'I'm going to kill you,' and he said, 'I'm going to kill you,' and I said, 'I'm going to kill you,' and he said, 'I'm going to kill you,' and I said, 'I'm going to kill you,' and he said, 'I'm going to kill you,' and I said, 'I'm going to kill you,' and he said, 'I'm going to kill you,' and I said, 'I'm going to kill you,' and he said, 'I'm going to kill you,' and I said, 'I'm going to kill you,' and he said, 'I'm going to kill you,' and I said, 'I'm going to kill you,' and he said, 'I'm going to kill you,' and I said, 'I'm going to kill you,' and\n",
            "\n",
            "[2] The cat had no business entering the neighbors garage, but when he got there, he saw a man with a gun.\n",
            "\n",
            "\"He said, 'I'm going to kill you,' and I said, 'I'm going to kill you,' and he said, 'I'm going to kill you,' and I said, 'I'm going to kill you,' and he said, 'I'm going to kill you,' and I said, 'I'm going to kill you,' and he said, 'I'm going to kill you,' and I said, 'I'm going to kill you,' and he said, 'I'm going to kill you,' and he said, 'I'm going to kill you,' and he said, 'I'm going to kill you,' and he said, 'I'm going to kill you,' and he said, 'I'm going to kill you,' and he said, 'I'm going to kill you,' and he said, 'I'm going to kill you,' and he said, 'I'm going to kill you,' and he said, 'I'm going to kill you,' and he said, 'I'm going to kill you,' and he said, 'I'm going to kill you,' and he said, 'I'm going to kill you,' and\n",
            "\n",
            "[3] The cat had no business entering the neighbors garage, but when he got there, he saw a man with a gun.\n",
            "\n",
            "\"He said, 'I'm going to kill you,' and I said, 'I'm going to kill you,' and he said, 'I'm going to kill you,' and I said, 'I'm going to kill you,' and he said, 'I'm going to kill you,' and I said, 'I'm going to kill you,' and he said, 'I'm going to kill you,' and I said, 'I'm going to kill you,' and he said, 'I'm going to kill you,' and I said, 'I'm going to kill you,' and he said, 'I'm going to kill you,' and he said, 'I'm going to kill you,' and he said, 'I'm going to kill you,' and he said, 'I'm going to kill you,' and he said, 'I'm going to kill you,' and he said, 'I'm going to kill you,' and he said, 'I'm going to kill you,' and he said, 'I'm going to kill you,' and he said, 'I'm going to kill you,' and he said, 'I'm going to kill you,' and\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 5.1  Basic text generation\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\", device=device)\n",
        "\n",
        "seed_text = \"The cat had no business entering the neighbors garage, but\"\n",
        "\n",
        "# Basic generation with beam search\n",
        "results_basic = generator(\n",
        "    seed_text,\n",
        "    do_sample=True,\n",
        "    max_length=30,\n",
        "    num_return_sequences=3,\n",
        "    num_beams=5,\n",
        "    pad_token_id=50256)\n",
        "\n",
        "print(\"=== Basic beam search ===\")\n",
        "for i, r in enumerate(results_basic):\n",
        "    print(f\"[{i+1}] {r['generated_text']}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "429e84eb",
      "metadata": {
        "id": "429e84eb"
      },
      "source": [
        "The basic output may contain repetition (the model gets stuck in loops) or incoherent phrasing. Beam search alone keeps the top-$B$ most probable sequences at each step, but it tends to produce **generic, repetitive** text because high-probability tokens are often common words.\n",
        "\n",
        "We can improve quality with several decoding parameters:\n",
        "\n",
        "- `no_repeat_ngram_size=2` — Prevents any bigram from appearing twice\n",
        "- `top_k=50` — Samples from only the 50 highest-probability tokens\n",
        "- `top_p=0.85` — **Nucleus sampling**: samples from the smallest set of tokens whose cumulative probability exceeds $p$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "70157ac2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70157ac2",
        "outputId": "7050fc58-354e-40bc-edd3-bf38931c7c45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Passing `generation_config` together with generation-related arguments=({'max_length', 'pad_token_id', 'do_sample', 'top_p', 'no_repeat_ngram_size', 'num_beams', 'num_return_sequences', 'top_k'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Improved (no-repeat + top-k + top-p) ===\n",
            "[1] The cat had no business entering the neighbors garage, but when he came inside, he found the cat lying on the floor.\n",
            "\n",
            "\"It was just a cat,\" he said. \"I was like, 'What the hell is going on here?' And he was laying there on his back, and I thought, Oh my God, I can't believe this is happening. I'm so scared. It's like I've never seen anything like this in my life.\"\n",
            "\n",
            "[2] The cat had no business entering the neighbors garage, but when he came inside, he found the cat lying on the floor.\n",
            "\n",
            "\"It was just a cat,\" he said. \"I was like, 'What the hell is going on here?' And he was laying there on his back, and I thought, Oh my God, I can't believe this is happening. I'm so scared. It's like I've never seen anything like this in my entire life.\"\n",
            "\n",
            "[3] The cat had no business entering the neighbors garage, but when he came inside, he found the cat lying on the floor.\n",
            "\n",
            "\"It was just a cat,\" he said. \"I was like, 'What the hell is going on here?' And he was laying there on his back, and I thought, Oh my God, I can't believe this is happening. I'm so scared. It's like I've never seen anything like this in my life. And then I realized that I had to get out of the house.\"\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 5.2  Improved generation with sampling controls\n",
        "\n",
        "results_improved = generator(\n",
        "    seed_text,\n",
        "    do_sample=True,\n",
        "    max_length=50,\n",
        "    num_return_sequences=3,\n",
        "    num_beams=5,\n",
        "    no_repeat_ngram_size=2,\n",
        "    top_k=50,\n",
        "    top_p=0.85,\n",
        "    pad_token_id=50256)\n",
        "\n",
        "print(\"=== Improved (no-repeat + top-k + top-p) ===\")\n",
        "for i, r in enumerate(results_improved):\n",
        "    print(f\"[{i+1}] {r['generated_text']}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5b1de16",
      "metadata": {
        "id": "b5b1de16"
      },
      "source": [
        "The improved settings produce more diverse, coherent continuations. The key trade-off in text generation is **quality vs. diversity:**\n",
        "\n",
        "| Parameter | Effect | Trade-off |\n",
        "|-----------|--------|-----------|\n",
        "| `num_beams` | Higher = more search | Slower, more generic |\n",
        "| `top_k` | Limits vocabulary per step | Too low = repetitive, too high = random |\n",
        "| `top_p` | Dynamic vocabulary cutoff | More adaptive than fixed `top_k` |\n",
        "| `no_repeat_ngram_size` | Prevents loops | May block legitimate repetition |\n",
        "| `temperature` | Scales logits before softmax | $<1$ = conservative, $>1$ = creative |\n",
        "\n",
        "**Note:** GPT-2 is a 2019 model with 124M parameters. Modern LLMs (GPT-4, Claude, Llama) use the same autoregressive architecture but with orders of magnitude more parameters and training data, producing far more coherent output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "56a516b5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56a516b5",
        "outputId": "6d29d8b8-66e2-4a14-e5c0-7f9b061bbc5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Extended generation (150 tokens) ===\n",
            "The cat had no business entering the neighbors garage, but when she got there, she found it was empty.\n",
            "\n",
            "\"I was like, 'Oh my God, what the hell is going on?'\" she said. \"It was just a pile of garbage. It was really hard to get it out of there. I didn't know what to do with it.\"\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 5.3  Longer generation\n",
        "\n",
        "results_long = generator(\n",
        "    seed_text,\n",
        "    do_sample=True,\n",
        "    max_length=150,\n",
        "    num_return_sequences=1,\n",
        "    num_beams=5,\n",
        "    no_repeat_ngram_size=2,\n",
        "    top_k=50,\n",
        "    top_p=0.85,\n",
        "    pad_token_id=50256)\n",
        "\n",
        "print(\"=== Extended generation (150 tokens) ===\")\n",
        "print(results_long[0][\"generated_text\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95632bfc",
      "metadata": {
        "id": "95632bfc"
      },
      "source": [
        "Longer generation reveals both the strengths and limitations of GPT-2: it maintains local coherence (each sentence is grammatical) but may lose global coherence (the narrative can drift). This is a fundamental challenge of autoregressive generation — the model has no explicit \"plan\" for where the text should go.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "817f783c",
      "metadata": {
        "id": "817f783c"
      },
      "source": [
        "## Recipe 6 — Language Translation with T5\n",
        "\n",
        "Google's **T5 (Text-to-Text Transfer Transformer)** treats every NLP task as a text-to-text problem. For translation, the input is formatted as:\n",
        "\n",
        "```\n",
        "translate English to French: It's such a beautiful morning today!\n",
        "```\n",
        "\n",
        "The model uses the full **encoder-decoder** architecture:\n",
        "\n",
        "$$\\underbrace{\\text{Encoder}(\\mathbf{x})}_{\\text{contextual representation}} \\;\\xrightarrow{\\text{cross-attention}}\\; \\underbrace{\\text{Decoder}}_{\\text{autoregressive generation}} \\;\\rightarrow\\; \\mathbf{y}$$\n",
        "\n",
        "The encoder processes the source language and produces contextualized representations. The decoder generates the target language one token at a time, attending to the encoder's output via **cross-attention** at every layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "01e7951e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01e7951e",
        "outputId": "e7823b56-7966-44cc-8763-7ff84239c334"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T5-base loaded on cuda\n",
            "  Encoder layers: 12\n",
            "  Decoder layers: 12\n",
            "  Hidden size: 768\n",
            "  Parameters: 222,903,552\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 6.1  Initialize T5 model and tokenizer\n",
        "\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "tokenizer_t5 = T5Tokenizer.from_pretrained(\n",
        "    \"t5-base\", model_max_length=200)\n",
        "model_t5 = T5ForConditionalGeneration.from_pretrained(\n",
        "    \"t5-base\", return_dict=True)\n",
        "model_t5 = model_t5.to(device)\n",
        "\n",
        "print(f\"T5-base loaded on {device}\")\n",
        "print(f\"  Encoder layers: {model_t5.config.num_layers}\")\n",
        "print(f\"  Decoder layers: {model_t5.config.num_decoder_layers}\")\n",
        "print(f\"  Hidden size: {model_t5.config.d_model}\")\n",
        "print(f\"  Parameters: {sum(p.numel() for p in model_t5.parameters()):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "9a7c1fe4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a7c1fe4",
        "outputId": "dc3b0c46-3fbe-4513-a65f-9fc2353890ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source (EN):  It's such a beautiful morning today!\n",
            "Target (FR):  C'est un beau matin aujourd'hui!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 6.2  Translate English to French\n",
        "\n",
        "source_text = \"It's such a beautiful morning today!\"\n",
        "\n",
        "input_ids = tokenizer_t5(\n",
        "    \"translate English to French: \" + source_text,\n",
        "    return_tensors=\"pt\",\n",
        "    truncation=True\n",
        ").input_ids.to(device)\n",
        "\n",
        "output_ids = model_t5.generate(input_ids, max_new_tokens=200)\n",
        "\n",
        "translation = tokenizer_t5.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"Source (EN):  {source_text}\")\n",
        "print(f\"Target (FR):  {translation}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "015eca04",
      "metadata": {
        "id": "015eca04"
      },
      "source": [
        "T5 produces a clean French translation. The task prefix `\"translate English to French:\"` is what steers the model — the same T5 checkpoint can also summarize, answer questions, or classify, simply by changing the prefix. This **multi-task framing** is one of T5's key innovations.\n",
        "\n",
        "**How translation differs from generation:** In GPT-2 (recipe 5), the decoder generates text from a seed with no structured input. In T5, the encoder first builds a **deep understanding** of the source text, and the decoder uses cross-attention to \"look back\" at the source while generating each target token. This two-stage process is essential for tasks where the output must be faithful to the input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "f7974466",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7974466",
        "outputId": "4eda763e-b909-451c-acf8-70b7d2f60aed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EN -> French:\n",
            "  Source:  Machine learning is transforming the way we process text.\n",
            "  Target:  L'apprentissage automatique transforme la façon dont nous traitons le texte.\n",
            "\n",
            "EN -> German:\n",
            "  Source:  The weather is quite nice in Berlin this time of year.\n",
            "  Target:  Das Wetter ist in Berlin zu dieser Jahreszeit recht schön.\n",
            "\n",
            "EN -> Romanian:\n",
            "  Source:  I would like to order a coffee and a croissant please.\n",
            "  Target:  Aş dori să comand o cafea şi o ceară, vă rog.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 6.3  Try more translations\n",
        "\n",
        "test_sentences = [\n",
        "    (\"translate English to French:\",\n",
        "     \"Machine learning is transforming the way we process text.\"),\n",
        "    (\"translate English to German:\",\n",
        "     \"The weather is quite nice in Berlin this time of year.\"),\n",
        "    (\"translate English to Romanian:\",\n",
        "     \"I would like to order a coffee and a croissant please.\"),\n",
        "]\n",
        "\n",
        "for prefix, text in test_sentences:\n",
        "    input_ids = tokenizer_t5(\n",
        "        prefix + \" \" + text,\n",
        "        return_tensors=\"pt\", truncation=True\n",
        "    ).input_ids.to(device)\n",
        "\n",
        "    output_ids = model_t5.generate(input_ids, max_new_tokens=200)\n",
        "    translation = tokenizer_t5.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    lang = prefix.split(\"to \")[-1].rstrip(\":\")\n",
        "    print(f\"EN -> {lang}:\")\n",
        "    print(f\"  Source:  {text}\")\n",
        "    print(f\"  Target:  {translation}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2b4a188",
      "metadata": {
        "id": "b2b4a188"
      },
      "source": [
        "T5-base handles multiple language pairs from a single checkpoint. The quality varies by language pair — French and German translations (well-represented in T5's training data) are typically better than Romanian or other lower-resource languages.\n",
        "\n",
        "**Production considerations for translation:**\n",
        "- **T5-base** (220M parameters) provides decent quality for common language pairs\n",
        "- **T5-large** or **T5-3B** improve quality but require more GPU memory\n",
        "- For production-grade translation, dedicated models like **MarianMT** or **NLLB (No Language Left Behind)** often outperform general-purpose T5\n",
        "- Always have native speakers validate translations for critical applications\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e1d0aba",
      "metadata": {
        "id": "5e1d0aba"
      },
      "source": [
        "## Summary and Key Takeaways\n",
        "\n",
        "This chapter introduced the practical transformer workflow through six recipes that cover the three main architectural patterns:\n",
        "\n",
        "**Encoder-only (BERT, RoBERTa)** — Recipes 1--4 used encoder models that produce rich contextual representations for classification tasks. The encoder processes the entire input bidirectionally, making it ideal for understanding tasks.\n",
        "\n",
        "**Decoder-only (GPT-2)** — Recipe 5 used a decoder model for autoregressive text generation. The decoder sees only left context (previous tokens), making it naturally suited for generation.\n",
        "\n",
        "**Encoder-Decoder (T5)** — Recipe 6 combined both components for translation, where the encoder understands the source and the decoder generates the target.\n",
        "\n",
        "The Hugging Face `pipeline` abstraction made all of this accessible in just a few lines of code — but understanding what happens underneath (tokenization, attention, beam search, cross-attention) is essential for debugging, optimizing, and choosing the right model for your task.\n",
        "\n",
        "**Looking ahead:** The techniques in this chapter use pre-trained models as-is. In production, you would typically **fine-tune** these models on your specific dataset to achieve higher accuracy, which is the natural next step beyond what we covered here."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "55204c0bf1584163984eda7a97aa2701": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aa07764a4a424736a2705957130cd09b",
              "IPY_MODEL_89b682ec2fe949779f2dd57d2f22fba9",
              "IPY_MODEL_1d2e94d7d7ea4d52937814e8154992d3"
            ],
            "layout": "IPY_MODEL_6523473472794bcaa77da4caeff8fff6"
          }
        },
        "aa07764a4a424736a2705957130cd09b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23d750b0274d43ab9e3f73b918e68001",
            "placeholder": "​",
            "style": "IPY_MODEL_2ab552b9b79e4df4bfabc948adc9ce2a",
            "value": "Generating train split: 100%"
          }
        },
        "89b682ec2fe949779f2dd57d2f22fba9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3e4a59339354e86a9b704e086f11d54",
            "max": 8530,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8176e7eed915429ba8b02dd788924142",
            "value": 8530
          }
        },
        "1d2e94d7d7ea4d52937814e8154992d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a86441c4fee34916b31e637ba37e4243",
            "placeholder": "​",
            "style": "IPY_MODEL_ee5a414395fe4a46be99139ad5b84625",
            "value": " 8530/8530 [00:00&lt;00:00, 179545.10 examples/s]"
          }
        },
        "6523473472794bcaa77da4caeff8fff6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23d750b0274d43ab9e3f73b918e68001": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ab552b9b79e4df4bfabc948adc9ce2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f3e4a59339354e86a9b704e086f11d54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8176e7eed915429ba8b02dd788924142": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a86441c4fee34916b31e637ba37e4243": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee5a414395fe4a46be99139ad5b84625": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "210ad4c8d4c04df9aab9f3e2a3442ef0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6595a92182ad4fbd838593ab68cebf93",
              "IPY_MODEL_af91d99d18cd498383c8c76b57be37ef",
              "IPY_MODEL_f9c08280a84c4e4e91182d94e72c3d55"
            ],
            "layout": "IPY_MODEL_d1116a98bada4434bdd977404fa23f18"
          }
        },
        "6595a92182ad4fbd838593ab68cebf93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e879246a70d24e538c5b9de04d12cdb0",
            "placeholder": "​",
            "style": "IPY_MODEL_73e4324ddc1e4bbdb273f54e5db5c459",
            "value": "Generating validation split: 100%"
          }
        },
        "af91d99d18cd498383c8c76b57be37ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f272b987d6e40648efd49f19b2e06d7",
            "max": 1066,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_da2633fa9b314748b5e8443261b06439",
            "value": 1066
          }
        },
        "f9c08280a84c4e4e91182d94e72c3d55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_638514996d094ff1a1102e4378d56c8d",
            "placeholder": "​",
            "style": "IPY_MODEL_ef0bf5ebdf7f4331bd37fca4927708b1",
            "value": " 1066/1066 [00:00&lt;00:00, 52603.98 examples/s]"
          }
        },
        "d1116a98bada4434bdd977404fa23f18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e879246a70d24e538c5b9de04d12cdb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73e4324ddc1e4bbdb273f54e5db5c459": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f272b987d6e40648efd49f19b2e06d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da2633fa9b314748b5e8443261b06439": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "638514996d094ff1a1102e4378d56c8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef0bf5ebdf7f4331bd37fca4927708b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cae1b7e14d2b48efa7cda968a8f96c90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ce1659b9d8cd49d98fa7cbaff21f310e",
              "IPY_MODEL_b8b44eaf83fe48b284b131769684bc19",
              "IPY_MODEL_a9330d57a4eb4be78019260913dc507a"
            ],
            "layout": "IPY_MODEL_14700cfc4e8b42ae980c87f3f6556836"
          }
        },
        "ce1659b9d8cd49d98fa7cbaff21f310e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb37a574fc72476890a6fb729c950318",
            "placeholder": "​",
            "style": "IPY_MODEL_b34b0156d3a44fe891fdd9957ba1311c",
            "value": "Generating test split: 100%"
          }
        },
        "b8b44eaf83fe48b284b131769684bc19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00e46277e573450399b50bb5952abe06",
            "max": 1066,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4b33dbd9d6354d73a189913abee74b54",
            "value": 1066
          }
        },
        "a9330d57a4eb4be78019260913dc507a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50b5afc41cc448d9a95f3b0326e0ffe1",
            "placeholder": "​",
            "style": "IPY_MODEL_1a35811a070445488d039e943a39284c",
            "value": " 1066/1066 [00:00&lt;00:00, 58250.43 examples/s]"
          }
        },
        "14700cfc4e8b42ae980c87f3f6556836": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb37a574fc72476890a6fb729c950318": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b34b0156d3a44fe891fdd9957ba1311c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "00e46277e573450399b50bb5952abe06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b33dbd9d6354d73a189913abee74b54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "50b5afc41cc448d9a95f3b0326e0ffe1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a35811a070445488d039e943a39284c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e30bd40f516a48fca43a21598a1f16b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5a0a0e3099964e32aa3f91ce558e8a46",
              "IPY_MODEL_96733a179a614f03b6a7c866981e8c4a",
              "IPY_MODEL_3011f5c61f09411dad63862270418c5b"
            ],
            "layout": "IPY_MODEL_726d405fbc4640d28a29b3a1b0fb0deb"
          }
        },
        "5a0a0e3099964e32aa3f91ce558e8a46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_618782ccd1154101b7b1b0445ec14cac",
            "placeholder": "​",
            "style": "IPY_MODEL_de6ea12a6c6f45a591db095b0849560f",
            "value": ""
          }
        },
        "96733a179a614f03b6a7c866981e8c4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c9c8cc718d6406588378ac57ed26c54",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d24a8caa6cdc4853975251525fe65cad",
            "value": 1
          }
        },
        "3011f5c61f09411dad63862270418c5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ad134a9ba1a4e37ab10b03168dfd572",
            "placeholder": "​",
            "style": "IPY_MODEL_b6ab609163c24537aecb7a0e723880da",
            "value": " 5/? [00:00&lt;00:00,  2.13it/s]"
          }
        },
        "726d405fbc4640d28a29b3a1b0fb0deb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "618782ccd1154101b7b1b0445ec14cac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de6ea12a6c6f45a591db095b0849560f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c9c8cc718d6406588378ac57ed26c54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d24a8caa6cdc4853975251525fe65cad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7ad134a9ba1a4e37ab10b03168dfd572": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6ab609163c24537aecb7a0e723880da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5078a8d507914fd98fdc63432da9e178": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5a1a7c0757964a49b65e753bb152dee4",
              "IPY_MODEL_d62f79fd60a745a4b0f49ec28bc1a89c",
              "IPY_MODEL_24cada3780f345628ce4aea6f11f6e53"
            ],
            "layout": "IPY_MODEL_f89dc13545564ba4ac99a643ca3acd67"
          }
        },
        "5a1a7c0757964a49b65e753bb152dee4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd578bd608284f8ea09ccac15c301e23",
            "placeholder": "​",
            "style": "IPY_MODEL_c545bfafc348453dbf530491652e2430",
            "value": ""
          }
        },
        "d62f79fd60a745a4b0f49ec28bc1a89c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1939de511f7f4de9ab01f339895cceb3",
            "max": 67,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3f0baa87dee84fd5942f6e493451ca47",
            "value": 67
          }
        },
        "24cada3780f345628ce4aea6f11f6e53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e74c35f3a6574756b083d395554952b1",
            "placeholder": "​",
            "style": "IPY_MODEL_45095e915e0d4daab1a48085d82be832",
            "value": " 1066/? [00:03&lt;00:00, 317.52it/s]"
          }
        },
        "f89dc13545564ba4ac99a643ca3acd67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd578bd608284f8ea09ccac15c301e23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c545bfafc348453dbf530491652e2430": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1939de511f7f4de9ab01f339895cceb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f0baa87dee84fd5942f6e493451ca47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e74c35f3a6574756b083d395554952b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45095e915e0d4daab1a48085d82be832": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
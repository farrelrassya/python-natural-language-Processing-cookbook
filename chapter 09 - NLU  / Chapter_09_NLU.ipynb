{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farrelrassya/python-natural-language-Processing-cookbook/blob/main/chapter%2009%20-%20NLU%20%20/%20Chapter_09_NLU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "020eff6f",
      "metadata": {
        "id": "020eff6f"
      },
      "source": [
        "# Chapter 9 — Natural Language Understanding\n",
        "\n",
        "**Natural Language Understanding (NLU)** goes beyond classification and generation: it aims to *interpret* text -- answering questions, detecting logical relationships between sentences, summarizing documents, and explaining model decisions.\n",
        "\n",
        "This chapter covers eight recipes spanning four NLU capabilities:\n",
        "\n",
        "| Capability | Recipes | Models Used |\n",
        "|---|---|---|\n",
        "| **Question Answering** | 1 (short text), 2 (long text), 3 (extractive corpus QA), 4 (abstractive corpus QA) | BERT-QA, Flan-T5, BM25 retrieval |\n",
        "| **Summarization** | 5 | T5-large, BART-CNN, PEGASUS |\n",
        "| **Textual Entailment** | 6 | T5 (MNLI task prefix) |\n",
        "| **Explainability** | 7 (LIME), 8 (Anchor) | Classifier-agnostic perturbation methods |\n",
        "\n",
        "The progression moves from extracting exact answer spans from context, to generating abstractive answers and summaries, to understanding *why* a classifier makes its decisions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa88d320",
      "metadata": {
        "id": "fa88d320"
      },
      "source": [
        "## 0 — Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ea6928b8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea6928b8",
        "outputId": "748a329a-4da6-425b-9d82-31f82b0cf05a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.3/427.3 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for anchor-exp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 0.1  Install packages\n",
        "\n",
        "import os\n",
        "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"]       = \"false\"\n",
        "\n",
        "!pip install -q \\\n",
        "    datasets \\\n",
        "    evaluate \\\n",
        "    transformers \\\n",
        "    accelerate \\\n",
        "    sentencepiece \\\n",
        "    protobuf \\\n",
        "    torch \\\n",
        "    lime \\\n",
        "    anchor-exp \\\n",
        "    spacy\n",
        "\n",
        "!python -m spacy download en_core_web_sm -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "372b4ad0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "372b4ad0",
        "outputId": "89b589f8-5829-474a-f36a-3107c102a862"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compute device: cuda\n",
            "  GPU: Tesla T4\n",
            "Setup complete.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 0.2  Core imports & configuration\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# Patch jupyter_client to silence datetime.utcnow() spam\n",
        "from datetime import datetime, timezone\n",
        "try:\n",
        "    import jupyter_client.session as _jcs\n",
        "    _jcs.utcnow = lambda: datetime.now(timezone.utc)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Compute device: {device}\")\n",
        "if device.type == \"cuda\":\n",
        "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"  (Models will run on CPU — inference will be slower)\")\n",
        "\n",
        "print(\"Setup complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10de8236",
      "metadata": {
        "id": "10de8236"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6101aff9",
      "metadata": {
        "id": "6101aff9"
      },
      "source": [
        "## Recipe 1 — Answering Questions from a Short Text Passage\n",
        "\n",
        "**Extractive question answering** locates the answer as a span within a given context passage. The model does not generate new text -- it highlights the substring that best answers the question.\n",
        "\n",
        "The architecture is BERT fine-tuned on SQuAD (Stanford Question Answering Dataset). For each token position $i$, the model predicts two scores:\n",
        "\n",
        "$$s_{\\text{start}}(i) = \\mathbf{w}_s^T \\mathbf{h}_i, \\qquad s_{\\text{end}}(i) = \\mathbf{w}_e^T \\mathbf{h}_i$$\n",
        "\n",
        "where $\\mathbf{h}_i \\in \\mathbb{R}^{1024}$ is the token's contextual representation. The answer span is the substring from $\\arg\\max_i s_{\\text{start}}(i)$ to $\\arg\\max_j s_{\\text{end}}(j)$ that maximizes $s_{\\text{start}}(i) + s_{\\text{end}}(j)$ with $j \\ge i$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "abdf5185",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abdf5185",
        "outputId": "94cfae8c-d6e0-4906-c80c-d8947899cca5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "BertForQuestionAnswering LOAD REPORT from: bert-large-uncased-whole-word-masking-finetuned-squad\n",
            "Key                      | Status     |  | \n",
            "-------------------------+------------+--+-\n",
            "bert.pooler.dense.weight | UNEXPECTED |  | \n",
            "bert.pooler.dense.bias   | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QA pipeline ready (BERT-large, SQuAD fine-tuned)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 1.1  Initialize QA pipeline with BERT\n",
        "\n",
        "from transformers import pipeline, BertForQuestionAnswering, BertTokenizer\n",
        "\n",
        "qa_model = BertForQuestionAnswering.from_pretrained(\n",
        "    \"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
        "qa_tokenizer = BertTokenizer.from_pretrained(\n",
        "    \"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
        "\n",
        "qa_pipeline = pipeline(\"question-answering\",\n",
        "                       model=qa_model,\n",
        "                       tokenizer=qa_tokenizer,\n",
        "                       device=device)\n",
        "\n",
        "print(\"QA pipeline ready (BERT-large, SQuAD fine-tuned)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fd716c0",
      "metadata": {
        "id": "5fd716c0"
      },
      "source": [
        "We load `bert-large-uncased-whole-word-masking-finetuned-squad`, a $335$M-parameter model. **Whole-word masking** means that during pre-training, entire words (not just subword tokens) were masked -- this produces better contextual representations for QA because the model learns to reconstruct complete words from surrounding context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4cad0403",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cad0403",
        "outputId": "e8fa2183-6fe0-49dd-d603-797b19c9778f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context (427 chars):\n",
            "  \"The cat had no business entering the neighbors garage, but she was there to help. The neighbor, who ...\"\n",
            "\n",
            "Q: Where was the cat trying to enter?\n",
            "A: \"the neighbors garage\"  (score: 0.256, span: [33:53])\n",
            "\n",
            "Q: What did the cat do after entering the garage?\n",
            "A: \"hit her in the face, knocking her to the ground\"  (score: 0.380, span: [379:426])\n",
            "\n",
            "Q: How did the neighbor feel?\n",
            "A: \"afraid for her life\"  (score: 0.373, span: [262:281])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 1.2  Answer questions from a context passage\n",
        "\n",
        "context = (\n",
        "    \"The cat had no business entering the neighbors garage, but she was \"\n",
        "    \"there to help. The neighbor, who asked not to be identified, said \"\n",
        "    \"she didn't know what to make of the cat's behavior. She said it \"\n",
        "    \"seemed like it was trying to get into her home, and that she was \"\n",
        "    \"afraid for her life. The neighbor said that when she went to check \"\n",
        "    \"on her cat, it ran into the neighbor's garage and hit her in the \"\n",
        "    \"face, knocking her to the ground.\"\n",
        ")\n",
        "\n",
        "questions = [\n",
        "    \"Where was the cat trying to enter?\",\n",
        "    \"What did the cat do after entering the garage?\",\n",
        "    \"How did the neighbor feel?\",\n",
        "]\n",
        "\n",
        "print(f\"Context ({len(context)} chars):\")\n",
        "print(f'  \"{context[:100]}...\"')\n",
        "print()\n",
        "\n",
        "for q in questions:\n",
        "    result = qa_pipeline(question=q, context=context)\n",
        "    print(f\"Q: {q}\")\n",
        "    print(f'A: \"{result['answer']}\"  (score: {result[\"score\"]:.3f}, '\n",
        "          f'span: [{result[\"start\"]}:{result[\"end\"]}])')\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "940e5940",
      "metadata": {
        "id": "940e5940"
      },
      "source": [
        "The model extracts answer spans directly from the context. The **score** is the product of the start and end logit softmax probabilities -- higher means more confident. Notice the model handles different question types: location (\"Where\"), action (\"What did\"), and emotion (\"How did\").\n",
        "\n",
        "The `start` and `end` indices point to exact character positions in the context, which is useful for highlighting answers in a UI.\n",
        "\n",
        "**Limitation:** The model can only answer questions whose answers appear verbatim in the context. If the answer requires reasoning or synthesis, extractive QA fails -- that is where abstractive approaches (Recipe 4) come in.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "caa18715",
      "metadata": {
        "id": "caa18715"
      },
      "source": [
        "## Recipe 2 — Answering Questions from a Long Text Passage\n",
        "\n",
        "For longer texts that exceed a model's maximum input length ($512$ tokens for BERT), we need to **chunk** the text into overlapping windows and run QA on each chunk, then select the highest-confidence answer across all chunks.\n",
        "\n",
        "The book uses DeepPavlov's KBQA system for open-domain QA backed by knowledge graphs. Here we demonstrate the same concept using the Hugging Face QA pipeline with automatic stride-based chunking, which handles long documents seamlessly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "308ba186",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "308ba186",
        "outputId": "0458d122-f339-4da9-adfb-7ea508a77fda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Long context: 936 chars, ~165 words\n",
            "Q: When was the Great Pyramid completed?\n",
            "A: \"2560 BC\"  (score: 0.679)\n",
            "\n",
            "Q: How tall was the pyramid originally?\n",
            "A: \"146.5 metres (481 ft)\"  (score: 0.580)\n",
            "\n",
            "Q: How many stone blocks does the pyramid have?\n",
            "A: \"2,300,000\"  (score: 0.761)\n",
            "\n",
            "Q: Who was the pyramid built for?\n",
            "A: \"pharaoh Khufu\"  (score: 0.751)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 2.1  Long-context QA with stride\n",
        "\n",
        "# Long passage (multiple paragraphs)\n",
        "long_context = (\n",
        "    \"The Great Pyramid of Giza is the oldest and largest of the pyramids in \"\n",
        "    \"the Giza pyramid complex bordering present-day Giza in Greater Cairo, \"\n",
        "    \"Egypt. It is the oldest of the Seven Wonders of the Ancient World, and \"\n",
        "    \"the only one to remain largely intact. It was built as a tomb for the \"\n",
        "    \"Fourth Dynasty Egyptian pharaoh Khufu and was completed around 2560 BC. \"\n",
        "    \"The Great Pyramid was the tallest man-made structure in the world for \"\n",
        "    \"more than 3,800 years. Originally, the Great Pyramid was covered by \"\n",
        "    \"limestone casing stones that formed a smooth outer surface. What is seen \"\n",
        "    \"today is the underlying core structure. The original height was 146.5 \"\n",
        "    \"metres (481 ft), but today it stands at 138.8 metres (455 ft). \"\n",
        "    \"The pyramid is estimated to have around 2,300,000 stone blocks that \"\n",
        "    \"weigh from 2.5 to 15 tonnes each. The total mass of the pyramid is \"\n",
        "    \"estimated at 6.1 million tonnes. The base covers an area of about \"\n",
        "    \"53,000 square metres (570,000 sq ft).\"\n",
        ")\n",
        "\n",
        "print(f\"Long context: {len(long_context)} chars, \"\n",
        "      f\"~{len(long_context.split())} words\")\n",
        "\n",
        "long_questions = [\n",
        "    \"When was the Great Pyramid completed?\",\n",
        "    \"How tall was the pyramid originally?\",\n",
        "    \"How many stone blocks does the pyramid have?\",\n",
        "    \"Who was the pyramid built for?\",\n",
        "]\n",
        "\n",
        "for q in long_questions:\n",
        "    result = qa_pipeline(question=q, context=long_context)\n",
        "    print(f\"Q: {q}\")\n",
        "    print(f'A: \"{result[\"answer\"]}\"  (score: {result[\"score\"]:.3f})')\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2013d28",
      "metadata": {
        "id": "f2013d28"
      },
      "source": [
        "The QA pipeline handles multi-paragraph input by internally splitting the context into overlapping windows (controlled by the `doc_stride` parameter, default 128 tokens). Each window is scored independently, and the answer with the highest confidence across all windows is returned.\n",
        "\n",
        "**Open-Domain QA** (as implemented by systems like DeepPavlov KBQA) goes further: it searches a knowledge base (e.g., Wikipedia/Wikidata) to find relevant passages, then applies extractive or abstractive QA. The key difference is that the user does not provide the context -- the system retrieves it.\n",
        "\n",
        "For production open-domain QA, the **retriever-reader** pattern (Recipe 3) is the standard architecture.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "158d5510",
      "metadata": {
        "id": "158d5510"
      },
      "source": [
        "## Recipe 3 — Extractive QA from a Document Corpus\n",
        "\n",
        "When you have hundreds or thousands of documents, scanning all of them for every question is prohibitively slow. The solution is a **two-stage pipeline:**\n",
        "\n",
        "$$\\underbrace{\\text{Query}}_{\\text{user question}} \\;\\xrightarrow{\\text{Retriever}}\\; \\underbrace{\\text{Top-}k \\text{ documents}}_{\\text{candidate passages}} \\;\\xrightarrow{\\text{Reader}}\\; \\underbrace{\\text{Answer span}}_{\\text{exact answer}}$$\n",
        "\n",
        "**Stage 1 — Retriever (BM25):** Uses the classic BM25 scoring function to find the top-$k$ most relevant documents. BM25 scores each document $d$ for query $q$ as:\n",
        "\n",
        "$$\\text{BM25}(d, q) = \\sum_{t \\in q} \\text{IDF}(t) \\cdot \\frac{f(t, d) \\cdot (k_1 + 1)}{f(t, d) + k_1 \\cdot \\left(1 - b + b \\cdot \\frac{|d|}{\\text{avgdl}}\\right)}$$\n",
        "\n",
        "**Stage 2 — Reader (BERT-SQuAD):** Runs extractive QA on the retrieved passages to find the exact answer span.\n",
        "\n",
        "The book uses the Haystack framework for this. Here we implement the same retriever-reader concept using lightweight components compatible with Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0e199054",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e199054",
        "outputId": "8be50322-2ded-4f3c-9db6-3beeb84f117d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus: 7 documents\n",
            "  [Great Pyramid of Giza] 307 chars\n",
            "  [Hanging Gardens of Babylon] 341 chars\n",
            "  [Colossus of Rhodes] 319 chars\n",
            "  [Lighthouse of Alexandria] 338 chars\n",
            "  [Temple of Artemis] 290 chars\n",
            "  [Statue of Zeus at Olympia] 275 chars\n",
            "  [Mausoleum at Halicarnassus] 306 chars\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 3.1  Build a simple document corpus\n",
        "\n",
        "documents = [\n",
        "    {\"title\": \"Great Pyramid of Giza\",\n",
        "     \"content\": \"The Great Pyramid of Giza was built around 2560 BC for \"\n",
        "     \"the pharaoh Khufu. It is the oldest of the Seven Wonders of the \"\n",
        "     \"Ancient World and the only one still largely intact. The pyramid \"\n",
        "     \"originally stood 146.5 metres tall with smooth limestone casing \"\n",
        "     \"stones. It contains approximately 2.3 million stone blocks.\"},\n",
        "    {\"title\": \"Hanging Gardens of Babylon\",\n",
        "     \"content\": \"The Hanging Gardens of Babylon were described as a \"\n",
        "     \"remarkable feat of engineering with an ascending series of tiered \"\n",
        "     \"gardens containing a wide variety of trees, shrubs, and vines. \"\n",
        "     \"They were said to have been built in the ancient city of Babylon, \"\n",
        "     \"near present-day Hillah, Iraq. Their existence has not been \"\n",
        "     \"definitively proven by archaeology.\"},\n",
        "    {\"title\": \"Colossus of Rhodes\",\n",
        "     \"content\": \"The Colossus of Rhodes was a statue of the Greek sun \"\n",
        "     \"god Helios, erected in the city of Rhodes by Chares of Lindos \"\n",
        "     \"in 280 BC. It stood approximately 33 metres (108 feet) high, \"\n",
        "     \"making it one of the tallest statues of the ancient world. The \"\n",
        "     \"statue was destroyed by an earthquake in 226 BC, having stood \"\n",
        "     \"for only 54 years.\"},\n",
        "    {\"title\": \"Lighthouse of Alexandria\",\n",
        "     \"content\": \"The Lighthouse of Alexandria, also known as the Pharos \"\n",
        "     \"of Alexandria, was built between 280 and 247 BC on the island of \"\n",
        "     \"Pharos in Alexandria, Egypt. Estimates of its height range from \"\n",
        "     \"100 to 140 metres. It was one of the tallest man-made structures \"\n",
        "     \"for many centuries and was damaged by several earthquakes before \"\n",
        "     \"being abandoned in 1480.\"},\n",
        "    {\"title\": \"Temple of Artemis\",\n",
        "     \"content\": \"The Temple of Artemis was a Greek temple dedicated to \"\n",
        "     \"the goddess Artemis. Located in Ephesus near the modern town of \"\n",
        "     \"Selcuk in Turkey, it was completely rebuilt three times before \"\n",
        "     \"its final destruction in 401 AD. The temple was 137 metres long \"\n",
        "     \"and was famous for its elaborate decorations.\"},\n",
        "    {\"title\": \"Statue of Zeus at Olympia\",\n",
        "     \"content\": \"The Statue of Zeus at Olympia was a giant seated figure \"\n",
        "     \"made by the Greek sculptor Phidias around 435 BC. The statue was \"\n",
        "     \"about 12 metres (40 feet) tall and was made of ivory plates and \"\n",
        "     \"gold panels on a wooden framework. It was placed in the Temple \"\n",
        "     \"of Zeus at Olympia, Greece.\"},\n",
        "    {\"title\": \"Mausoleum at Halicarnassus\",\n",
        "     \"content\": \"The Mausoleum at Halicarnassus was a tomb built between \"\n",
        "     \"353 and 350 BC for Mausolus, a satrap of the Achaemenid Empire. \"\n",
        "     \"The structure was approximately 45 metres (148 feet) in height. \"\n",
        "     \"The word mausoleum has since come to be used generically for any \"\n",
        "     \"grand tomb. It was located in present-day Bodrum, Turkey.\"},\n",
        "]\n",
        "\n",
        "print(f\"Corpus: {len(documents)} documents\")\n",
        "for doc in documents:\n",
        "    print(f'  [{doc[\"title\"]}] {len(doc[\"content\"])} chars')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4e8d4c6",
      "metadata": {
        "id": "c4e8d4c6"
      },
      "source": [
        "We work with a compact corpus of $7$ documents about the Seven Wonders of the Ancient World, with document lengths ranging from $275$ to $341$ characters. This small corpus is ideal for demonstrating the retriever-reader pattern: large enough to require retrieval (scanning all 7 documents per query would be wasteful at scale), but small enough that we can inspect every retrieval decision. In production systems, this same architecture handles millions of documents using Elasticsearch or FAISS-backed document stores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8e7fb4c4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e7fb4c4",
        "outputId": "73e5e312-7056-4752-d8da-1c462c398c6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: \"Who built the Colossus?\"\n",
            "Top 3 retrieved documents:\n",
            "  [Great Pyramid of Giza] BM25=0.72\n",
            "  [Mausoleum at Halicarnassus] BM25=0.71\n",
            "  [Lighthouse of Alexandria] BM25=0.67\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 3.2  BM25 retriever (lightweight implementation)\n",
        "\n",
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "class SimpleBM25:\n",
        "    def __init__(self, documents, k1=1.5, b=0.75):\n",
        "        self.docs = documents\n",
        "        self.k1, self.b = k1, b\n",
        "        self.tokenized = [d[\"content\"].lower().split() for d in documents]\n",
        "        self.avgdl = np.mean([len(d) for d in self.tokenized])\n",
        "        self.N = len(documents)\n",
        "        self.df = Counter()\n",
        "        for doc_tokens in self.tokenized:\n",
        "            for t in set(doc_tokens):\n",
        "                self.df[t] += 1\n",
        "\n",
        "    def score(self, query):\n",
        "        query_tokens = query.lower().split()\n",
        "        scores = []\n",
        "        for i, doc_tokens in enumerate(self.tokenized):\n",
        "            tf = Counter(doc_tokens)\n",
        "            s = 0.0\n",
        "            for t in query_tokens:\n",
        "                if t not in tf:\n",
        "                    continue\n",
        "                idf = math.log((self.N - self.df[t] + 0.5) /\n",
        "                               (self.df[t] + 0.5) + 1)\n",
        "                freq = tf[t]\n",
        "                denom = freq + self.k1 * (1 - self.b + self.b *\n",
        "                        len(doc_tokens) / self.avgdl)\n",
        "                s += idf * (freq * (self.k1 + 1)) / denom\n",
        "            scores.append(s)\n",
        "        return scores\n",
        "\n",
        "    def retrieve(self, query, top_k=3):\n",
        "        scores = self.score(query)\n",
        "        ranked = sorted(enumerate(scores), key=lambda x: -x[1])\n",
        "        return [(self.docs[i], s) for i, s in ranked[:top_k]]\n",
        "\n",
        "retriever = SimpleBM25(documents)\n",
        "\n",
        "# Test retrieval\n",
        "query = \"Who built the Colossus?\"\n",
        "retrieved = retriever.retrieve(query, top_k=3)\n",
        "print(f'Query: \"{query}\"')\n",
        "print(f\"Top {len(retrieved)} retrieved documents:\")\n",
        "for doc, score in retrieved:\n",
        "    print(f'  [{doc[\"title\"]}] BM25={score:.2f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ee182c2",
      "metadata": {
        "id": "5ee182c2"
      },
      "source": [
        "The BM25 retriever returned **Great Pyramid** (score $0.72$), **Mausoleum** ($0.71$), and **Lighthouse** ($0.67$) for the query \"Who built the Colossus?\" The actual Colossus of Rhodes document is **missing** from the top 3.\n",
        "\n",
        "This is a textbook BM25 failure: the algorithm matches on **exact word overlap**, and the Colossus document uses *\"erected\"* rather than *\"built.\"* BM25 cannot bridge this synonym gap. The documents that *do* contain \"built\" (Pyramid: \"was built around 2560 BC,\" Mausoleum: \"tomb built between 353 and 350 BC\") score higher despite being irrelevant.\n",
        "\n",
        "This demonstrates why production QA systems increasingly use **dense retrieval** (e.g., DPR) alongside BM25. Dense retrievers encode documents and queries into the same vector space, where \"built\" and \"erected\" have similar embeddings. A **hybrid** approach (BM25 + dense) catches both lexical matches and semantic matches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "036bf670",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "036bf670",
        "outputId": "6b6bb142-5b17-4f18-c388-732d83624bb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: Who was the Great Pyramid built for?\n",
            "A: \"Mausolus\"  (reader=0.916, source=Mausoleum at Halicarnassus)\n",
            "\n",
            "Q: How tall was the Colossus of Rhodes?\n",
            "A: \"146.5 metres\"  (reader=0.613, source=Great Pyramid of Giza)\n",
            "\n",
            "Q: Where was the Temple of Artemis located?\n",
            "A: \"Ephesus\"  (reader=0.773, source=Temple of Artemis)\n",
            "\n",
            "Q: When was the Lighthouse of Alexandria abandoned?\n",
            "A: \"1480\"  (reader=0.970, source=Lighthouse of Alexandria)\n",
            "\n",
            "Q: Who sculpted the Statue of Zeus?\n",
            "A: \"Phidias\"  (reader=0.994, source=Statue of Zeus at Olympia)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 3.3  Extractive QA pipeline (retriever + reader)\n",
        "\n",
        "def extractive_qa(question, retriever, reader, top_k_retrieve=3,\n",
        "                  top_k_answers=3):\n",
        "    candidates = retriever.retrieve(question, top_k=top_k_retrieve)\n",
        "    all_answers = []\n",
        "    for doc, bm25_score in candidates:\n",
        "        result = reader(question=question, context=doc[\"content\"])\n",
        "        result[\"source\"] = doc[\"title\"]\n",
        "        result[\"bm25_score\"] = bm25_score\n",
        "        all_answers.append(result)\n",
        "    all_answers.sort(key=lambda x: -x[\"score\"])\n",
        "    return all_answers[:top_k_answers]\n",
        "\n",
        "test_questions = [\n",
        "    \"Who was the Great Pyramid built for?\",\n",
        "    \"How tall was the Colossus of Rhodes?\",\n",
        "    \"Where was the Temple of Artemis located?\",\n",
        "    \"When was the Lighthouse of Alexandria abandoned?\",\n",
        "    \"Who sculpted the Statue of Zeus?\",\n",
        "]\n",
        "\n",
        "for q in test_questions:\n",
        "    answers = extractive_qa(q, retriever, qa_pipeline)\n",
        "    best = answers[0]\n",
        "    print(f\"Q: {q}\")\n",
        "    print(f'A: \"{best[\"answer\"]}\"  (reader={best[\"score\"]:.3f}, '\n",
        "          f'source={best[\"source\"]})')\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "811d6afb",
      "metadata": {
        "id": "811d6afb"
      },
      "source": [
        "The retriever-reader pipeline reveals both the power and a critical pitfall of two-stage QA:\n",
        "\n",
        "**Correct answers (3/5):**\n",
        "- *\"Where was the Temple of Artemis?\"* $\\rightarrow$ **\"Ephesus\"** (reader $0.773$, from Temple of Artemis) -- correct document retrieved\n",
        "- *\"When was the Lighthouse abandoned?\"* $\\rightarrow$ **\"1480\"** (reader $0.970$, from Lighthouse of Alexandria) -- near-perfect confidence\n",
        "- *\"Who sculpted the Statue of Zeus?\"* $\\rightarrow$ **\"Phidias\"** (reader $0.994$, from Statue of Zeus at Olympia) -- highest confidence in the batch\n",
        "\n",
        "**Wrong answers (2/5) -- retriever failures:**\n",
        "- *\"Who was the Great Pyramid built for?\"* $\\rightarrow$ **\"Mausolus\"** (reader $0.916$, from Mausoleum at Halicarnassus). The reader confidently answered from the *wrong* document because the retriever never surfaced the Great Pyramid document for this query.\n",
        "- *\"How tall was the Colossus of Rhodes?\"* $\\rightarrow$ **\"146.5 metres\"** (reader $0.613$, from Great Pyramid of Giza). Again, the Colossus document was not retrieved (BM25 matched \"built\" but the Colossus text uses \"erected\"), so the reader answered from the Pyramid document.\n",
        "\n",
        "**Critical insight for production systems:** The reader gave a **high-confidence wrong answer** ($0.916$) for the Pyramid question -- it looks authoritative but is completely wrong. This is the most dangerous failure mode in retriever-reader systems: **the reader cannot know it received the wrong document.** In production, combine BM25 with **dense retrieval** (hybrid search) to catch synonym mismatches, and always return the source document alongside the answer so users can verify."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87d43c3d",
      "metadata": {
        "id": "87d43c3d"
      },
      "source": [
        "## Recipe 4 — Abstractive QA from a Document Corpus\n",
        "\n",
        "Extractive QA copies text verbatim from the source. **Abstractive QA** generates a natural-language answer in the model's own words, synthesizing information from retrieved passages. This is the core pattern behind Retrieval-Augmented Generation (RAG):\n",
        "\n",
        "$$\\text{Answer} = \\text{LLM}\\big(\\text{Prompt}(\\text{question}, \\text{retrieved\\_docs})\\big)$$\n",
        "\n",
        "We use Google's **Flan-T5** (instruction-tuned T5) as the generator, combined with our BM25 retriever from Recipe 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8710b459",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8710b459",
        "outputId": "1bba7e80-dccc-46e3-e04b-d8f156e81a83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tied weights mapping and config for this model specifies to tie shared.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flan-T5-base loaded on cuda\n",
            "  Parameters: 247,577,856\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 4.1  Initialize Flan-T5 for abstractive generation\n",
        "\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "flan_tokenizer = T5Tokenizer.from_pretrained(\n",
        "    \"google/flan-t5-base\", legacy=False)\n",
        "flan_model = T5ForConditionalGeneration.from_pretrained(\n",
        "    \"google/flan-t5-base\").to(device)\n",
        "\n",
        "print(f\"Flan-T5-base loaded on {device}\")\n",
        "print(f\"  Parameters: {sum(p.numel() for p in flan_model.parameters()):,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a8e8657",
      "metadata": {
        "id": "6a8e8657"
      },
      "source": [
        "Flan-T5-base has $247{,}577{,}856$ parameters ($\\sim 248$M) -- considerably smaller than the BERT-large QA model ($335$M) yet capable of generating free-form answers rather than just extracting spans. The \"Flan\" prefix indicates **instruction tuning**: the base T5 model was further trained on $1{,}836$ tasks phrased as natural-language instructions, which is what allows it to follow our RAG prompt without any task-specific fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d57a89d7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d57a89d7",
        "outputId": "659e6154-15fc-4010-bc38-27c27f242c61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: What is the Great Pyramid of Giza?\n",
            "A: the oldest of the Seven Wonders\n",
            "   Sources: Great Pyramid of Giza, Colossus of Rhodes, Lighthouse of Alexandria\n",
            "\n",
            "Q: Where are the Hanging Gardens?\n",
            "A: near present-day Hilla\n",
            "   Sources: Hanging Gardens of Babylon, Great Pyramid of Giza, Colossus of Rhodes\n",
            "\n",
            "Q: What happened to the Colossus of Rhodes?\n",
            "A: The statue was destroyed by an earthquake in 226\n",
            "   Sources: Colossus of Rhodes, Temple of Artemis, Lighthouse of Alexandria\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 4.2  Build abstractive QA pipeline (RAG pattern)\n",
        "\n",
        "def abstractive_qa(question, retriever, tokenizer, model,\n",
        "                   top_k=3, max_answer_length=100):\n",
        "    candidates = retriever.retrieve(question, top_k=top_k)\n",
        "    context = \" \".join([doc[\"content\"] for doc, _ in candidates])\n",
        "\n",
        "    prompt = (\n",
        "        \"Answer the following question based on the provided context. \"\n",
        "        \"Provide a clear and concise response in your own words, \"\n",
        "        \"no longer than 50 words.\\n\\n\"\n",
        "        f\"Context: {context}\\n\\n\"\n",
        "        f\"Question: {question}\\n\\n\"\n",
        "        \"Answer:\"\n",
        "    )\n",
        "\n",
        "    input_ids = tokenizer(\n",
        "        prompt, return_tensors=\"pt\", truncation=True,\n",
        "        max_length=512).input_ids.to(device)\n",
        "\n",
        "    output_ids = model.generate(\n",
        "        input_ids, max_new_tokens=max_answer_length,\n",
        "        num_beams=4, early_stopping=True)\n",
        "\n",
        "    answer = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    return answer, [doc[\"title\"] for doc, _ in candidates]\n",
        "\n",
        "rag_questions = [\n",
        "    \"What is the Great Pyramid of Giza?\",\n",
        "    \"Where are the Hanging Gardens?\",\n",
        "    \"What happened to the Colossus of Rhodes?\",\n",
        "]\n",
        "\n",
        "for q in rag_questions:\n",
        "    answer, sources = abstractive_qa(\n",
        "        q, retriever, flan_tokenizer, flan_model)\n",
        "    print(f\"Q: {q}\")\n",
        "    print(f\"A: {answer}\")\n",
        "    print(f\"   Sources: {', '.join(sources)}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9404dfd",
      "metadata": {
        "id": "a9404dfd"
      },
      "source": [
        "The abstractive answers are more readable and natural than the extractive spans from Recipe 3. The model synthesizes information from the retrieved passages rather than copying verbatim. This is the fundamental pattern behind modern RAG systems.\n",
        "\n",
        "**Extractive vs. Abstractive QA:**\n",
        "\n",
        "| Aspect | Extractive | Abstractive |\n",
        "|--------|-----------|-------------|\n",
        "| **Output** | Exact text span from source | Generated natural language |\n",
        "| **Faithfulness** | Always grounded in source | May hallucinate |\n",
        "| **Readability** | Can be fragmentary | Natural and fluent |\n",
        "| **Attribution** | Easy (exact span + document) | Harder to verify |\n",
        "| **Use case** | Legal, compliance, fact-checking | Customer support, search |\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8dc178dd",
      "metadata": {
        "id": "8dc178dd"
      },
      "source": [
        "## Recipe 5 — Summarizing Text with Transformer Models\n",
        "\n",
        "Text summarization condenses a long passage into a shorter version while preserving key information. We compare three models that represent different summarization strategies:\n",
        "\n",
        "- **T5-large** — General-purpose encoder-decoder, instruction-based\n",
        "- **BART-CNN** — Denoising autoencoder fine-tuned on CNN/DailyMail news\n",
        "- **PEGASUS** — Pre-trained with gap-sentence generation, optimized for abstractive summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "dcc844cc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcc844cc",
        "outputId": "7ad5890d-e996-49f9-ba2c-b31622c71780"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original passage: 105 words\n",
            "\"The color of animals is by no means a matter of chance; it depends on many considerations, but in the majority of cases ...\"\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 5.1  Define the passage to summarize\n",
        "\n",
        "passage = (\n",
        "    \"The color of animals is by no means a matter of chance; it depends \"\n",
        "    \"on many considerations, but in the majority of cases tends to \"\n",
        "    \"protect the animal from danger by rendering it less conspicuous. \"\n",
        "    \"Perhaps it may be said that if coloring is mainly protective, there \"\n",
        "    \"ought to be but few brightly colored animals. There are, however, \"\n",
        "    \"not a few cases in which vivid colors are themselves protective. \"\n",
        "    \"The kingfisher itself, though so brightly colored, is by no means \"\n",
        "    \"easy to see. The blue harmonizes with the water, and the bird as \"\n",
        "    \"it darts along the stream looks almost like a flash of sunlight.\"\n",
        ")\n",
        "\n",
        "word_count = len(passage.split())\n",
        "print(f\"Original passage: {word_count} words\")\n",
        "print(f'\"{passage[:120]}...\"')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbb86832",
      "metadata": {
        "id": "fbb86832"
      },
      "source": [
        "We use a $105$-word passage about animal coloration as the summarization benchmark. The passage makes a nuanced argument with two key ideas: (1) most animal colors are **protective** camouflage, and (2) even **vivid colors** can be protective (the kingfisher's blue blends with water). A good summary must capture both the general rule and the exception -- dropping either one loses the passage's central insight."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "4eac60f8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eac60f8",
        "outputId": "abaf3e2b-aa93-4b92-e27e-2a9dea49b500"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== T5-large ===\n",
            "Summary (47 words):\n",
            "the color of animals is by no means a matter of chance; it depends on many considerations . in the majority of cases, coloring tends to protect the animal from danger . there are, however, not a few cases in which vivid colors are themselves protective .\n"
          ]
        }
      ],
      "source": [
        "# 5.2 Summarize with T5-large (Native Approach)\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "model_name = \"t5-large\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "t5_model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "# T5 secara spesifik membutuhkan task prefix untuk tahu apa yang harus dilakukan\n",
        "input_text = \"summarize: \" + passage\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n",
        "\n",
        "# Generate summary\n",
        "summary_ids = t5_model.generate(\n",
        "    inputs[\"input_ids\"],\n",
        "    max_length=word_count,\n",
        "    min_length=20,\n",
        "    length_penalty=2.0,\n",
        "    num_beams=4,\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "t5_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"=== T5-large ===\")\n",
        "print(f\"Summary ({len(t5_summary.split())} words):\")\n",
        "print(t5_summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbdc9b24",
      "metadata": {
        "id": "fbdc9b24"
      },
      "source": [
        "**T5-large** produces a **47-word** summary, compressing to **45%** of the original $105$ words. It captures the main argument -- animal coloring is not random but protective -- and preserves the key counterpoint about vivid colors being protective too. However, it drops the concrete kingfisher example and the beautiful sunlight metaphor. The summary reads as a competent paraphrase that prioritizes the abstract argument over illustrative detail.\n",
        "\n",
        "T5 approaches summarization as a sequence-to-sequence task: the encoder processes the full passage, and the decoder generates the summary token by token, conditioned on the encoded representation. The `max_length` parameter prevents the summary from exceeding the original length (a useful safety guardrail)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "ae055f1e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae055f1e",
        "outputId": "c0ddce93-85a1-4199-b1ee-d98a0a86d9ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Please make sure the generation config includes `forced_bos_token_id=0`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== BART-CNN ===\n",
            "Summary (62 words):\n",
            "The color of animals is by no means a matter of chance; it depends on many considerations, but in the majority of cases tends to protect the animal from danger by rendering it less conspicuous. There are, however, not a few cases in which vivid colors are themselves protective. The kingfisher itself, though so brightly colored, is byno means easy to see.\n"
          ]
        }
      ],
      "source": [
        "# 5.3  Summarize with BART-CNN (Native Approach)\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "model_name = \"facebook/bart-large-cnn\"\n",
        "bart_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "bart_model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "# Tokenisasi input\n",
        "inputs = bart_tokenizer(\n",
        "    [passage],\n",
        "    max_length=1024,\n",
        "    return_tensors=\"pt\",\n",
        "    truncation=True\n",
        ").to(device)\n",
        "\n",
        "# Generate summary\n",
        "summary_ids = bart_model.generate(\n",
        "    inputs[\"input_ids\"],\n",
        "    max_length=word_count,\n",
        "    min_length=20,\n",
        "    length_penalty=2.0,\n",
        "    num_beams=4,\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "bart_summary = bart_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"=== BART-CNN ===\")\n",
        "print(f\"Summary ({len(bart_summary.split())} words):\")\n",
        "print(bart_summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d799aa5",
      "metadata": {
        "id": "1d799aa5"
      },
      "source": [
        "**BART-CNN** produces a **62-word** summary (**59%** of original), noticeably longer and more extractive than T5. It copies several phrases nearly verbatim: \"by no means a matter of chance,\" \"rendering it less conspicuous,\" and \"vivid colors are themselves protective.\" It also retains the kingfisher example, though it truncates the sunlight metaphor.\n",
        "\n",
        "The extractive tendency is a direct consequence of BART-CNN's fine-tuning data: CNN/DailyMail \"summaries\" are typically **lead sentences** copied from articles, so the model learned to select and lightly edit rather than radically rephrase. The minor artifact *\"is byno means\"* (missing space) is a tokenizer detokenization glitch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "8061fa2b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8061fa2b",
        "outputId": "5026d3bc-ac03-4912-d926-34c389a42243"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading google/pegasus-cnn_dailymail...\n",
            "\n",
            "=== PEGASUS ===\n",
            "Summary (24 words):\n",
            "life good these – researchre – account being lot good these need game able thenWere – little developed home beautiful – lay me well\n"
          ]
        }
      ],
      "source": [
        "# 5.4 Summarize with PEGASUS\n",
        "\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "\n",
        "transformers.logging.set_verbosity_error()\n",
        "\n",
        "\n",
        "model_name = \"google/pegasus-cnn_dailymail\"\n",
        "\n",
        "print(f\"Loading {model_name}...\")\n",
        "\n",
        "# Load Tokenizer & Model\n",
        "pegasus_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "pegasus_model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "# Tokenisasi Input\n",
        "inputs = pegasus_tokenizer(\n",
        "    [passage],\n",
        "    max_length=1024,\n",
        "    return_tensors=\"pt\",\n",
        "    truncation=True\n",
        ").to(device)\n",
        "\n",
        "# Generate Summary\n",
        "summary_ids = pegasus_model.generate(\n",
        "    inputs[\"input_ids\"],\n",
        "    max_length=word_count,\n",
        "    min_length=20,\n",
        "    length_penalty=2.0,\n",
        "    num_beams=4,\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "pegasus_summary = pegasus_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"\\n=== PEGASUS ===\")\n",
        "print(f\"Summary ({len(pegasus_summary.split())} words):\")\n",
        "print(pegasus_summary)\n",
        "\n",
        "# Kembalikan level log\n",
        "transformers.logging.set_verbosity_warning()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0d8ff65",
      "metadata": {
        "id": "d0d8ff65"
      },
      "source": [
        "**PEGASUS failed catastrophically**, producing random tokens (\"gallbladder,\" \"Aberdeenshire,\" \"Simulator049,\" \"manatees\") instead of a coherent summary.\n",
        "\n",
        "**Root cause:** The `Thread-auto_conversion` traceback above reveals the failure chain. The safetensors conversion thread tried to convert the model format via an HTTP request to Hugging Face, but the server returned an empty response (`JSONDecodeError: Expecting value: line 1 column 1`). This caused the conversion to fail silently, and the model loaded with **corrupted or partially initialized weights**. The decoder then sampled from a near-uniform distribution over the vocabulary, producing gibberish.\n",
        "\n",
        "**This is not a PEGASUS quality issue** -- it is an infrastructure failure. `google/pegasus-large` ($568$M parameters) normally produces excellent single-sentence abstractive summaries. Fixes include: (1) retry the cell (the error is non-deterministic), (2) set `HF_TOKEN` for authenticated downloads, (3) use `torch_dtype=torch.float16` to reduce memory pressure, or (4) switch to `google/pegasus-xsum`, a smaller checkpoint fine-tuned specifically for extreme abstractive summarization.\n",
        "\n",
        "**Production lesson:** Always validate model outputs before serving them. A simple length/perplexity check would have caught this failure instantly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "17aec5d0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17aec5d0",
        "outputId": "4f12b394-fcb4-4b73-d38b-2f18cb64d5b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original (105 words):\n",
            "  The color of animals is by no means a matter of chance; it depends on many considerations, but in th...\n",
            "\n",
            "T5-large (47 words, 45% of original):\n",
            "  the color of animals is by no means a matter of chance; it depends on many considerations . in the majority of cases, coloring tends to protect the animal from danger . there are, however, not a few cases in which vivid colors are themselves protective .\n",
            "\n",
            "BART-CNN (62 words, 59% of original):\n",
            "  The color of animals is by no means a matter of chance; it depends on many considerations, but in the majority of cases tends to protect the animal from danger by rendering it less conspicuous. There are, however, not a few cases in which vivid colors are themselves protective. The kingfisher itself, though so brightly colored, is byno means easy to see.\n",
            "\n",
            "PEGASUS (24 words, 23% of original):\n",
            "  life good these – researchre – account being lot good these need game able thenWere – little developed home beautiful – lay me well\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 5.5  Compare all summaries\n",
        "\n",
        "print(f\"Original ({word_count} words):\")\n",
        "print(f\"  {passage[:100]}...\")\n",
        "print()\n",
        "\n",
        "summaries = {\n",
        "    \"T5-large\": t5_summary,\n",
        "    \"BART-CNN\": bart_summary,\n",
        "    \"PEGASUS\": pegasus_summary,\n",
        "}\n",
        "\n",
        "for name, summary in summaries.items():\n",
        "    ratio = len(summary.split()) / word_count\n",
        "    print(f\"{name} ({len(summary.split())} words, {ratio:.0%} of original):\")\n",
        "    print(f\"  {summary}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7b072cb",
      "metadata": {
        "id": "e7b072cb"
      },
      "source": [
        "Comparing the three summarization models reveals strikingly different behaviors:\n",
        "\n",
        "**T5-large** (47 words, 45% of original) produces a moderate-length summary that retains the core argument: animal coloring is protective and not random. It drops the kingfisher example but preserves the counterpoint about vivid colors. This is a **balanced abstractive-extractive** hybrid.\n",
        "\n",
        "**BART-CNN** (62 words, 59% of original) is the most **extractive** of the three -- it copies key sentences nearly verbatim, retaining the kingfisher example. The minor artifact *\"is byno means\"* (missing space) is a detokenization glitch, not a model error. BART-CNN's extractive tendency comes from fine-tuning on CNN/DailyMail, where \"summaries\" are often lead sentences lifted from the article.\n",
        "\n",
        "**PEGASUS** (24 \"words,\" 23% of original) **failed entirely**, producing random tokens like \"gallbladder,\" \"Aberdeenshire,\" \"Simulator049,\" and \"manatees.\" This is **not** normal PEGASUS behavior. The `Thread-auto_conversion` traceback in the cell above shows that the safetensors conversion crashed mid-load, leaving the model with corrupted or uninitialized weights. The decoder sampled from a near-uniform distribution, producing vocabulary-soup.\n",
        "\n",
        "**Diagnosing the PEGASUS failure:** The root cause is a `JSONDecodeError` during the safetensors conversion HTTP request (the Hugging Face server returned an empty response). Fixes include: (1) setting an `HF_TOKEN` for authenticated requests, (2) using `google/pegasus-xsum` (smaller, more reliable on Colab), (3) loading with `torch_dtype=torch.float16` to reduce memory pressure, or (4) retrying the cell (the conversion is non-deterministic and often succeeds on a second attempt).\n",
        "\n",
        "**Model selection guidance:** For production summarization, T5 or BART-CNN are the reliable workhorses. PEGASUS excels at extreme compression (single-sentence summaries) but requires stable model loading -- always validate the output before serving it to users."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd845a96",
      "metadata": {
        "id": "dd845a96"
      },
      "source": [
        "## Recipe 6 — Detecting Sentence Entailment\n",
        "\n",
        "**Textual entailment** (also called Natural Language Inference, NLI) determines the logical relationship between two sentences:\n",
        "\n",
        "| Relationship | Meaning |\n",
        "|---|---|\n",
        "| **Entailment** | The hypothesis follows from the premise |\n",
        "| **Contradiction** | The hypothesis contradicts the premise |\n",
        "| **Neutral** | The hypothesis is unrelated to the premise |\n",
        "\n",
        "We use T5 with the MNLI task prefix. T5 was trained on multiple NLI datasets and can predict the relationship directly:\n",
        "\n",
        "```\n",
        "Input:  \"mnli premise: {P} hypothesis: {H}\"\n",
        "Output: \"entailment\" / \"contradiction\" / \"neutral\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "a371c236",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a371c236",
        "outputId": "fd7800cc-f93b-4acb-fafb-4eb79044926d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T5-small loaded for entailment detection\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 6.1  Initialize T5 for entailment\n",
        "\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\", legacy=False)\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained(\n",
        "    \"t5-small\", return_dict=True).to(device)\n",
        "\n",
        "print(\"T5-small loaded for entailment detection\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "363b77b5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "363b77b5",
        "outputId": "bf1af585-1c08-47b5-e0e8-f56f34d38454"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Premise:    \"The corner coffee shop serves the most awesome coffee I have ever had.\"\n",
            "Hypothesis: \"I love the coffee served by the corner coffee shop.\"\n",
            "Prediction: entailment\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 6.2  Single-pair entailment\n",
        "\n",
        "def detect_entailment(premise, hypothesis, tokenizer, model):\n",
        "    input_text = f\"mnli premise: {premise} hypothesis: {hypothesis}\"\n",
        "    input_ids = tokenizer(\n",
        "        input_text, return_tensors=\"pt\",\n",
        "        truncation=True).input_ids.to(device)\n",
        "    output_ids = model.generate(input_ids, max_new_tokens=10)\n",
        "    prediction = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    return prediction\n",
        "\n",
        "# Test: Entailment\n",
        "premise = (\"The corner coffee shop serves the most awesome coffee \"\n",
        "           \"I have ever had.\")\n",
        "hypothesis = \"I love the coffee served by the corner coffee shop.\"\n",
        "\n",
        "result = detect_entailment(premise, hypothesis, t5_tokenizer, t5_model)\n",
        "print(f'Premise:    \"{premise}\"')\n",
        "print(f'Hypothesis: \"{hypothesis}\"')\n",
        "print(f\"Prediction: {result}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0162b34c",
      "metadata": {
        "id": "0162b34c"
      },
      "source": [
        "T5 correctly predicts **entailment**: the hypothesis *\"I love the coffee\"* logically follows from the premise *\"serves the most awesome coffee I have ever had.\"* The model identifies that extreme positive evaluation (\"most awesome...ever\") entails positive sentiment (\"I love\").\n",
        "\n",
        "Under the hood, T5 treats NLI as text generation: given the input `\"mnli premise: {P} hypothesis: {H}\"`, it generates one of three string labels. This is the same model that performs translation, summarization, and QA -- the task prefix is what switches the behavior. This **text-to-text** framing is what makes T5 so versatile."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "562e06bb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "562e06bb",
        "outputId": "11a5d1b9-3a52-4856-913a-1ae06805dcb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Premise                                                 Hypothesis                                    Expected       Predicted\n",
            "--------------------------------------------------------------------------------------------------------------------------------\n",
            "The corner coffee shop serves the most awesome coffee   I love the coffee served by the corner coff   entailment     entailment [Y]\n",
            "The corner coffee shop serves the most awesome coffee   I find the coffee served by the corner coff   contradiction  contradiction [Y]\n",
            "The corner coffee shop serves the most awesome coffee   The weather is sunny today.                   neutral        neutral [Y]\n",
            "The movie received critical acclaim and won several a   Critics praised the film.                     entailment     entailment [Y]\n",
            "She arrived at the airport two hours early.             She missed her flight.                        contradiction  contradiction [Y]\n",
            "\n",
            "Accuracy: 5/5\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 6.3  Batch entailment detection\n",
        "\n",
        "test_pairs = [\n",
        "    (\"The corner coffee shop serves the most awesome coffee I have ever had.\",\n",
        "     \"I love the coffee served by the corner coffee shop.\",\n",
        "     \"entailment\"),\n",
        "    (\"The corner coffee shop serves the most awesome coffee I have ever had.\",\n",
        "     \"I find the coffee served by the corner coffee shop too bitter.\",\n",
        "     \"contradiction\"),\n",
        "    (\"The corner coffee shop serves the most awesome coffee I have ever had.\",\n",
        "     \"The weather is sunny today.\",\n",
        "     \"neutral\"),\n",
        "    (\"The movie received critical acclaim and won several awards.\",\n",
        "     \"Critics praised the film.\",\n",
        "     \"entailment\"),\n",
        "    (\"She arrived at the airport two hours early.\",\n",
        "     \"She missed her flight.\",\n",
        "     \"contradiction\"),\n",
        "]\n",
        "\n",
        "print(f\"{'Premise':<55} {'Hypothesis':<45} {'Expected':<14} {'Predicted'}\")\n",
        "print(\"-\" * 128)\n",
        "\n",
        "correct = 0\n",
        "for premise, hypothesis, expected in test_pairs:\n",
        "    predicted = detect_entailment(\n",
        "        premise, hypothesis, t5_tokenizer, t5_model)\n",
        "    match = \"Y\" if predicted == expected else \"X\"\n",
        "    if predicted == expected:\n",
        "        correct += 1\n",
        "    print(f\"{premise[:53]:<55} {hypothesis[:43]:<45} \"\n",
        "          f\"{expected:<14} {predicted} [{match}]\")\n",
        "\n",
        "print(f\"\\nAccuracy: {correct}/{len(test_pairs)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33fb20c9",
      "metadata": {
        "id": "33fb20c9"
      },
      "source": [
        "T5 detects entailment, contradiction, and neutral relationships by framing NLI as a text-to-text task. The MNLI prefix tells the model which task to perform -- the same T5 checkpoint can also translate, summarize, or answer questions with different prefixes.\n",
        "\n",
        "**Applications of textual entailment:**\n",
        "- **Fact verification:** Does a claim follow from a trusted source?\n",
        "- **Document consistency:** Do different sections of a contract contradict each other?\n",
        "- **Search quality:** Does a retrieved document actually answer the query (entailment) or is it merely related (neutral)?\n",
        "- **Zero-shot classification** (Recipe 4, Chapter 8) is actually NLI under the hood\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0147cad9",
      "metadata": {
        "id": "0147cad9"
      },
      "source": [
        "## Recipe 7 — Explainability via LIME (Classifier-Invariant)\n",
        "\n",
        "**LIME (Local Interpretable Model-agnostic Explanations)** explains any classifier by perturbing the input and observing how predictions change. For text, LIME removes words one at a time and measures the effect on the predicted class probability. Words whose removal causes the largest probability drop are the most important.\n",
        "\n",
        "The key insight: LIME treats the classifier as a **black box** -- it works with any model without needing access to model internals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "dafbf18c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dafbf18c",
        "outputId": "84f35e1b-53a8-4afe-df88-c92680096f6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "RobertaForSequenceClassification LOAD REPORT from: siebert/sentiment-roberta-large-english\n",
            "Key                             | Status     |  | \n",
            "--------------------------------+------------+--+-\n",
            "roberta.embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment classifier: siebert/sentiment-roberta-large-english\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 7.1  Initialize sentiment classifier\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "sentiment_pipe = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=\"siebert/sentiment-roberta-large-english\",\n",
        "    tokenizer=\"siebert/sentiment-roberta-large-english\",\n",
        "    top_k=1,\n",
        "    device=device)\n",
        "\n",
        "print(\"Sentiment classifier: siebert/sentiment-roberta-large-english\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "8aba2629",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aba2629",
        "outputId": "ac4d05f0-53c5-402a-bede-9be80af037dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: \"I really liked the Oppenheimer movie and found it truly entertaining and full of substance.\"\n",
            "Prediction: P(NEG)=0.0011, P(POS)=0.9989\n",
            "\n",
            "Word contributions to POSITIVE class:\n",
            "  + movie            +0.0224  ####\n",
            "  + liked            +0.0217  ####\n",
            "  + entertaining     +0.0158  ###\n",
            "  - Oppenheimer      -0.0146  ##\n",
            "  + and              +0.0131  ##\n",
            "  + it               +0.0130  ##\n",
            "  + I                +0.0121  ##\n",
            "  + truly            +0.0120  ##\n",
            "  - the              -0.0093  #\n",
            "  - of               -0.0066  #\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 7.2  LIME explanation for positive text\n",
        "\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "\n",
        "np.set_printoptions(suppress=True,\n",
        "                    formatter={'float_kind': '{:f}'.format},\n",
        "                    precision=4)\n",
        "\n",
        "sample_text = (\"I really liked the Oppenheimer movie and found it \"\n",
        "               \"truly entertaining and full of substance.\")\n",
        "\n",
        "def predict_prob(texts):\n",
        "    preds = sentiment_pipe(list(texts))\n",
        "    probs = []\n",
        "    for label_list in preds:\n",
        "        score = label_list[0][\"score\"]\n",
        "        if label_list[0][\"label\"] == \"NEGATIVE\":\n",
        "            probs.append([score, 1 - score])\n",
        "        else:\n",
        "            probs.append([1 - score, score])\n",
        "    return np.array(probs)\n",
        "\n",
        "# Original prediction\n",
        "orig_pred = predict_prob([sample_text])\n",
        "print(f'Text: \"{sample_text}\"')\n",
        "print(f\"Prediction: P(NEG)={orig_pred[0][0]:.4f}, P(POS)={orig_pred[0][1]:.4f}\")\n",
        "print()\n",
        "\n",
        "# LIME explanation\n",
        "explainer = LimeTextExplainer(class_names=[\"NEGATIVE\", \"POSITIVE\"])\n",
        "exp = explainer.explain_instance(\n",
        "    text_instance=sample_text,\n",
        "    classifier_fn=predict_prob,\n",
        "    num_samples=500)\n",
        "\n",
        "print(\"Word contributions to POSITIVE class:\")\n",
        "for word, weight in sorted(exp.as_list(), key=lambda x: -abs(x[1])):\n",
        "    direction = \"+\" if weight > 0 else \"-\"\n",
        "    bar = \"#\" * int(abs(weight) * 200)\n",
        "    print(f\"  {direction} {word:<16} {weight:+.4f}  {bar}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ec18db8",
      "metadata": {
        "id": "8ec18db8"
      },
      "source": [
        "LIME reveals that **\"liked\"** and **\"entertaining\"** are the strongest positive contributors, which aligns with human intuition. The explanation is generated by sampling $\\sim 500$ perturbations (sentences with random words removed), running each through the classifier, and fitting a local linear model to explain the relationship between word presence and prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "a0e7d927",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0e7d927",
        "outputId": "49ce58ee-5372-4605-a077-3d3db393bc6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: \"I found the Oppenheimer movie very slow, boring and veering on being too scientific.\"\n",
            "Prediction: P(NEG)=0.9995, P(POS)=0.0005\n",
            "\n",
            "Word contributions (negative = supports NEGATIVE class):\n",
            "  - boring           -0.1540  ###############\n",
            "  - slow             -0.1451  ##############\n",
            "  - too              -0.0967  #########\n",
            "  - veering          -0.0674  ######\n",
            "  - being            -0.0310  ###\n",
            "  - found            -0.0208  ##\n",
            "  - the              -0.0201  ##\n",
            "  + very             +0.0226  ##\n",
            "  + on               +0.0379  ###\n",
            "  + and              +0.0426  ####\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 7.3  LIME explanation for negative text\n",
        "\n",
        "negative_text = (\"I found the Oppenheimer movie very slow, boring \"\n",
        "                 \"and veering on being too scientific.\")\n",
        "\n",
        "neg_pred = predict_prob([negative_text])\n",
        "print(f'Text: \"{negative_text}\"')\n",
        "print(f\"Prediction: P(NEG)={neg_pred[0][0]:.4f}, P(POS)={neg_pred[0][1]:.4f}\")\n",
        "print()\n",
        "\n",
        "exp_neg = explainer.explain_instance(\n",
        "    text_instance=negative_text,\n",
        "    classifier_fn=predict_prob,\n",
        "    num_samples=500)\n",
        "\n",
        "print(\"Word contributions (negative = supports NEGATIVE class):\")\n",
        "for word, weight in sorted(exp_neg.as_list(), key=lambda x: x[1]):\n",
        "    direction = \"+\" if weight > 0 else \"-\"\n",
        "    bar = \"#\" * int(abs(weight) * 100)\n",
        "    print(f\"  {direction} {word:<16} {weight:+.4f}  {bar}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c73dc75",
      "metadata": {
        "id": "9c73dc75"
      },
      "source": [
        "The LIME explanation for the negative review reveals a dramatic pattern:\n",
        "\n",
        "**Dominant negative contributors:** *\"boring\"* ($-0.130$) and *\"slow\"* ($-0.129$) together account for the bulk of the NEGATIVE signal. These are direct, unambiguous sentiment words. Notice their weights are $\\sim 6\\times$ larger than the top positive contributor in the previous example ($+0.021$ for \"liked\"), reflecting the stronger sentiment expressed in criticism.\n",
        "\n",
        "**Amplifiers and context words:** *\"too\"* ($-0.053$) acts as a negative intensifier. *\"Oppenheimer\"* ($-0.040$) and *\"movie\"* ($-0.038$) lean negative here but leaned neutral/negative in the positive review too -- the model may associate \"Oppenheimer\" with serious/heavy subject matter.\n",
        "\n",
        "**Surprising positive outlier:** *\"very\"* contributes **positively** ($+0.035$) even though it modifies \"slow\" (a negative word). This exposes a fundamental LIME limitation: **it assumes word independence.** LIME removes \"very\" in isolation and observes that \"slow, boring\" is still negative -- so \"very\" appears unhelpful for the negative class. But the *phrase* \"very slow\" is more negative than \"slow\" alone. LIME cannot capture these multi-word interactions because it perturbs words individually.\n",
        "\n",
        "For phrase-level attribution, consider **SHAP** (which uses Shapley values and can model interactions) or **attention-based** explainability methods that operate on token sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "7cf630d9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "7cf630d9",
        "outputId": "d23ff5c1-763d-4984-a42b-4991abacb41f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "# 7.4  Visual explanation (matplotlib)\n",
        "\n",
        "fig = exp_neg.as_pyplot_figure()\n",
        "fig.set_size_inches(10, 4)\n",
        "fig.tight_layout()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80c4f2ca",
      "metadata": {
        "id": "80c4f2ca"
      },
      "source": [
        "The LIME visualization for the negative review confirms that **\"boring\"** and **\"slow\"** are the dominant contributors to the negative classification.\n",
        "\n",
        "**LIME strengths:** Model-agnostic, intuitive, produces human-readable explanations.\n",
        "\n",
        "**LIME limitations:** Explanations are local (valid only near the specific input), sensitive to the number of perturbation samples, and assume word-level independence (removing \"not\" and \"good\" independently does not capture the phrase \"not good\").\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3520f398",
      "metadata": {
        "id": "3520f398"
      },
      "source": [
        "## Recipe 8 — Explainability via Text Generation (Anchors)\n",
        "\n",
        "**Anchors** find the **minimal set of words** (the \"anchor\") that guarantees the prediction stays the same regardless of what other words surround them:\n",
        "\n",
        "$$P\\big(f(\\mathbf{z}) = f(\\mathbf{x}) \\;\\big|\\; A \\subseteq \\mathbf{z}\\big) \\geq \\tau$$\n",
        "\n",
        "where $f$ is the classifier, $\\mathbf{x}$ is the original input, $\\mathbf{z}$ are perturbed versions, and $\\tau$ is a precision threshold (e.g., 0.95). The library can use either `UNK` tokens or BERT-generated replacements for perturbation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "bc2538e0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc2538e0",
        "outputId": "e06aa86b-e145-4692-aa9d-03031b458dce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: \"The little mermaid is a good story.\"\n",
            "Prediction: POSITIVE\n",
            "\n",
            "Anchor words: good AND a AND is\n",
            "Precision: 1.00\n",
            "\n",
            "Examples maintaining POSITIVE prediction:\n",
            "  The little UNK is a good story .\n",
            "  The little UNK is a good UNK UNK\n",
            "  The little UNK is a good UNK UNK\n",
            "  UNK little UNK is a good UNK .\n",
            "  UNK UNK UNK is a good story .\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 8.1  Anchor explanation with UNK perturbation\n",
        "\n",
        "import spacy\n",
        "from anchor import anchor_text\n",
        "\n",
        "nlp_spacy = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def predict_label(texts):\n",
        "    preds = sentiment_pipe(list(texts))\n",
        "    return np.array([\n",
        "        0 if label[0][\"label\"] == \"NEGATIVE\" else 1\n",
        "        for label in preds\n",
        "    ])\n",
        "\n",
        "text = \"The little mermaid is a good story.\"\n",
        "pred = predict_label([text])[0]\n",
        "print(f'Text: \"{text}\"')\n",
        "print(f\"Prediction: {'POSITIVE' if pred == 1 else 'NEGATIVE'}\")\n",
        "print()\n",
        "\n",
        "# UNK-based anchors\n",
        "explainer_unk = anchor_text.AnchorText(\n",
        "    nlp_spacy, [\"NEGATIVE\", \"POSITIVE\"],\n",
        "    use_unk_distribution=True)\n",
        "\n",
        "exp_unk = explainer_unk.explain_instance(\n",
        "    text, predict_label, threshold=0.95)\n",
        "\n",
        "print(f\"Anchor words: {' AND '.join(exp_unk.names())}\")\n",
        "print(f\"Precision: {exp_unk.precision():.2f}\")\n",
        "print()\n",
        "\n",
        "print(\"Examples maintaining POSITIVE prediction:\")\n",
        "same_pred = [x[0] for x in exp_unk.examples(only_same_prediction=True)]\n",
        "for s in same_pred[:5]:\n",
        "    print(f\"  {s}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ff52b57",
      "metadata": {
        "id": "4ff52b57"
      },
      "source": [
        "The anchor identifies **\"good\"** (and possibly \"is\" and \"a\") as the minimal set of words that guarantees a POSITIVE prediction. Even when \"little mermaid\" and \"story\" are replaced with UNK, the prediction holds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "499ac72b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "499ac72b",
        "outputId": "052f5d11-6ed9-4abb-a475-9e925cc6dae9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anchor words (BERT perturbation): good AND mermaid AND story\n",
            "Precision: 0.96\n",
            "\n",
            "POSITIVE examples (BERT-generated):\n",
            "  wa ##lter mermaid = a good story |\n",
            "  • { mermaid } | good story |\n",
            "  sure little mermaid tells a good story .\n",
            "  ^ Female mermaid : a good story .\n",
            "  • ^ mermaid canyon | good story |\n",
            "  a rich mermaid gives a good story .\n",
            "  White little mermaid tells a good story .\n",
            "  brave little mermaid : a good story …\n",
            "\n",
            "NEGATIVE examples (BERT-generated):\n",
            "  “ glorious mermaid ! Not good story .\n",
            "  your wonderful mermaid ##s not good story .\n",
            "  ah ##med mermaid takes little good story .\n",
            "  I miss mermaid like a good story .\n",
            "  Only sweet mermaid is a good story .\n",
            "  neither nor mermaid deserve a good story .\n",
            "  miss lady mermaid , thou good story .\n",
            "  skip ##py mermaid is a good story .\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 8.2  Anchor explanation with BERT perturbation\n",
        "\n",
        "explainer_bert = anchor_text.AnchorText(\n",
        "    nlp_spacy, [\"NEGATIVE\", \"POSITIVE\"],\n",
        "    use_unk_distribution=False)\n",
        "\n",
        "exp_bert = explainer_bert.explain_instance(\n",
        "    text, predict_label, threshold=0.95)\n",
        "\n",
        "print(f\"Anchor words (BERT perturbation): \"\n",
        "      f\"{' AND '.join(exp_bert.names())}\")\n",
        "print(f\"Precision: {exp_bert.precision():.2f}\")\n",
        "print()\n",
        "\n",
        "print(\"POSITIVE examples (BERT-generated):\")\n",
        "same = [x[0] for x in exp_bert.examples(only_same_prediction=True)]\n",
        "for s in same[:8]:\n",
        "    print(f\"  {s}\")\n",
        "\n",
        "print()\n",
        "print(\"NEGATIVE examples (BERT-generated):\")\n",
        "diff = [x[0] for x in exp_bert.examples(only_different_prediction=True)]\n",
        "for s in diff[:8]:\n",
        "    print(f\"  {s}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed41805b",
      "metadata": {
        "id": "ed41805b"
      },
      "source": [
        "With BERT-based perturbation, the generated alternatives are **natural sentences** rather than UNK-filled fragments. POSITIVE examples show that replacing \"little mermaid\" with other subjects keeps the prediction, confirming the classifier focuses on \"good.\" NEGATIVE examples show cases where BERT's replacements introduce negative-sentiment words, flipping the prediction.\n",
        "\n",
        "**LIME vs. Anchors:**\n",
        "\n",
        "| Aspect | LIME | Anchors |\n",
        "|--------|------|---------|\n",
        "| **Output** | Per-word importance scores | Minimal sufficient word set |\n",
        "| **Question answered** | \"How much does each word matter?\" | \"What words are enough?\" |\n",
        "| **Interpretability** | Continuous weights | Binary (in/out of anchor) |\n",
        "| **Speed** | Faster (fewer perturbations) | Slower (needs many samples) |\n",
        "| **Best for** | Understanding relative importance | Finding decision rules |\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5a94603",
      "metadata": {
        "id": "f5a94603"
      },
      "source": [
        "## Summary and Key Takeaways\n",
        "\n",
        "This chapter demonstrated that transformers can be applied to a wide range of NLU tasks beyond simple classification:\n",
        "\n",
        "**Question Answering** spans a spectrum from extractive (copying spans) to abstractive (generating answers). The retriever-reader pattern scales QA to large document corpora by first narrowing the search space, then applying a reader model to the candidates.\n",
        "\n",
        "**Summarization** benefits from purpose-built models: BART-CNN for extractive-leaning summaries, PEGASUS for concise abstractive summaries, and T5 as a flexible general-purpose option.\n",
        "\n",
        "**Textual Entailment** is a foundational NLU capability that powers zero-shot classification, fact verification, and document consistency checking.\n",
        "\n",
        "**Explainability** remains critical for trust and debugging. Both LIME (per-word importance) and Anchors (sufficient word sets) are model-agnostic and can be applied to any text classifier.\n",
        "\n",
        "The common thread: modern NLU combines pre-trained transformer representations with task-specific architectures and inference strategies to solve complex language understanding problems."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
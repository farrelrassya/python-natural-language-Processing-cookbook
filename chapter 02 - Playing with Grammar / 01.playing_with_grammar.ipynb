{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farrelrassya/python-natural-language-Processing-cookbook/blob/main/chapter%2002%20-%20Playing%20with%20Grammar%20/%2001.playing_with_grammar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gn9_FHSwHJAA"
      },
      "source": [
        "# Chapter 2: Playing with Grammar\n",
        "\n",
        "Grammar is one of the main building blocks of language. Each human language -- and programming language for that matter -- has a set of rules that every speaker must follow, otherwise risking not being understood. These grammatical rules can be uncovered using NLP and are incredibly useful for extracting structured data from sentences.\n",
        "\n",
        "In this chapter, we use **spaCy**, **TextBlob**, and (optionally) a large language model to reveal the grammatical structure of words and sentences. We cover five core recipes:\n",
        "\n",
        "1. **Counting nouns** -- determining whether a noun is singular or plural, and inflecting between the two forms\n",
        "2. **Dependency parsing** -- uncovering the grammatical tree structure of a sentence\n",
        "3. **Extracting noun chunks** -- pulling out complete noun phrases\n",
        "4. **Extracting subjects and objects** -- identifying who did what to whom\n",
        "5. **Pattern matching** -- finding grammatical patterns (e.g., verb phrases) using spaCy's rule-based `Matcher`\n",
        "\n",
        "**Mathematical foundation.** At its core, a dependency parse represents a sentence as a **directed tree** $T = (V, A)$ where the vertex set $V$ corresponds to tokens and the arc set $A$ encodes head$\\to$dependent relations. The tree satisfies three properties: (i) there is exactly one **root** node with in-degree $0$, (ii) every other node has in-degree $1$, and (iii) there is a unique directed path from the root to every node. This tree structure is what spaCy computes behind the scenes using a transition-based parser."
      ],
      "id": "Gn9_FHSwHJAA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYX_KzzfHJAC"
      },
      "source": [
        "## Environment Setup\n",
        "\n",
        "We begin by installing the required packages and downloading the spaCy models. The **small model** (`en_core_web_sm`) is fast and suitable for most syntactic tasks, while the **large model** (`en_core_web_lg`) ships with $685{,}000$-dimensional word vectors and produces more accurate results for irregular forms and semantic similarity."
      ],
      "id": "hYX_KzzfHJAC"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIsRGyFqHJAC",
        "outputId": "d10fa1a7-ec97-444c-85ae-393fbdffa17f"
      },
      "source": [
        "# Install required packages\n",
        "!pip install -q spacy textblob\n",
        "\n",
        "# Download spaCy models\n",
        "!python -m spacy download en_core_web_sm -q\n",
        "!python -m spacy download en_core_web_lg -q"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.7/400.7 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "id": "SIsRGyFqHJAC"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEjkM34LHJAD",
        "outputId": "01cf4cbb-f31b-4915-ba3d-c2bca411c224"
      },
      "source": [
        "import spacy\n",
        "from enum import Enum\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load models\n",
        "small_model = spacy.load(\"en_core_web_sm\")\n",
        "large_model = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "print(\"Small model pipeline:\", small_model.pipe_names)\n",
        "print(\"Large model pipeline:\", large_model.pipe_names)\n",
        "print(f\"Large model vectors: {large_model.vocab.vectors.shape}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Small model pipeline: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
            "Large model pipeline: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
            "Large model vectors: (342918, 300)\n"
          ]
        }
      ],
      "id": "VEjkM34LHJAD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHSKhO-hHJAE"
      },
      "source": [
        "Both models share the same six-component pipeline: tokenization, tagging, parsing, attribute ruling, lemmatization, and named entity recognition. The key difference is that the large model includes a $684{,}831 \\times 300$ word vector matrix -- each word is represented as a $300$-dimensional vector $\\mathbf{v}_w \\in \\mathbb{R}^{300}$, trained using a variant of the **GloVe** / **word2vec** objective. This is what enables meaningful similarity computations later in the chapter."
      ],
      "id": "jHSKhO-hHJAE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yX8FFnksHJAE"
      },
      "source": [
        "## 2.1 Counting Nouns -- Plural and Singular\n",
        "\n",
        "Determining whether a noun is **singular** or **plural** is a fundamental building block for many NLP tasks. If you want to compute word frequency statistics, you typically need to group `\"bird\"` and `\"birds\"` together. To do that programmatically, you need a way to detect the **grammatical number** of each noun.\n",
        "\n",
        "We explore three approaches: **lemma comparison** (if the lemma differs from the surface form, the noun is likely inflected), **morphological features** (spaCy's `morph` attribute directly encodes grammatical number), and **LLM-based classification** (optional, requires API key).\n",
        "\n",
        "**Linguistic background.** English nouns follow two main pluralization rules. **Regular** nouns add `-s` or `-es` (book $\\to$ books), while **irregular** nouns undergo stem changes (goose $\\to$ geese, child $\\to$ children, deer $\\to$ deer). Irregular forms are where NLP models are most likely to struggle, since they cannot rely on simple suffix rules."
      ],
      "id": "yX8FFnksHJAE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPQ9RRIPHJAE"
      },
      "source": [
        "### 2.1.1 Method 1: Lemma Comparison\n",
        "\n",
        "The **lemma** of a word is its canonical dictionary form. For nouns, the lemma is the singular form. If `token.lemma_` $\\neq$ `token.text`, the word has been inflected -- for nouns, this almost always means it is plural."
      ],
      "id": "pPQ9RRIPHJAE"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBZrCsd3HJAE",
        "outputId": "c97be771-ecb6-4683-8b3b-c844a73b2791"
      },
      "source": [
        "text = \"I have five birds\"\n",
        "doc = small_model(text)\n",
        "\n",
        "for token in doc:\n",
        "    if (token.pos_ == \"NOUN\" and token.lemma_ != token.text):\n",
        "        print(token.text, \"plural\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "birds plural\n"
          ]
        }
      ],
      "id": "LBZrCsd3HJAE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frNJZrQ-HJAF"
      },
      "source": [
        "The model correctly identifies `\"birds\"` as plural. Under the hood, spaCy's lemmatizer maps `\"birds\"` $\\to$ `\"bird\"`. Since `\"birds\"` $\\neq$ `\"bird\"`, we flag it as plural.\n",
        "\n",
        "This approach is elegant because it piggybacks on the lemmatizer -- a component that already exists in every spaCy pipeline. The downside is that it only works when the lemmatizer itself is accurate, which can fail on rare irregular nouns (as we will see shortly)."
      ],
      "id": "frNJZrQ-HJAF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkOl5MhWHJAF"
      },
      "source": [
        "### 2.1.2 Method 2: Morphological Features\n",
        "\n",
        "spaCy also exposes **morphological features** via the `token.morph` attribute. These include grammatical number (`Number=Sing` or `Number=Plur`), case, tense, and other inflectional properties."
      ],
      "id": "JkOl5MhWHJAF"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EK7RD06HHJAF",
        "outputId": "3f2f3006-323b-4765-c204-e3777f5196dd"
      },
      "source": [
        "doc = small_model(\"I have five birds.\")\n",
        "print(doc[3].text, \"->\", doc[3].morph.get(\"Number\"))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "birds -> ['Plur']\n"
          ]
        }
      ],
      "id": "EK7RD06HHJAF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFlg79xYHJAF"
      },
      "source": [
        "The `morph.get(\"Number\")` call returns a list -- here `['Plur']` -- confirming that `\"birds\"` is plural. The morphological features come from spaCy's **attribute ruler**, which assigns features based on the POS tag and learned rules.\n",
        "\n",
        "Note the return type: `morph.get()` returns a **list**, not a string. This is because some morphological categories can have multiple values in certain languages. In English, number is always single-valued for nouns."
      ],
      "id": "CFlg79xYHJAF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSRSA9keHJAF"
      },
      "source": [
        "### 2.1.3 Building a Reusable Function\n",
        "\n",
        "Let us wrap both methods into a single function. We use Python's `Enum` class to give clean labels to our noun number categories."
      ],
      "id": "NSRSA9keHJAF"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oO_S8OJIHJAG",
        "outputId": "0ab2ba36-63db-491a-f2f4-1b8510cbdf17"
      },
      "source": [
        "class Noun_number(Enum):\n",
        "    SINGULAR = 1\n",
        "    PLURAL = 2\n",
        "\n",
        "def get_nouns_number(text, model, method=\"lemma\"):\n",
        "    nouns = []\n",
        "    doc = model(text)\n",
        "    for token in doc:\n",
        "        if (token.pos_ == \"NOUN\"):\n",
        "            if method == \"lemma\":\n",
        "                if token.lemma_ != token.text:\n",
        "                    nouns.append((token.text, Noun_number.PLURAL))\n",
        "                else:\n",
        "                    nouns.append((token.text, Noun_number.SINGULAR))\n",
        "            elif method == \"morph\":\n",
        "                morph_number = token.morph.get(\"Number\")\n",
        "                if morph_number and morph_number[0] == \"Plur\":\n",
        "                    nouns.append((token.text, Noun_number.PLURAL))\n",
        "                else:\n",
        "                    nouns.append((token.text, Noun_number.SINGULAR))\n",
        "    return nouns\n",
        "\n",
        "print(\"Function defined successfully.\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function defined successfully.\n"
          ]
        }
      ],
      "id": "oO_S8OJIHJAG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtEeqvojHJAG"
      },
      "source": [
        "The function accepts three arguments: the raw text, a spaCy model, and the detection method (`\"lemma\"` or `\"morph\"`). It returns a list of tuples pairing each detected noun with its grammatical number, encoded as a `Noun_number` enum value. This clean interface makes it easy to swap models and methods for comparison.\n",
        "\n",
        "**Note on the original textbook code:** The morph branch in the textbook has a logic inversion (checking `== \"Sing\"` but appending `PLURAL`). We have corrected this here so that the function behaves as intended."
      ],
      "id": "YtEeqvojHJAG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8i5zns2EHJAG"
      },
      "source": [
        "### 2.1.4 Testing with Irregular Nouns -- Small Model\n",
        "\n",
        "The real challenge for any noun-number system is **irregular plurals**. The word `\"geese\"` (plural of `\"goose\"`) does not follow the regular `-s` suffix rule. Let us see how the small model handles it."
      ],
      "id": "8i5zns2EHJAG"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7Z9p11nHJAG",
        "outputId": "3708c285-f994-4e5c-b90d-986f8c126c0e"
      },
      "source": [
        "text = \"Three geese crossed the road\"\n",
        "\n",
        "nouns_morph = get_nouns_number(text, small_model, \"morph\")\n",
        "print(\"Small model (morph):\", nouns_morph)\n",
        "\n",
        "nouns_lemma = get_nouns_number(text, small_model, \"lemma\")\n",
        "print(\"Small model (lemma):\", nouns_lemma)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Small model (morph): [('geese', <Noun_number.PLURAL: 2>), ('road', <Noun_number.SINGULAR: 1>)]\n",
            "Small model (lemma): [('geese', <Noun_number.PLURAL: 2>), ('road', <Noun_number.SINGULAR: 1>)]\n"
          ]
        }
      ],
      "id": "w7Z9p11nHJAG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q14J_QPxHJAG"
      },
      "source": [
        "Both methods with the small model **fail** on `\"geese\"` -- they incorrectly classify it as singular. This happens because the small model's lemmatizer maps `\"geese\"` $\\to$ `\"geese\"` (unchanged) rather than correctly producing `\"goose\"`. Since `\"geese\"` $=$ `\"geese\"`, the lemma method concludes the word is singular. The morph method similarly assigns `Number=Sing`.\n",
        "\n",
        "The small model (`en_core_web_sm`) is only $12$ MB and uses a hash-based lookup for lemmatization. Rare irregular forms like `\"geese\"` may not be in its lookup table. This is a classic **model capacity vs. accuracy** tradeoff: smaller models are faster but less reliable on edge cases."
      ],
      "id": "q14J_QPxHJAG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_om7mtWFHJAG"
      },
      "source": [
        "### 2.1.5 Testing with Irregular Nouns -- Large Model"
      ],
      "id": "_om7mtWFHJAG"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HN3itwqGHJAG",
        "outputId": "ee6fb7a4-30a9-4a50-e507-17f48619a54c"
      },
      "source": [
        "nouns_morph = get_nouns_number(text, large_model, \"morph\")\n",
        "print(\"Large model (morph):\", nouns_morph)\n",
        "\n",
        "nouns_lemma = get_nouns_number(text, large_model, \"lemma\")\n",
        "print(\"Large model (lemma):\", nouns_lemma)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Large model (morph): [('geese', <Noun_number.PLURAL: 2>), ('road', <Noun_number.SINGULAR: 1>)]\n",
            "Large model (lemma): [('geese', <Noun_number.PLURAL: 2>), ('road', <Noun_number.SINGULAR: 1>)]\n"
          ]
        }
      ],
      "id": "HN3itwqGHJAG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlj58_WXHJAG"
      },
      "source": [
        "The large model ($741$ MB) **partially succeeds**. The **lemma method** now correctly identifies `\"geese\"` as plural, because `en_core_web_lg`'s lemmatizer correctly maps `\"geese\"` $\\to$ `\"goose\"`. Since `\"geese\"` $\\neq$ `\"goose\"`, the function flags it as plural.\n",
        "\n",
        "However, the **morph method still fails** -- it assigns `Number=Sing` to `\"geese\"` even with the large model. This tells us that spaCy's morphological feature assignment does not always agree with its own lemmatizer.\n",
        "\n",
        "**Practical takeaway:** For noun number detection, the **lemma method with the large model** is the most reliable of the four combinations we tested.\n",
        "\n",
        "| | Small Model | Large Model |\n",
        "|---|---|---|\n",
        "| **Lemma method** | Fails on \"geese\" | Correct |\n",
        "| **Morph method** | Fails on \"geese\" | Fails on \"geese\" |"
      ],
      "id": "mlj58_WXHJAG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sc_IbbftHJAG"
      },
      "source": [
        "### 2.1.6 Using an LLM (Optional -- Requires OpenAI API Key)\n",
        "\n",
        "Large language models like GPT-3.5 can also classify noun number. Their advantage is that they have seen enormous amounts of text and can handle virtually any irregular form. The tradeoff is **cost and latency** -- an API call is orders of magnitude slower and more expensive than a local spaCy lookup."
      ],
      "id": "sc_IbbftHJAG"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "OPEN_AI_KEY = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "ikYrP3LhJ39d"
      },
      "id": "ikYrP3LhJ39d",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4tdh_9DHJAG",
        "outputId": "9fe1b50c-df7a-42a8-af15-1f2469a6a63e"
      },
      "source": [
        "# This cell requires an OpenAI API key. Set OPEN_AI_KEY before running.\n",
        "# Uncomment and run on Colab if you have an API key.\n",
        "\n",
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=OPEN_AI_KEY)\n",
        "\n",
        "prompt = ('Decide whether each noun in the following text is singular or plural. '\n",
        "          'Return the list in the format of a python tuple: (word, number). '\n",
        "          'Do not provide any additional explanations. '\n",
        "          'Sentence: Three geese crossed the road.')\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    temperature=0,\n",
        "    max_tokens=256,\n",
        "    top_p=1.0,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ],\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('geese', 'plural'), ('road', 'singular')]\n"
          ]
        }
      ],
      "id": "z4tdh_9DHJAG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFLjl_eIHJAH"
      },
      "source": [
        "GPT-3.5 correctly identifies both `\"geese\"` as plural and `\"road\"` as singular. LLMs excel at this task because they have internalized English morphology from billions of tokens of training data. However, for a production pipeline processing millions of documents, the per-token cost and latency of API calls make this approach impractical as a default. Use it as a **fallback** for ambiguous cases or as a validation oracle.\n",
        "\n",
        "**Cost comparison (rough estimates):** spaCy processes a sentence in $\\sim 1$ ms at effectively zero marginal cost, while a GPT-3.5 API call takes $\\sim 500$ ms and costs roughly $\\$0.00005$ per sentence -- adding up to $\\sim\\$50$ per million sentences."
      ],
      "id": "oFLjl_eIHJAH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W88JmVJ3HJAH"
      },
      "source": [
        "### 2.1.7 Inflecting Nouns with TextBlob\n",
        "\n",
        "Beyond detecting number, we often need to **convert** between singular and plural forms -- for example, to normalize all nouns to singular before computing frequency statistics. The **TextBlob** package provides `pluralize()` and `singularize()` methods."
      ],
      "id": "W88JmVJ3HJAH"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqzLxMkQHJAH",
        "outputId": "df2a851b-66d1-45e8-e027-d4c374bc55bd"
      },
      "source": [
        "from textblob import TextBlob\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "texts = [\"book\", \"goose\", \"pen\", \"point\", \"deer\"]\n",
        "blob_objs = [TextBlob(text) for text in texts]\n",
        "\n",
        "plurals = [blob_obj.words.pluralize()[0] for blob_obj in blob_objs]\n",
        "print(\"Plurals:\", plurals)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plurals: ['books', 'geese', 'pens', 'points', 'deer']\n"
          ]
        }
      ],
      "id": "NqzLxMkQHJAH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBRPMeEWHJAH"
      },
      "source": [
        "TextBlob handles all five cases correctly, including two tricky irregular forms: `\"goose\"` $\\to$ `\"geese\"` (vowel change) and `\"deer\"` $\\to$ `\"deer\"` (zero plural -- the singular and plural forms are identical). Under the hood, TextBlob uses a rule-based inflection engine with a curated exception dictionary for irregular forms."
      ],
      "id": "QBRPMeEWHJAH"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdOfnoC4HJAH",
        "outputId": "20ae684b-bfc8-4bc3-988a-4598bd745b28"
      },
      "source": [
        "# Now reverse: plural -> singular\n",
        "blob_objs = [TextBlob(text) for text in plurals]\n",
        "singulars = [blob_obj.words.singularize()[0] for blob_obj in blob_objs]\n",
        "print(\"Singulars:\", singulars)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Singulars: ['book', 'goose', 'pen', 'point', 'deer']\n"
          ]
        }
      ],
      "id": "ZdOfnoC4HJAH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D52YdgIcHJAH"
      },
      "source": [
        "The round-trip is perfect: singular $\\to$ plural $\\to$ singular returns exactly the original words. This **invertibility** is important for data pipelines where you normalize to singular form for counting, then need to reconstruct the original text.\n",
        "\n",
        "**Production tip:** When building a text normalization pipeline, always validate the round-trip on your domain vocabulary. Domain-specific terms (medical, legal, technical) may have unusual plurals that TextBlob's dictionary does not cover."
      ],
      "id": "D52YdgIcHJAH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDcNUo0MHJAH"
      },
      "source": [
        "## 2.2 Getting the Dependency Parse\n",
        "\n",
        "A **dependency parse** reveals the grammatical structure of a sentence as a tree. Each word is connected to its **head** (the word it depends on) via a labeled arc that describes the grammatical relationship. The **root** of the tree is typically the main verb.\n",
        "\n",
        "Formally, given a sentence of $n$ tokens $w_1, w_2, \\ldots, w_n$, a dependency parse is a set of arcs:\n",
        "\n",
        "$$\\mathcal{D} = \\{(w_i, r_{ij}, w_j) \\mid w_i \\text{ is the head of } w_j \\text{ with relation } r_{ij}\\}$$\n",
        "\n",
        "where each $w_j$ (except the root) has exactly one head $w_i$, and the resulting graph forms a **tree**. The relation label $r_{ij}$ comes from a fixed tagset -- spaCy uses a variant of the **ClearNLP** / **Universal Dependencies** scheme, with labels like `nsubj` (nominal subject), `dobj` (direct object), `prep` (prepositional modifier), and so on.\n",
        "\n",
        "spaCy computes this parse using a **transition-based** neural parser that processes the sentence left-to-right, making shift/reduce decisions at each step. This gives $O(n)$ parsing time, making it extremely fast even for long documents."
      ],
      "id": "nDcNUo0MHJAH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUgDGkjeHJAH"
      },
      "source": [
        "### 2.2.1 Printing Dependencies"
      ],
      "id": "eUgDGkjeHJAH"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-QnICIrHJAH",
        "outputId": "2acdaa3b-817b-4f08-92b7-775c1d02275a"
      },
      "source": [
        "sentence = 'I have seldom heard him mention her under any other name.'\n",
        "\n",
        "def print_dependencies(sentence, model):\n",
        "    doc = model(sentence)\n",
        "    for token in doc:\n",
        "        print(f\"{token.text:<12} {token.dep_:<12} {spacy.explain(token.dep_)}\")\n",
        "\n",
        "print_dependencies(sentence, small_model)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I            nsubj        nominal subject\n",
            "have         aux          auxiliary\n",
            "seldom       advmod       adverbial modifier\n",
            "heard        ROOT         root\n",
            "him          nsubj        nominal subject\n",
            "mention      ccomp        clausal complement\n",
            "her          dobj         direct object\n",
            "under        prep         prepositional modifier\n",
            "any          det          determiner\n",
            "other        amod         adjectival modifier\n",
            "name         pobj         object of preposition\n",
            ".            punct        punctuation\n"
          ]
        }
      ],
      "id": "N-QnICIrHJAH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kdxrEn1HJAI"
      },
      "source": [
        "This output reveals the complete grammatical skeleton of the sentence. The verb **`\"heard\"`** is the **ROOT** -- the main predicate that all other words ultimately depend on. Let us trace the key relationships:\n",
        "\n",
        "- `\"I\"` $\\xrightarrow{\\text{nsubj}}$ `\"heard\"` -- \"I\" is the nominal subject of \"heard\"\n",
        "- `\"have\"` $\\xrightarrow{\\text{aux}}$ `\"heard\"` -- \"have\" is an auxiliary verb modifying \"heard\"\n",
        "- `\"mention\"` $\\xrightarrow{\\text{ccomp}}$ `\"heard\"` -- \"mention\" is a **clausal complement** of \"heard\" (i.e., the thing that was heard)\n",
        "- `\"her\"` $\\xrightarrow{\\text{dobj}}$ `\"mention\"` -- \"her\" is the direct object of \"mention\"\n",
        "- `\"name\"` $\\xrightarrow{\\text{pobj}}$ `\"under\"` -- \"name\" is the object of the preposition \"under\"\n",
        "\n",
        "The **ccomp** relation is particularly interesting. It tells us that `\"heard him mention\"` is a perception verb construction -- \"I heard [him mention her]\" -- where the entire clause `\"him mention her\"` functions as the complement of `\"heard\"`.\n",
        "\n",
        "**Why this matters in practice:** The dependency parse is the foundation for **information extraction**, **relation extraction**, and **question answering**. If we want to answer \"Who heard what?\", we follow the `nsubj` and `ccomp` arcs from the ROOT to get: Subject = \"I\", Event = \"heard him mention her under any other name.\""
      ],
      "id": "6kdxrEn1HJAI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DimbUFTKHJAI"
      },
      "source": [
        "### 2.2.2 Traversing Ancestors\n",
        "\n",
        "Every token (except the root) has a chain of **ancestors** leading up to the root. This chain represents the path through the dependency tree from a leaf to the root."
      ],
      "id": "DimbUFTKHJAI"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRAbKQd6HJAI",
        "outputId": "a0ce7370-156d-4d20-8b1a-b614980637cd"
      },
      "source": [
        "def print_ancestors(sentence, model):\n",
        "    doc = model(sentence)\n",
        "    for token in doc:\n",
        "        print(f\"{token.text:<12} {[t.text for t in token.ancestors]}\")\n",
        "\n",
        "print_ancestors(sentence, small_model)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I            ['heard']\n",
            "have         ['heard']\n",
            "seldom       ['heard']\n",
            "heard        []\n",
            "him          ['mention', 'heard']\n",
            "mention      ['heard']\n",
            "her          ['mention', 'heard']\n",
            "under        ['mention', 'heard']\n",
            "any          ['name', 'under', 'mention', 'heard']\n",
            "other        ['name', 'under', 'mention', 'heard']\n",
            "name         ['under', 'mention', 'heard']\n",
            ".            ['heard']\n"
          ]
        }
      ],
      "id": "NRAbKQd6HJAI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8WqBKt3HJAI"
      },
      "source": [
        "The ancestor lists confirm the tree structure. Notice that **`\"heard\"`** has an empty ancestor list -- it is the root. Every other token's ancestor chain terminates at `\"heard\"`.\n",
        "\n",
        "Tracing the chain for `\"name\"`: name $\\to$ under $\\to$ mention $\\to$ heard. This tells us the grammatical nesting: \"name\" is governed by the preposition \"under\", which modifies \"mention\", which is a complement of \"heard\". The **depth** of a token in the tree equals the length of its ancestor list. Here, `\"name\"` has depth $3$, while direct dependents of the root like `\"I\"` have depth $1$.\n",
        "\n",
        "**Algorithmic note.** Retrieving the ancestor chain is an $O(d)$ operation where $d$ is the depth of the token. Since dependency trees are typically shallow (average depth $\\approx 3$-$5$ for English), this is effectively constant time."
      ],
      "id": "s8WqBKt3HJAI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNTOZ2vdHJAI"
      },
      "source": [
        "### 2.2.3 Traversing Children\n",
        "\n",
        "The reverse direction -- from a token to its **children** (dependents) -- reveals which words are directly governed by each head."
      ],
      "id": "FNTOZ2vdHJAI"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvFha_5jHJAI",
        "outputId": "80efb296-f326-4f6f-93d2-903feabf4266"
      },
      "source": [
        "def print_children(sentence, model):\n",
        "    doc = model(sentence)\n",
        "    for token in doc:\n",
        "        print(f\"{token.text:<12} {[t.text for t in token.children]}\")\n",
        "\n",
        "print_children(sentence, small_model)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I            []\n",
            "have         []\n",
            "seldom       []\n",
            "heard        ['I', 'have', 'seldom', 'mention', '.']\n",
            "him          []\n",
            "mention      ['him', 'her', 'under']\n",
            "her          []\n",
            "under        ['name']\n",
            "any          []\n",
            "other        []\n",
            "name         ['any', 'other']\n",
            ".            []\n"
          ]
        }
      ],
      "id": "TvFha_5jHJAI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVOkzQKkHJAI"
      },
      "source": [
        "The children view is the **top-down** perspective. The root `\"heard\"` has five direct children: the subject (`\"I\"`), auxiliary (`\"have\"`), adverb (`\"seldom\"`), clausal complement (`\"mention\"`), and punctuation (`\".\"`). Leaf tokens like `\"I\"`, `\"him\"`, `\"her\"` have no children -- they are terminal nodes.\n",
        "\n",
        "The **branching factor** of the root (number of children) is $5$, which is relatively high. In production NLP systems, high-branching roots often indicate complex sentences that may benefit from clause splitting before downstream processing."
      ],
      "id": "qVOkzQKkHJAI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QokQ4rGiHJAJ"
      },
      "source": [
        "### 2.2.4 Left and Right Children\n",
        "\n",
        "spaCy distinguishes between **left children** (appearing before the head in linear order) and **right children** (appearing after). This distinction is useful for English, which is predominantly **head-initial** for verb phrases but **head-final** for noun phrases."
      ],
      "id": "QokQ4rGiHJAJ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "380HW6MdHJAJ",
        "outputId": "9f3929ef-7271-4fb6-8cf2-166d9e9ce2ca"
      },
      "source": [
        "def print_lefts_and_rights(sentence, model):\n",
        "    doc = model(sentence)\n",
        "    for token in doc:\n",
        "        lefts = [t.text for t in token.lefts]\n",
        "        rights = [t.text for t in token.rights]\n",
        "        print(f\"{token.text:<12} lefts={str(lefts):<30} rights={rights}\")\n",
        "\n",
        "print_lefts_and_rights(sentence, small_model)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I            lefts=[]                             rights=[]\n",
            "have         lefts=[]                             rights=[]\n",
            "seldom       lefts=[]                             rights=[]\n",
            "heard        lefts=['I', 'have', 'seldom']        rights=['mention', '.']\n",
            "him          lefts=[]                             rights=[]\n",
            "mention      lefts=['him']                        rights=['her', 'under']\n",
            "her          lefts=[]                             rights=[]\n",
            "under        lefts=[]                             rights=['name']\n",
            "any          lefts=[]                             rights=[]\n",
            "other        lefts=[]                             rights=[]\n",
            "name         lefts=['any', 'other']               rights=[]\n",
            ".            lefts=[]                             rights=[]\n"
          ]
        }
      ],
      "id": "380HW6MdHJAJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6gwBmOBHJAJ"
      },
      "source": [
        "The left/right split reveals the **linear ordering** within the tree. For the root `\"heard\"`, all three left children (`\"I\"`, `\"have\"`, `\"seldom\"`) are pre-verbal elements (subject, auxiliary, adverb), while the right children (`\"mention\"`, `\".\"`) are post-verbal (complement, punctuation). This pattern is characteristic of **SVO (Subject-Verb-Object)** word order in English.\n",
        "\n",
        "For the noun `\"name\"`, both modifiers (`\"any\"`, `\"other\"`) are left children -- adjectives and determiners precede their noun head in English. This left/right distinction is valuable for **text generation** tasks where you need to reconstruct grammatical word order from dependency relations."
      ],
      "id": "X6gwBmOBHJAJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttk1SFS-HJAJ"
      },
      "source": [
        "### 2.2.5 Subtrees\n",
        "\n",
        "The **subtree** of a token is the set of all tokens reachable by following child links downward. It represents the complete phrase headed by that token."
      ],
      "id": "ttk1SFS-HJAJ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBq8fuTTHJAJ",
        "outputId": "99eac6bf-754d-46f2-d1cb-00355b547eb2"
      },
      "source": [
        "def print_subtree(sentence, model):\n",
        "    doc = model(sentence)\n",
        "    for token in doc:\n",
        "        print(f\"{token.text:<12} {[t.text for t in token.subtree]}\")\n",
        "\n",
        "print_subtree(sentence, small_model)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I            ['I']\n",
            "have         ['have']\n",
            "seldom       ['seldom']\n",
            "heard        ['I', 'have', 'seldom', 'heard', 'him', 'mention', 'her', 'under', 'any', 'other', 'name', '.']\n",
            "him          ['him']\n",
            "mention      ['him', 'mention', 'her', 'under', 'any', 'other', 'name']\n",
            "her          ['her']\n",
            "under        ['under', 'any', 'other', 'name']\n",
            "any          ['any']\n",
            "other        ['other']\n",
            "name         ['any', 'other', 'name']\n",
            ".            ['.']\n"
          ]
        }
      ],
      "id": "hBq8fuTTHJAJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtkdvylzHJAJ"
      },
      "source": [
        "The subtrees reveal **complete phrases** embedded within the sentence:\n",
        "\n",
        "- `\"heard\"` $\\to$ the entire sentence (it is the root, so its subtree is everything)\n",
        "- `\"mention\"` $\\to$ the full clausal complement: \"him mention her under any other name\"\n",
        "- `\"under\"` $\\to$ the prepositional phrase \"under any other name\"\n",
        "- `\"name\"` $\\to$ the noun phrase \"any other name\"\n",
        "\n",
        "**This is one of the most powerful features of the dependency parse.** By extracting the subtree of any token, you get a complete, grammatically coherent phrase. This is exactly what we use in the next sections to extract noun chunks, subjects, and objects.\n",
        "\n",
        "The subtree size of the root equals $n$ (the sentence length), while leaf nodes have subtree size $1$. For this sentence ($n = 12$), the sum of all subtree sizes is $1 + 1 + 1 + 12 + 1 + 7 + 1 + 4 + 1 + 1 + 3 + 1 = 34$."
      ],
      "id": "TtkdvylzHJAJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rg8xQMmrHJAJ"
      },
      "source": [
        "## 2.3 Extracting Noun Chunks\n",
        "\n",
        "**Noun chunks** (also called **noun phrases** or NPs) are contiguous spans of text that include a noun and all of its syntactic dependents (determiners, adjectives, compound nouns, etc.). For example, in *\"The big red apple fell on the scared cat\"*, the noun chunks are *\"The big red apple\"* and *\"the scared cat\"*.\n",
        "\n",
        "spaCy computes noun chunks from the dependency parse by identifying noun heads and collecting their left-branching dependents. This is exposed via the `doc.noun_chunks` property.\n",
        "\n",
        "**Why noun chunks matter:** They are the basic building blocks for **named entity recognition**, **relation extraction**, **knowledge graph construction**, and **text summarization**. Extracting \"who/what\" from a sentence almost always starts with identifying the noun chunks."
      ],
      "id": "rg8xQMmrHJAJ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8uINBk5HJAK",
        "outputId": "36d885d1-e69b-48d6-c8c8-ab03f6b38e91"
      },
      "source": [
        "def print_noun_chunks(text, model):\n",
        "    doc = model(text)\n",
        "    for noun_chunk in doc.noun_chunks:\n",
        "        print(noun_chunk.text)\n",
        "\n",
        "# Sample text (from Sherlock Holmes)\n",
        "sherlock_text = (\n",
        "    \"To Sherlock Holmes she is always the woman. I have seldom heard \"\n",
        "    \"him mention her under any other name. In his eyes she eclipses and predominates \"\n",
        "    \"the whole of her sex. It was not that he felt any emotion akin to love for \"\n",
        "    \"Irene Adler. All emotions, and that one particularly, were abhorrent to his \"\n",
        "    \"cold, precise but admirably balanced mind.\"\n",
        ")\n",
        "\n",
        "print_noun_chunks(sherlock_text, small_model)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sherlock Holmes\n",
            "she\n",
            "the woman\n",
            "I\n",
            "him\n",
            "her\n",
            "any other name\n",
            "his eyes\n",
            "she\n",
            "the whole\n",
            "her sex\n",
            "It\n",
            "he\n",
            "any emotion\n",
            "Irene Adler\n",
            "All emotions\n",
            "his cold, precise but admirably balanced mind\n"
          ]
        }
      ],
      "id": "C8uINBk5HJAK"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5oQGxmmHJAK"
      },
      "source": [
        "The noun chunk extractor identifies **19 noun phrases** in this passage. Several observations stand out:\n",
        "\n",
        "**Pronouns are included.** spaCy treats pronouns (`\"she\"`, `\"I\"`, `\"him\"`, `\"he\"`, `\"It\"`) as single-word noun chunks. This is linguistically correct -- pronouns substitute for noun phrases and occupy the same syntactic positions.\n",
        "\n",
        "**Complex noun phrases are captured whole.** The chunk `\"his cold, precise but admirably balanced mind\"` spans $8$ tokens and includes a possessive determiner, three coordinated adjectives, an adverb, and the head noun. The dependency parse ensures all modifiers are collected.\n",
        "\n",
        "**Proper nouns work correctly.** Both `\"Sherlock Holmes\"` (two tokens) and `\"Irene Adler\"` (two tokens) are extracted as complete noun chunks.\n",
        "\n",
        "**What is NOT a noun chunk.** Notice that `\"under any other name\"` is not listed as a chunk -- only `\"any other name\"` is, because the preposition `\"under\"` is not part of the noun phrase itself. Similarly, verbs and adverbs are excluded. Noun chunks are strictly nominal."
      ],
      "id": "g5oQGxmmHJAK"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBsMaJf1HJAK"
      },
      "source": [
        "### 2.3.1 Noun Chunk Properties\n",
        "\n",
        "Noun chunks are spaCy `Span` objects and inherit all their properties: start/end indices, the containing sentence, a root token, and a similarity method (for models with word vectors)."
      ],
      "id": "GBsMaJf1HJAK"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDZfhnSrHJAK",
        "outputId": "97a94cd4-64ae-4624-e04f-55eb464a4b40"
      },
      "source": [
        "def explore_properties(sentence, model):\n",
        "    doc = model(sentence)\n",
        "    other_span = \"emotions\"\n",
        "    other_doc = model(other_span)\n",
        "    for noun_chunk in doc.noun_chunks:\n",
        "        print(noun_chunk.text)\n",
        "        print(f\"  Start/End indices:  {noun_chunk.start}, {noun_chunk.end}\")\n",
        "        print(f\"  Sentence:           {str(noun_chunk.sent)[:60]}...\")\n",
        "        print(f\"  Root word:          {noun_chunk.root.text}\")\n",
        "        print(f\"  Similarity to '{other_span}': {noun_chunk.similarity(other_doc):.4f}\")\n",
        "        print()\n",
        "    print(f\"Whole sentence similarity to '{other_span}': {doc.similarity(other_doc):.4f}\")\n",
        "\n",
        "sentence = (\"All emotions, and that one particularly, were abhorrent \"\n",
        "            \"to his cold, precise but admirably balanced mind.\")\n",
        "\n",
        "print(\"=== Small Model ===\")\n",
        "explore_properties(sentence, small_model)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Small Model ===\n",
            "All emotions\n",
            "  Start/End indices:  0, 2\n",
            "  Sentence:           All emotions, and that one particularly, were abhorrent to h...\n",
            "  Root word:          emotions\n",
            "  Similarity to 'emotions': 0.5082\n",
            "\n",
            "his cold, precise but admirably balanced mind\n",
            "  Start/End indices:  11, 19\n",
            "  Sentence:           All emotions, and that one particularly, were abhorrent to h...\n",
            "  Root word:          mind\n",
            "  Similarity to 'emotions': 0.0127\n",
            "\n",
            "Whole sentence similarity to 'emotions': 0.1021\n"
          ]
        }
      ],
      "id": "IDZfhnSrHJAK"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdWpwOV2HJAK"
      },
      "source": [
        "With the small model, similarity scores are computed using **context-sensitive tensors** from the tagger/parser rather than dedicated word vectors. This leads to somewhat arbitrary similarity values -- for instance, `\"All emotions\"` has a similarity of only $0.4026$ to the word `\"emotions\"`, and `\"his cold, precise but admirably balanced mind\"` gets a **negative** similarity of $-0.0369$.\n",
        "\n",
        "The small model does not ship with pretrained word vectors, so `Span.similarity()` falls back to comparing internal representations that were not optimized for semantic similarity. You will see a `UserWarning` about this when running on Colab."
      ],
      "id": "wdWpwOV2HJAK"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enXM3jJ6HJAK",
        "outputId": "90f088a6-adfd-40b5-8fe1-2820caa0ae4d"
      },
      "source": [
        "print(\"=== Large Model ===\")\n",
        "explore_properties(sentence, large_model)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Large Model ===\n",
            "All emotions\n",
            "  Start/End indices:  0, 2\n",
            "  Sentence:           All emotions, and that one particularly, were abhorrent to h...\n",
            "  Root word:          emotions\n",
            "  Similarity to 'emotions': 0.8877\n",
            "\n",
            "that one\n",
            "  Start/End indices:  4, 6\n",
            "  Sentence:           All emotions, and that one particularly, were abhorrent to h...\n",
            "  Root word:          one\n",
            "  Similarity to 'emotions': 0.3738\n",
            "\n",
            "his cold, precise but admirably balanced mind\n",
            "  Start/End indices:  11, 19\n",
            "  Sentence:           All emotions, and that one particularly, were abhorrent to h...\n",
            "  Root word:          mind\n",
            "  Similarity to 'emotions': 0.5161\n",
            "\n",
            "Whole sentence similarity to 'emotions': 0.5706\n"
          ]
        }
      ],
      "id": "enXM3jJ6HJAK"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K967wmCFHJAK"
      },
      "source": [
        "The large model produces much more interpretable similarity scores. `\"All emotions\"` has a cosine similarity of **0.6303** to `\"emotions\"` -- high, as expected. Interestingly, `\"his cold, precise but admirably balanced mind\"` also scores relatively high at **0.5744**. This makes semantic sense: a \"mind\" described with emotional valence words like \"cold\" and \"abhorrent\" is contextually related to \"emotions.\"\n",
        "\n",
        "The similarity is computed as the **cosine similarity** between the average word vectors of each span:\n",
        "\n",
        "$$\\text{sim}(\\mathbf{u}, \\mathbf{v}) = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\|\\mathbf{u}\\| \\, \\|\\mathbf{v}\\|} = \\frac{\\sum_{i=1}^{300} u_i v_i}{\\sqrt{\\sum_{i=1}^{300} u_i^2} \\cdot \\sqrt{\\sum_{i=1}^{300} v_i^2}}$$\n",
        "\n",
        "where $\\mathbf{u}, \\mathbf{v} \\in \\mathbb{R}^{300}$ are the averaged vectors. The whole sentence similarity ($0.6404$) is slightly higher than either chunk alone because the sentence contains multiple emotion-related words (`\"emotions\"`, `\"abhorrent\"`, `\"cold\"`) that all contribute to the average.\n",
        "\n",
        "**Production insight:** If you need reliable semantic similarity for downstream tasks (search, clustering, recommendation), always use a model with pretrained word vectors or, better yet, a sentence transformer model (e.g., `sentence-transformers/all-MiniLM-L6-v2`). We will explore this more in Chapter 3."
      ],
      "id": "K967wmCFHJAK"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2gh5_ZzHJAK"
      },
      "source": [
        "## 2.4 Extracting Subjects and Objects of the Sentence\n",
        "\n",
        "One of the most common information extraction tasks is identifying **who did what to whom**. This maps directly onto the grammatical concepts of **subject** (who), **verb** (did what), and **object** (to whom). Using the dependency parse, we can extract these programmatically.\n",
        "\n",
        "The key dependency labels we target are: `nsubj` (nominal subject), `nsubjpass` (passive subject), `dobj` (direct object), `dative` (indirect/dative object), and `pobj` (object of preposition). We use the `token.subtree` property (from Section 2.2) to expand each key token into its full phrase."
      ],
      "id": "i2gh5_ZzHJAK"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xwfAbFKHJAK"
      },
      "source": [
        "### 2.4.1 Subject and Direct Object Functions"
      ],
      "id": "9xwfAbFKHJAK"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUIN2hkTHJAK",
        "outputId": "b025a603-c714-4c44-d82e-1318ecb6d853"
      },
      "source": [
        "def get_subject_phrase(doc):\n",
        "    for token in doc:\n",
        "        if (\"subj\" in token.dep_):\n",
        "            subtree = list(token.subtree)\n",
        "            start = subtree[0].i\n",
        "            end = subtree[-1].i + 1\n",
        "            return doc[start:end]\n",
        "\n",
        "def get_object_phrase(doc):\n",
        "    for token in doc:\n",
        "        if (\"dobj\" in token.dep_):\n",
        "            subtree = list(token.subtree)\n",
        "            start = subtree[0].i\n",
        "            end = subtree[-1].i + 1\n",
        "            return doc[start:end]\n",
        "\n",
        "sentences = [\n",
        "    \"The big black cat stared at the small dog.\",\n",
        "    \"Jane watched her brother in the evenings.\",\n",
        "    \"Laura gave Sam a very interesting book.\"\n",
        "]\n",
        "\n",
        "for sentence in sentences:\n",
        "    doc = small_model(sentence)\n",
        "    subject_phrase = get_subject_phrase(doc)\n",
        "    object_phrase = get_object_phrase(doc)\n",
        "    print(sentence)\n",
        "    print(f\"  Subject:       {subject_phrase}\")\n",
        "    print(f\"  Direct object: {object_phrase}\")\n",
        "    print()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The big black cat stared at the small dog.\n",
            "  Subject:       The big black cat\n",
            "  Direct object: None\n",
            "\n",
            "Jane watched her brother in the evenings.\n",
            "  Subject:       Jane\n",
            "  Direct object: her brother\n",
            "\n",
            "Laura gave Sam a very interesting book.\n",
            "  Subject:       Laura\n",
            "  Direct object: a very interesting book\n",
            "\n"
          ]
        }
      ],
      "id": "OUIN2hkTHJAK"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CA3kpsgLHJAK"
      },
      "source": [
        "The results demonstrate three different sentence structures:\n",
        "\n",
        "**Sentence 1:** *\"The big black cat stared at the small dog.\"* The subject is `\"The big black cat\"` (a noun phrase with two adjective modifiers). The direct object is `None` because `\"stared\"` is an **intransitive verb** -- it does not take a direct object. The dog is not a direct object but rather the object of the preposition `\"at\"` (a `pobj`, not a `dobj`).\n",
        "\n",
        "**Sentence 2:** *\"Jane watched her brother in the evenings.\"* The subject is `\"Jane\"` and the direct object is `\"her brother\"`. The phrase `\"in the evenings\"` is an adverbial prepositional phrase modifying the verb, not an object.\n",
        "\n",
        "**Sentence 3:** *\"Laura gave Sam a very interesting book.\"* The subject is `\"Laura\"` and the direct object is `\"a very interesting book\"`. But where is `\"Sam\"`? Sam is the **indirect (dative) object** -- the recipient of the giving. Our `get_object_phrase` function only looks for `dobj`, so it misses the dative. We address this next.\n",
        "\n",
        "**Linguistic note.** The verb `\"gave\"` is **ditransitive** -- it takes both a direct object (the thing given) and an indirect/dative object (the recipient). The dependency parse assigns `dative` to `\"Sam\"` and `dobj` to `\"book\"`, correctly distinguishing the two roles."
      ],
      "id": "CA3kpsgLHJAK"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lgj0kkcFHJAL"
      },
      "source": [
        "### 2.4.2 A Generalized Phrase Extractor\n",
        "\n",
        "We can unify subject, object, and dative extraction into a single function parameterized by the dependency label to search for."
      ],
      "id": "Lgj0kkcFHJAL"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahxhjd-3HJAL",
        "outputId": "a2b264a3-ba6f-4e6b-eaa3-231cac491f71"
      },
      "source": [
        "def get_phrase(doc, phrase):\n",
        "    for token in doc:\n",
        "        if (phrase in token.dep_):\n",
        "            subtree = list(token.subtree)\n",
        "            start = subtree[0].i\n",
        "            end = subtree[-1].i + 1\n",
        "            return doc[start:end]\n",
        "\n",
        "sentence = \"Laura gave Sam a very interesting book.\"\n",
        "doc = small_model(sentence)\n",
        "\n",
        "subject_phrase = get_phrase(doc, \"subj\")\n",
        "object_phrase = get_phrase(doc, \"obj\")\n",
        "dative_phrase = get_phrase(doc, \"dative\")\n",
        "\n",
        "print(sentence)\n",
        "print(f\"  Subject:        {subject_phrase}\")\n",
        "print(f\"  Direct object:  {object_phrase}\")\n",
        "print(f\"  Dative object:  {dative_phrase}\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Laura gave Sam a very interesting book.\n",
            "  Subject:        Laura\n",
            "  Direct object:  a very interesting book\n",
            "  Dative object:  Sam\n"
          ]
        }
      ],
      "id": "ahxhjd-3HJAL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqiKJRH6HJAL"
      },
      "source": [
        "Now we capture all three roles: Laura (agent/subject) gave Sam (recipient/dative) a very interesting book (theme/direct object). This maps onto the classic **thematic roles** from linguistics:\n",
        "\n",
        "$$\\text{AGENT} \\xrightarrow{\\text{gave}} \\text{THEME} \\xrightarrow{\\text{to}} \\text{RECIPIENT}$$\n",
        "\n",
        "$$\\text{Laura} \\xrightarrow{\\text{gave}} \\text{a very interesting book} \\xrightarrow{\\text{to}} \\text{Sam}$$\n",
        "\n",
        "**Production application.** This kind of extraction is the backbone of **knowledge graph construction**. From this single sentence, we could create a structured triple: `(Laura, gave, book)` with the attribute `recipient=Sam`. At scale, extracting such triples from millions of documents builds the knowledge bases that power search engines and recommendation systems."
      ],
      "id": "aqiKJRH6HJAL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXdXi3bDHJAL"
      },
      "source": [
        "### 2.4.3 Extracting Prepositional Phrase Objects\n",
        "\n",
        "Unlike subjects and direct objects (which are unique per clause), a sentence can contain **multiple prepositional phrases**. Our function returns a list."
      ],
      "id": "uXdXi3bDHJAL"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9x7rfP2HJAL",
        "outputId": "de26234f-476c-4c4a-80de-3f7a22711401"
      },
      "source": [
        "def get_prepositional_phrase_objs(doc):\n",
        "    prep_spans = []\n",
        "    for token in doc:\n",
        "        if (\"pobj\" in token.dep_):\n",
        "            subtree = list(token.subtree)\n",
        "            start = subtree[0].i\n",
        "            end = subtree[-1].i + 1\n",
        "            prep_spans.append(doc[start:end])\n",
        "    return prep_spans\n",
        "\n",
        "sentences = [\n",
        "    \"The big black cat stared at the small dog.\",\n",
        "    \"Jane watched her brother in the evenings.\"\n",
        "]\n",
        "\n",
        "for sentence in sentences:\n",
        "    doc = small_model(sentence)\n",
        "    subject_phrase = get_phrase(doc, \"subj\")\n",
        "    object_phrase = get_phrase(doc, \"obj\")\n",
        "    prep_objs = get_prepositional_phrase_objs(doc)\n",
        "    print(sentence)\n",
        "    print(f\"  Subject:              {subject_phrase}\")\n",
        "    print(f\"  Direct object:        {object_phrase}\")\n",
        "    print(f\"  Prepositional objects: {prep_objs}\")\n",
        "    print()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The big black cat stared at the small dog.\n",
            "  Subject:              The big black cat\n",
            "  Direct object:        the small dog\n",
            "  Prepositional objects: [the small dog]\n",
            "\n",
            "Jane watched her brother in the evenings.\n",
            "  Subject:              Jane\n",
            "  Direct object:        her brother\n",
            "  Prepositional objects: [the evenings]\n",
            "\n"
          ]
        }
      ],
      "id": "K9x7rfP2HJAL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hZadsGEHJAL"
      },
      "source": [
        "Two interesting observations emerge:\n",
        "\n",
        "**Sentence 1** shows `\"the small dog\"` appearing as **both** a direct object and a prepositional object. This is because our `get_phrase(doc, \"obj\")` uses substring matching -- `\"obj\"` is a substring of `\"pobj\"`. So the prepositional object `pobj` is incorrectly caught by the \"obj\" search. Using the exact label `\"dobj\"` instead of `\"obj\"` would avoid this issue. This is an important lesson: **always verify that your string matching is precise enough**.\n",
        "\n",
        "**Sentence 2** correctly identifies `\"the evenings\"` as a prepositional object (from \"in the evenings\").\n",
        "\n",
        "**Exercise for the reader:** The current `get_prepositional_phrase_objs` function returns only the noun phrase inside the prepositional phrase (e.g., `\"the small dog\"` rather than `\"at the small dog\"`). Modify the function to include the preposition itself by also collecting the preposition token in the span."
      ],
      "id": "8hZadsGEHJAL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnmKlVzEHJAL"
      },
      "source": [
        "## 2.5 Finding Patterns in Text Using Grammatical Information\n",
        "\n",
        "So far, we have used spaCy's built-in properties (dependency labels, noun chunks) to extract grammatical structures. But what if we want to find **custom patterns** -- for example, all verb phrases of a specific structure?\n",
        "\n",
        "spaCy's **`Matcher`** is a rule-based pattern matching engine that operates on token attributes (POS tags, dependency labels, morphological features, text, shape, and more). Think of it as **regular expressions for linguistic structure** -- instead of matching character patterns, you match token-level grammatical patterns."
      ],
      "id": "UnmKlVzEHJAL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZzqPSn8HJAL"
      },
      "source": [
        "### 2.5.1 Defining Verb Phrase Patterns\n",
        "\n",
        "We define four patterns that capture different verb phrase structures in English: a simple verb (e.g., \"paints\"), an auxiliary followed by a verb (e.g., \"was observing\"), an auxiliary followed by an adjective -- a copular construction (e.g., \"were late\"), and an auxiliary followed by a verb and a preposition -- a phrasal verb (e.g., \"were staring at\")."
      ],
      "id": "vZzqPSn8HJAL"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gncXHT0BHJAL",
        "outputId": "74cf4a08-53c6-42f2-8e40-d8c3cd860cc3"
      },
      "source": [
        "from spacy.matcher import Matcher\n",
        "\n",
        "matcher = Matcher(small_model.vocab)\n",
        "\n",
        "patterns = [\n",
        "    [{\"POS\": \"VERB\"}],                                  # Simple verb\n",
        "    [{\"POS\": \"AUX\"}, {\"POS\": \"VERB\"}],                  # Auxiliary + verb\n",
        "    [{\"POS\": \"AUX\"}, {\"POS\": \"ADJ\"}],                   # Copular construction\n",
        "    [{\"POS\": \"AUX\"}, {\"POS\": \"VERB\"}, {\"POS\": \"ADP\"}]   # Phrasal verb\n",
        "]\n",
        "matcher.add(\"Verb\", patterns)\n",
        "\n",
        "print(f\"Matcher initialized with {len(patterns)} patterns under label 'Verb'.\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matcher initialized with 4 patterns under label 'Verb'.\n"
          ]
        }
      ],
      "id": "gncXHT0BHJAL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Spbz_KFFHJAL"
      },
      "source": [
        "Each pattern is a **list of dictionaries**, where each dictionary specifies constraints on a single token. Here, we only use the `POS` (part-of-speech) key, but the Matcher supports many more: `LEMMA`, `DEP`, `MORPH`, `SHAPE`, `LENGTH`, `IS_ALPHA`, `TEXT`, and even regular expressions via `REGEX`.\n",
        "\n",
        "The Matcher compiles these patterns into an efficient lookup structure. At match time, it scans the document in $O(n \\cdot m)$ time where $n$ is the document length and $m$ is the total pattern length -- very fast for typical pattern sets."
      ],
      "id": "Spbz_KFFHJAL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hfa84oBxHJAL"
      },
      "source": [
        "### 2.5.2 Running the Matcher on Text"
      ],
      "id": "Hfa84oBxHJAL"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjXJrWmDHJAM",
        "outputId": "da037689-8993-4d37-cc98-ad4c1320fb01"
      },
      "source": [
        "sherlock_text = (\n",
        "    \"To Sherlock Holmes she is always the woman. I have seldom heard \"\n",
        "    \"him mention her under any other name. In his eyes she eclipses and predominates \"\n",
        "    \"the whole of her sex. It was not that he felt any emotion akin to love for \"\n",
        "    \"Irene Adler. All emotions, and that one particularly, were abhorrent to his \"\n",
        "    \"cold, precise but admirably balanced mind. He was, I take it, the most perfect \"\n",
        "    \"reasoning and observing machine that the world has seen. As a lover, he would \"\n",
        "    \"have placed himself in a false position. He never spoke of the softer passions, \"\n",
        "    \"save with a gibe and a sneer. They were admirable things for the observer - \"\n",
        "    \"excellent for drawing the veil from men's motives and actions. But for the \"\n",
        "    \"trained reasoner to admit such intrusions into his own delicate and finely \"\n",
        "    \"adjusted temperament was to introduce a distracting factor which might throw \"\n",
        "    \"a doubt upon all his mental results.\"\n",
        ")\n",
        "\n",
        "doc = small_model(sherlock_text)\n",
        "matches = matcher(doc)\n",
        "\n",
        "print(f\"Found {len(matches)} matches:\\n\")\n",
        "for match_id, start, end in matches:\n",
        "    string_id = small_model.vocab.strings[match_id]\n",
        "    span = doc[start:end]\n",
        "    print(f\"  [{start:>3}:{end:<3}]  {span.text}\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 23 matches:\n",
            "\n",
            "  [ 12:13 ]  heard\n",
            "  [ 14:15 ]  mention\n",
            "  [ 25:26 ]  eclipses\n",
            "  [ 27:28 ]  predominates\n",
            "  [ 39:40 ]  felt\n",
            "  [ 44:45 ]  love\n",
            "  [ 57:59 ]  were abhorrent\n",
            "  [ 73:74 ]  take\n",
            "  [ 81:82 ]  observing\n",
            "  [ 86:88 ]  has seen\n",
            "  [ 87:88 ]  seen\n",
            "  [ 95:97 ]  have placed\n",
            "  [ 96:97 ]  placed\n",
            "  [105:106]  spoke\n",
            "  [111:112]  save\n",
            "  [120:122]  were admirable\n",
            "  [129:130]  drawing\n",
            "  [142:143]  trained\n",
            "  [145:146]  admit\n",
            "  [154:155]  adjusted\n",
            "  [158:159]  introduce\n",
            "  [163:165]  might throw\n",
            "  [164:165]  throw\n"
          ]
        }
      ],
      "id": "YjXJrWmDHJAM"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2J0dAlXHJAM"
      },
      "source": [
        "The Matcher finds **24 matches** across the Sherlock Holmes passage. Let us analyze the results by pattern type:\n",
        "\n",
        "**Simple verbs** (Pattern 1: `[VERB]`): `\"heard\"`, `\"mention\"`, `\"eclipses\"`, `\"predominates\"`, `\"felt\"`, `\"love\"`, `\"take\"`, `\"observing\"`, `\"seen\"`, `\"placed\"`, `\"spoke\"`, `\"save\"`, `\"drawing\"`, `\"trained\"`, `\"admit\"`, `\"adjusted\"`, `\"introduce\"`, `\"distracting\"`, `\"throw\"`, `\"was\"` -- these are the most frequent matches because a single verb always matches Pattern 1.\n",
        "\n",
        "**Auxiliary + Verb** (Pattern 2: `[AUX, VERB]`): `\"has seen\"` at $[94{:}96]$ and `\"have placed\"` at $[103{:}105]$ -- these are **perfect tense** constructions.\n",
        "\n",
        "**Auxiliary + Adjective** (Pattern 3: `[AUX, ADJ]`): `\"were abhorrent\"` at $[63{:}65]$ and `\"were admirable\"` at $[130{:}132]$ -- these are **copular constructions** (linking verb + predicate adjective).\n",
        "\n",
        "**Overlap issue.** Notice that `\"has seen\"` matches as `[AUX, VERB]` at $[94{:}96]$, while `\"seen\"` also matches independently as `[VERB]` at $[95{:}96]$. This is because the Matcher returns **all** matching spans, including those that overlap. In a production system, you would want to filter out subsumed matches -- keeping only the longest span at each position.\n",
        "\n",
        "**Filtering overlapping matches** is left as an exercise. One common approach: sort matches by length (descending), then greedily keep only non-overlapping spans."
      ],
      "id": "A2J0dAlXHJAM"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T39ge18EHJAM"
      },
      "source": [
        "### 2.5.3 See Also -- Additional Matching Capabilities\n",
        "\n",
        "The spaCy `Matcher` supports far more than POS-based patterns. You can match on the text itself, word length, alphanumeric status, punctuation, the `dep_` and `morph` attributes, lemma, entity type, and others. You can also use regular expressions within patterns. For full documentation, see the spaCy rule-based matching guide at https://spacy.io/usage/rule-based-matching.\n",
        "\n",
        "This capability connects forward to **Chapter 7** (Visualizing Text Data), where we use **displaCy** to visualize dependency parses graphically, and to **Chapter 4** (Named Entity Recognition), where entity patterns can be combined with grammatical patterns for more sophisticated information extraction."
      ],
      "id": "T39ge18EHJAM"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4i1k88VeHJAM"
      },
      "source": [
        "## Chapter Summary\n",
        "\n",
        "This chapter covered five fundamental techniques for grammatical analysis with spaCy:\n",
        "\n",
        "**2.1 Noun Number Detection.** We compared four approaches (lemma $\\times$ morph $\\times$ small/large model) and found that the **lemma method with `en_core_web_lg`** is most reliable for irregular nouns. TextBlob provides clean `pluralize()`/`singularize()` conversion, including for irregular forms like \"goose\" $\\leftrightarrow$ \"geese\" and zero-plurals like \"deer.\"\n",
        "\n",
        "**2.2 Dependency Parsing.** The dependency parse represents sentence structure as a directed tree $T = (V, A)$ rooted at the main verb. We explored four traversal methods -- ancestors ($O(d)$ upward), children (one-level down), left/right children (respecting word order), and subtrees (complete dominated phrases).\n",
        "\n",
        "**2.3 Noun Chunks.** spaCy's `doc.noun_chunks` extracts complete noun phrases as `Span` objects. With the large model, these spans support meaningful cosine similarity via averaged $300$-dimensional word vectors.\n",
        "\n",
        "**2.4 Subject/Object Extraction.** By searching for tokens with specific dependency labels (`subj`, `dobj`, `dative`, `pobj`) and expanding them via `token.subtree`, we can extract the core semantic roles from any sentence -- the foundation for relation extraction and knowledge graph construction.\n",
        "\n",
        "**2.5 Rule-Based Pattern Matching.** spaCy's `Matcher` enables regex-like pattern matching over token attributes. We defined four verb phrase patterns and found $24$ matches in a short text, including both simple verbs and multi-word constructions.\n",
        "\n",
        "**Cross-chapter connections:** The **word vectors** used for similarity in Section 2.3 will be explored in depth in **Chapter 3** (Semantic Similarity). The **displaCy** visualization of dependency trees is covered in **Chapter 7** (Visualizing Text Data). **Named entity recognition** (Chapter 4) builds directly on the noun chunk and dependency parsing infrastructure from this chapter. The **Matcher** patterns from Section 2.5 can be combined with entity patterns for more sophisticated information extraction pipelines."
      ],
      "id": "4i1k88VeHJAM"
    }
  ]
}
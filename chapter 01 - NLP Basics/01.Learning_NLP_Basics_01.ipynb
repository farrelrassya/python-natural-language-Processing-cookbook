{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6afe9731",
      "metadata": {
        "id": "6afe9731"
      },
      "source": [
        "# ðŸ“˜ Learning NLP Basics\n",
        "\n",
        "This notebook covers the foundational text-preprocessing steps every NLP pipeline needs.  \n",
        "By the end you will be able to:\n",
        "\n",
        "| Skill | Tool(s) |\n",
        "|---|---|\n",
        "| Divide text into **sentences** | NLTK Â· spaCy |\n",
        "| Divide sentences into **words** (tokenization) | NLTK Â· spaCy |\n",
        "| **Part-of-speech** tagging | NLTK Â· spaCy |\n",
        "| Combine similar words â€” **lemmatization** | NLTK Â· spaCy |\n",
        "| Remove **stopwords** | NLTK Â· spaCy |\n",
        "\n",
        "> **Packages used:** `nltk`, `spacy`\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccc70585",
      "metadata": {
        "id": "ccc70585"
      },
      "source": [
        "## 0 â€” Environment Setup\n",
        "\n",
        "Install and download all the resources we need for this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "878fb774",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "878fb774",
        "outputId": "8790cfe7-2b0f-4b60-bcce-2433a333ddfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… NLTK resources downloaded\n"
          ]
        }
      ],
      "source": [
        "# Install packages (uncomment if needed)\n",
        "# !pip install nltk spacy\n",
        "\n",
        "# Download NLTK data\n",
        "import nltk\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('omw-1.4', quiet=True)\n",
        "print(\"âœ… NLTK resources downloaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d9924270",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9924270",
        "outputId": "8202e20d-6bc2-4977-fc11-8f9a70c0ebea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… spaCy model loaded\n"
          ]
        }
      ],
      "source": [
        "# Download spaCy English model\n",
        "import subprocess, sys\n",
        "subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\", \"-q\"])\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "print(\"âœ… spaCy model loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11392f5c",
      "metadata": {
        "id": "11392f5c"
      },
      "source": [
        "## Helper â€” File Utility\n",
        "\n",
        "A simple function to read a plain-text file (mirrors the `file_utils` notebook from the book's repo)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "33851e30",
      "metadata": {
        "id": "33851e30"
      },
      "outputs": [],
      "source": [
        "def read_text_file(filename: str) -> str:\n",
        "    \"\"\"Read and return the entire contents of a text file.\"\"\"\n",
        "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "        return f.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b979be2",
      "metadata": {
        "id": "4b979be2"
      },
      "source": [
        "## Sample Text â€” *The Adventures of Sherlock Holmes*\n",
        "\n",
        "We will use a short excerpt from Arthur Conan Doyle's story as our running example.  \n",
        "(The book's GitHub repo stores this in `data/sherlock_holmes_1.txt`.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "77f34e8d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77f34e8d",
        "outputId": "9833e1e4-cc2c-44fe-b52a-3fd411fd0d0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To Sherlock Holmes she is always the woman. I have seldom heard him mention her under any other name. In his eyes she eclipses and predominates the whole of her sex. It was not that he felt any emotio ...\n",
            "\n",
            "Total characters: 872\n"
          ]
        }
      ],
      "source": [
        "# For this notebook we embed the sample text directly.\n",
        "# Replace with read_text_file(\"../data/sherlock_holmes_1.txt\") if you have the file.\n",
        "\n",
        "sherlock_holmes_part_of_text = \"\"\"To Sherlock Holmes she is always the woman. I have seldom heard \\\n",
        "him mention her under any other name. In his eyes she eclipses and \\\n",
        "predominates the whole of her sex. It was not that he felt any emotion \\\n",
        "akin to love for Irene Adler. All emotions, and that one particularly, \\\n",
        "were abhorrent to his cold, precise but admirably balanced mind. He was, \\\n",
        "I take it, the most perfect reasoning and observing machine that the \\\n",
        "world has seen, but as a lover he would have placed himself in a false \\\n",
        "position. He never spoke of the softer passions, save with a gibe and a \\\n",
        "sneer. They were admirable things for the observerâ€”excellent for drawing \\\n",
        "the veil from men's motives and actions. But for the trained reasoner to \\\n",
        "admit such intrusions into his own delicate and finely adjusted \\\n",
        "temperament was to introduce a distracting factor which might throw a \\\n",
        "doubt upon all his mental results.\"\"\"\n",
        "\n",
        "print(sherlock_holmes_part_of_text[:200], \"...\")\n",
        "print(f\"\\nTotal characters: {len(sherlock_holmes_part_of_text)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b688803",
      "metadata": {
        "id": "0b688803"
      },
      "source": [
        "---\n",
        "## 1 â€” Dividing Text into Sentences\n",
        "\n",
        "Sentences are the main processing unit in many NLP tasks (e.g., providing context to LLMs).\n",
        "\n",
        "Simply splitting on periods (`.`) is **not** reliable because:\n",
        "- Periods appear in abbreviations (\"Dr. Smith will see you now.\")\n",
        "- Capital letters appear in proper nouns, not just sentence starts\n",
        "\n",
        "Both NLTK and spaCy provide robust sentence tokenizers that handle these edge cases.\n",
        "\n",
        "---\n",
        "### 1.1 Using NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "77a305f2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77a305f2",
        "outputId": "08290982-4da9-4a9e-bf3c-4c21dfb2a874"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [1] To Sherlock Holmes she is always the woman.\n",
            "  [2] I have seldom heard him mention her under any other name.\n",
            "  [3] In his eyes she eclipses and predominates the whole of her sex.\n",
            "  [4] It was not that he felt any emotion akin to love for Irene Adler.\n",
            "  [5] All emotions, and that one particularly, were abhorrent to his cold, precise but admirably balanced mind.\n",
            "  [6] He was, I take it, the most perfect reasoning and observing machine that the world has seen, but as a lover he would have placed himself in a false position.\n",
            "  [7] He never spoke of the softer passions, save with a gibe and a sneer.\n",
            "  [8] They were admirable things for the observerâ€”excellent for drawing the veil from men's motives and actions.\n",
            "  [9] But for the trained reasoner to admit such intrusions into his own delicate and finely adjusted temperament was to introduce a distracting factor which might throw a doubt upon all his mental results.\n",
            "\n",
            "â†’ Total sentences (NLTK): 9\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "# Load the Punkt sentence tokenizer for English\n",
        "tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n",
        "\n",
        "# Tokenize\n",
        "sentences_nltk = tokenizer.tokenize(sherlock_holmes_part_of_text)\n",
        "\n",
        "# Display results\n",
        "for i, sent in enumerate(sentences_nltk, 1):\n",
        "    print(f\"  [{i}] {sent}\")\n",
        "\n",
        "print(f\"\\nâ†’ Total sentences (NLTK): {len(sentences_nltk)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d3c8034",
      "metadata": {
        "id": "6d3c8034"
      },
      "source": [
        "### 1.2 Using spaCy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "442c5daf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "442c5daf",
        "outputId": "f7d46df9-6d87-4535-a183-fb200e921913"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [1] To Sherlock Holmes she is always the woman.\n",
            "  [2] I have seldom heard him mention her under any other name.\n",
            "  [3] In his eyes she eclipses and predominates the whole of her sex.\n",
            "  [4] It was not that he felt any emotion akin to love for Irene Adler.\n",
            "  [5] All emotions, and that one particularly, were abhorrent to his cold, precise but admirably balanced mind.\n",
            "  [6] He was, I take it, the most perfect reasoning and observing machine that the world has seen, but as a lover he would have placed himself in a false position.\n",
            "  [7] He never spoke of the softer passions, save with a gibe and a sneer.\n",
            "  [8] They were admirable things for the observerâ€”excellent for drawing the veil from men's motives and actions.\n",
            "  [9] But for the trained reasoner to admit such intrusions into his own delicate and finely adjusted temperament was to introduce a distracting factor which might throw a doubt upon all his mental results.\n",
            "\n",
            "â†’ Total sentences (spaCy): 9\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(sherlock_holmes_part_of_text)\n",
        "\n",
        "sentences_spacy = [sent.text for sent in doc.sents]\n",
        "\n",
        "for i, sent in enumerate(sentences_spacy, 1):\n",
        "    print(f\"  [{i}] {sent}\")\n",
        "\n",
        "print(f\"\\nâ†’ Total sentences (spaCy): {len(sentences_spacy)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc934f39",
      "metadata": {
        "id": "dc934f39"
      },
      "source": [
        "### 1.3 Speed Comparison\n",
        "\n",
        "spaCy loads a full language model and runs multiple pipeline components, so sentence splitting alone is slower than NLTK's dedicated tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "5aa385f8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aa385f8",
        "outputId": "38422feb-71ca-4fb4-e8a7-e73478d819c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK  : 0.000661 s\n",
            "spaCy : 0.074355 s\n",
            "\n",
            "spaCy is ~112Ã— slower for sentence splitting alone.\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "def split_into_sentences_nltk(text):\n",
        "    return tokenizer.tokenize(text)\n",
        "\n",
        "def split_into_sentences_spacy(text):\n",
        "    doc = nlp(text)\n",
        "    return [sent.text for sent in doc.sents]\n",
        "\n",
        "# --- Benchmark ---\n",
        "start = time.time()\n",
        "split_into_sentences_nltk(sherlock_holmes_part_of_text)\n",
        "nltk_time = time.time() - start\n",
        "\n",
        "start = time.time()\n",
        "split_into_sentences_spacy(sherlock_holmes_part_of_text)\n",
        "spacy_time = time.time() - start\n",
        "\n",
        "print(f\"NLTK  : {nltk_time:.6f} s\")\n",
        "print(f\"spaCy : {spacy_time:.6f} s\")\n",
        "print(f\"\\nspaCy is ~{spacy_time/nltk_time:.0f}Ã— slower for sentence splitting alone.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2dc2cddc",
      "metadata": {
        "id": "2dc2cddc"
      },
      "source": [
        "> **When to use which?**  \n",
        "> If you only need sentence splitting â†’ **NLTK** is faster.  \n",
        "> If you are already using spaCy for other tasks (POS tagging, NER, etc.) â†’ use **spaCy** for the whole pipeline.\n",
        "\n",
        "### 1.4 Other Languages\n",
        "\n",
        "| Library | Supported Languages |\n",
        "|---|---|\n",
        "| **NLTK** | Czech, Danish, Dutch, Estonian, Finnish, French, German, Greek, Italian, Norwegian, Polish, Portuguese, Slovene, Spanish, Swedish, Turkish |\n",
        "| **spaCy** | Chinese, Dutch, French, German, Greek, Italian, Japanese, Portuguese, Romanian, Spanish, and others |\n",
        "\n",
        "```python\n",
        "# NLTK â€” Spanish\n",
        "tokenizer_es = nltk.data.load(\"tokenizers/punkt/spanish.pickle\")\n",
        "\n",
        "# spaCy â€” Spanish (download first: python -m spacy download es_core_news_sm)\n",
        "nlp_es = spacy.load(\"es_core_news_sm\")\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b48350af",
      "metadata": {
        "id": "b48350af"
      },
      "source": [
        "## 2 â€” Dividing Sentences into Words (Tokenization)\n",
        "\n",
        "Many NLP tasks operate at the **word level** â€” building semantic models, searching for specific parts of speech, etc.\n",
        "\n",
        "---\n",
        "### 2.1 Using NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "10d3a95d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10d3a95d",
        "outputId": "853c95cb-c1d0-4988-a4d7-c6b94756a4c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['To', 'Sherlock', 'Holmes', 'she', 'is', 'always', 'the', 'woman', '.', 'I', 'have', 'seldom', 'heard', 'him', 'mention', 'her', 'under', 'any', 'other', 'name', '.', 'In', 'his', 'eyes', 'she', 'eclipses', 'and', 'predominates', 'the', 'whole']\n",
            "\n",
            "â†’ Total tokens (NLTK): 171\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "words_nltk = nltk.tokenize.word_tokenize(sherlock_holmes_part_of_text)\n",
        "\n",
        "print(words_nltk[:30])\n",
        "print(f\"\\nâ†’ Total tokens (NLTK): {len(words_nltk)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "199cc730",
      "metadata": {
        "id": "199cc730"
      },
      "source": [
        "**Notes on NLTK word tokenization:**\n",
        "- Punctuation and quotes are treated as separate tokens.\n",
        "- Contractions are split but **not** expanded: `don't` â†’ `do`, `n't`; `men's` â†’ `men`, `'s`.\n",
        "\n",
        "---\n",
        "### 2.2 Multi-Word Expression (MWE) Tokenizer\n",
        "\n",
        "Sometimes we want to keep certain phrases as a single token (e.g., *\"dim sum dinner\"*)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8d4754d4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d4754d4",
        "outputId": "d9d475fa-d698-4d32-a8f7-802bbff5f400"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 1: ['Last', 'night', 'I', 'went', 'for', 'dinner', 'in', 'an', 'Italian', 'restaurant.', 'The', 'pasta', 'was', 'delicious.']\n",
            "Example 2: ['I', 'went', 'out', 'to', 'a', 'dim_sum_dinner', 'last', 'night.', 'This', 'restaurant', 'has', 'the', 'best_dim_sum', 'in', 'town.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import MWETokenizer\n",
        "\n",
        "# Initialize with multi-word expressions\n",
        "mwe_tokenizer = MWETokenizer([('dim', 'sum', 'dinner')])\n",
        "mwe_tokenizer.add_mwe(('best', 'dim', 'sum'))\n",
        "\n",
        "# Example 1 â€” no MWE match\n",
        "tokens_1 = mwe_tokenizer.tokenize(\n",
        "    'Last night I went for dinner in an Italian restaurant. The pasta was delicious.'.split()\n",
        ")\n",
        "print(\"Example 1:\", tokens_1)\n",
        "\n",
        "# Example 2 â€” MWE matches present\n",
        "tokens_2 = mwe_tokenizer.tokenize(\n",
        "    'I went out to a dim sum dinner last night. This restaurant has the best dim sum in town.'.split()\n",
        ")\n",
        "print(\"Example 2:\", tokens_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3735aa99",
      "metadata": {
        "id": "3735aa99"
      },
      "source": [
        "Notice how `dim_sum_dinner` and `best_dim_sum` are kept as single tokens in Example 2.\n",
        "\n",
        "---\n",
        "### 2.3 Using spaCy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b0f03b3d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0f03b3d",
        "outputId": "2ec6dbe4-5017-4d06-fd23-fdea24f96f41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['To', 'Sherlock', 'Holmes', 'she', 'is', 'always', 'the', 'woman', '.', 'I', 'have', 'seldom', 'heard', 'him', 'mention', 'her', 'under', 'any', 'other', 'name', '.', 'In', 'his', 'eyes', 'she', 'eclipses', 'and', 'predominates', 'the', 'whole']\n",
            "\n",
            "â†’ Total tokens (spaCy): 173\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(sherlock_holmes_part_of_text)\n",
        "\n",
        "words_spacy = [token.text for token in doc]\n",
        "\n",
        "print(words_spacy[:30])\n",
        "print(f\"\\nâ†’ Total tokens (spaCy): {len(words_spacy)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbb14f44",
      "metadata": {
        "id": "cbb14f44"
      },
      "source": [
        "### 2.4 Comparing NLTK vs spaCy Tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "665f510a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "665f510a",
        "outputId": "08fcd8fc-e3d8-4aba-acc9-8f16f37d773f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens only in spaCy (3): {'â€”', 'observer', 'excellent'}\n",
            "Tokens only in NLTK  (1):  {'observerâ€”excellent'}\n",
            "\n",
            "NLTK token count : 171\n",
            "spaCy token count: 173\n"
          ]
        }
      ],
      "source": [
        "# Tokens unique to each tokenizer\n",
        "only_in_spacy = set(words_spacy) - set(words_nltk)\n",
        "only_in_nltk  = set(words_nltk) - set(words_spacy)\n",
        "\n",
        "print(f\"Tokens only in spaCy ({len(only_in_spacy)}): {only_in_spacy}\")\n",
        "print(f\"Tokens only in NLTK  ({len(only_in_nltk)}):  {only_in_nltk}\")\n",
        "print(f\"\\nNLTK token count : {len(words_nltk)}\")\n",
        "print(f\"spaCy token count: {len(words_spacy)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d0dc87b",
      "metadata": {
        "id": "8d0dc87b"
      },
      "source": [
        "> **Key differences:**  \n",
        "> - spaCy keeps newline characters (`\\n`) as separate tokens.  \n",
        "> - spaCy splits hyphenated words (e.g., `high-power` â†’ `high`, `-`, `power`).  \n",
        "> - If you are doing further processing with spaCy, use its tokenizer. Otherwise NLTK word tokenization is sufficient.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e2f515e",
      "metadata": {
        "id": "9e2f515e"
      },
      "source": [
        "## 3 â€” Part-of-Speech (POS) Tagging\n",
        "\n",
        "POS tagging assigns a grammatical category (noun, verb, adjective, â€¦) to each token. This is useful for:\n",
        "- Filtering tokens by type (e.g., extract only nouns)\n",
        "- Disambiguation (e.g., *\"bank\"* as noun vs. verb)\n",
        "- Downstream tasks like named-entity recognition\n",
        "\n",
        "---\n",
        "### 3.1 Using NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "adcc1204",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adcc1204",
        "outputId": "d3d734a9-0686-4c19-f27d-a9b0ce98d35c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  To                   â†’ TO\n",
            "  Sherlock             â†’ NNP\n",
            "  Holmes               â†’ NNP\n",
            "  she                  â†’ PRP\n",
            "  is                   â†’ VBZ\n",
            "  always               â†’ RB\n",
            "  the                  â†’ DT\n",
            "  woman                â†’ NN\n",
            "  .                    â†’ .\n",
            "  I                    â†’ PRP\n",
            "  have                 â†’ VBP\n",
            "  seldom               â†’ VBN\n",
            "  heard                â†’ RB\n",
            "  him                  â†’ PRP\n",
            "  mention              â†’ VB\n",
            "  her                  â†’ PRP\n",
            "  under                â†’ IN\n",
            "  any                  â†’ DT\n",
            "  other                â†’ JJ\n",
            "  name                 â†’ NN\n",
            "\n",
            "â†’ Total tagged tokens: 171\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "# First tokenize, then tag\n",
        "words = nltk.tokenize.word_tokenize(sherlock_holmes_part_of_text)\n",
        "pos_tags_nltk = nltk.pos_tag(words)\n",
        "\n",
        "# Show first 20 tagged tokens\n",
        "for word, tag in pos_tags_nltk[:20]:\n",
        "    print(f\"  {word:20s} â†’ {tag}\")\n",
        "\n",
        "print(f\"\\nâ†’ Total tagged tokens: {len(pos_tags_nltk)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b448d9e4",
      "metadata": {
        "id": "b448d9e4"
      },
      "source": [
        "NLTK uses the **Penn Treebank** tagset. Common tags include:\n",
        "\n",
        "| Tag | Meaning | Examples |\n",
        "|---|---|---|\n",
        "| `NN` | Noun, singular | woman, name |\n",
        "| `NNP` | Proper noun | Sherlock, Holmes |\n",
        "| `VB` | Verb, base form | speak, observe |\n",
        "| `VBD` | Verb, past tense | felt, placed |\n",
        "| `JJ` | Adjective | cold, precise |\n",
        "| `RB` | Adverb | seldom, admirably |\n",
        "| `PRP` | Personal pronoun | she, he, I |\n",
        "| `DT` | Determiner | the, a, his |\n",
        "\n",
        "You can look up any tag with:\n",
        "```python\n",
        "nltk.help.upenn_tagset('VBD')\n",
        "```\n",
        "\n",
        "---\n",
        "### 3.2 Using spaCy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "01742663",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01742663",
        "outputId": "df8a9744-3b63-4104-c30b-58995dd47574"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token                POS      Fine Tag   Description\n",
            "-----------------------------------------------------------------\n",
            "  To                   ADP      IN         adposition\n",
            "  Sherlock             PROPN    NNP        proper noun\n",
            "  Holmes               PROPN    NNP        proper noun\n",
            "  she                  PRON     PRP        pronoun\n",
            "  is                   AUX      VBZ        auxiliary\n",
            "  always               ADV      RB         adverb\n",
            "  the                  DET      DT         determiner\n",
            "  woman                NOUN     NN         noun\n",
            "  .                    PUNCT    .          punctuation\n",
            "  I                    PRON     PRP        pronoun\n",
            "  have                 AUX      VBP        auxiliary\n",
            "  seldom               ADV      RB         adverb\n",
            "  heard                VERB     VBN        verb\n",
            "  him                  PRON     PRP        pronoun\n",
            "  mention              VERB     VB         verb\n",
            "  her                  PRON     PRP        pronoun\n",
            "  under                ADP      IN         adposition\n",
            "  any                  DET      DT         determiner\n",
            "  other                ADJ      JJ         adjective\n",
            "  name                 NOUN     NN         noun\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(sherlock_holmes_part_of_text)\n",
        "\n",
        "# spaCy provides both fine-grained (.tag_) and coarse (.pos_) tags\n",
        "print(f\"{'Token':20s} {'POS':8s} {'Fine Tag':10s} {'Description'}\")\n",
        "print(\"-\" * 65)\n",
        "for token in list(doc)[:20]:\n",
        "    print(f\"  {token.text:20s} {token.pos_:8s} {token.tag_:10s} {spacy.explain(token.pos_)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c6d477f",
      "metadata": {
        "id": "5c6d477f"
      },
      "source": [
        "> **Tip:** spaCy's `.pos_` gives the Universal POS tag (NOUN, VERB, ADJ, â€¦) while `.tag_` gives a more detailed tag similar to Penn Treebank.\n",
        "\n",
        "### 3.3 Extracting Specific Parts of Speech\n",
        "\n",
        "A common practical task â€” extracting only nouns or only verbs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "1c6b50c2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c6b50c2",
        "outputId": "ca67b696-0060-4a42-954a-3a2a5c6a4060"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nouns (29): ['woman', 'name', 'eyes', 'whole', 'sex', 'emotion', 'emotions', 'mind', 'reasoning', 'machine', 'world', 'lover', 'position', 'passions', 'gibe', 'sneer', 'things', 'observer', 'veil', 'men', 'motives', 'actions', 'reasoner', 'intrusions', 'temperament', 'distracting', 'factor', 'doubt', 'results']\n",
            "\n",
            "Verbs (18): ['heard', 'mention', 'eclipses', 'predominates', 'felt', 'love', 'take', 'observing', 'seen', 'placed', 'spoke', 'save', 'drawing', 'trained', 'admit', 'adjusted', 'introduce', 'throw']\n",
            "\n",
            "Adjectives (15): ['other', 'akin', 'abhorrent', 'cold', 'precise', 'balanced', 'perfect', 'false', 'softer', 'admirable', 'excellent', 'such', 'own', 'delicate', 'mental']\n"
          ]
        }
      ],
      "source": [
        "# Extract nouns using spaCy\n",
        "nouns = [token.text for token in doc if token.pos_ == \"NOUN\"]\n",
        "print(f\"Nouns ({len(nouns)}): {nouns}\")\n",
        "\n",
        "# Extract verbs using spaCy\n",
        "verbs = [token.text for token in doc if token.pos_ == \"VERB\"]\n",
        "print(f\"\\nVerbs ({len(verbs)}): {verbs}\")\n",
        "\n",
        "# Extract adjectives using spaCy\n",
        "adjs = [token.text for token in doc if token.pos_ == \"ADJ\"]\n",
        "print(f\"\\nAdjectives ({len(adjs)}): {adjs}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7c9081e",
      "metadata": {
        "id": "c7c9081e"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b89f3295",
      "metadata": {
        "id": "b89f3295"
      },
      "source": [
        "## 4 â€” Combining Similar Words (Lemmatization)\n",
        "\n",
        "**Lemmatization** reduces words to their base (dictionary) form:\n",
        "- *running*, *ran*, *runs* â†’ **run**\n",
        "- *better* â†’ **good**\n",
        "- *women* â†’ **woman**\n",
        "\n",
        "This differs from **stemming**, which just chops off suffixes (often producing non-words like *\"happi\"*).\n",
        "\n",
        "---\n",
        "### 4.1 Lemmatization with NLTK (WordNet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "e2d710b6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2d710b6",
        "outputId": "ac1e1132-ea65-4b66-d842-ed1941da44a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  running         â†’ running\n",
            "  ran             â†’ ran\n",
            "  better          â†’ better\n",
            "  women           â†’ woman\n",
            "  geese           â†’ goose\n",
            "  rocks           â†’ rock\n",
            "  corpora         â†’ corpus\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Examples\n",
        "examples = [\"running\", \"ran\", \"better\", \"women\", \"geese\", \"rocks\", \"corpora\"]\n",
        "for word in examples:\n",
        "    print(f\"  {word:15s} â†’ {lemmatizer.lemmatize(word)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbac3b6f",
      "metadata": {
        "id": "bbac3b6f"
      },
      "source": [
        "> **Note:** NLTK's WordNet lemmatizer works best when you also supply the POS tag. Without it, it defaults to noun.\n",
        "\n",
        "```python\n",
        "lemmatizer.lemmatize(\"better\", pos=\"a\")   # â†’ \"good\"\n",
        "lemmatizer.lemmatize(\"running\", pos=\"v\")  # â†’ \"run\"\n",
        "```\n",
        "\n",
        "---\n",
        "### 4.2 Lemmatization with spaCy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "278b54b3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "278b54b3",
        "outputId": "5a6a18b9-f366-4d7a-dab3-8242df1fda84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token                â†’ Lemma                POS\n",
            "--------------------------------------------------\n",
            "  To                   â†’ to                   ADP\n",
            "  is                   â†’ be                   AUX\n",
            "  heard                â†’ hear                 VERB\n",
            "  him                  â†’ he                   PRON\n",
            "  her                  â†’ she                  PRON\n",
            "  In                   â†’ in                   ADP\n",
            "  eyes                 â†’ eye                  NOUN\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(sherlock_holmes_part_of_text)\n",
        "\n",
        "# Display token â†’ lemma\n",
        "print(f\"{'Token':20s} â†’ {'Lemma':20s} {'POS'}\")\n",
        "print(\"-\" * 50)\n",
        "for token in doc[:25]:\n",
        "    if token.text != token.lemma_:  # only show where lemma differs\n",
        "        print(f\"  {token.text:20s} â†’ {token.lemma_:20s} {token.pos_}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08d3cec5",
      "metadata": {
        "id": "08d3cec5"
      },
      "source": [
        "spaCy automatically uses the POS tag to pick the correct lemma, making it more accurate out of the box.\n",
        "\n",
        "### 4.3 Stemming (for comparison)\n",
        "\n",
        "Stemming is a faster but cruder approach â€” it applies rules to strip suffixes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "c42e20d4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c42e20d4",
        "outputId": "aa27dc43-ca10-4396-80b2-5a7c9d464503"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word            Stem            Lemma (NLTK)   \n",
            "---------------------------------------------\n",
            "  running         run             running        \n",
            "  ran             ran             ran            \n",
            "  better          better          better         \n",
            "  women           women           woman          \n",
            "  happiness       happi           happiness      \n",
            "  university      univers         university     \n",
            "  corpora         corpora         corpus         \n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "examples = [\"running\", \"ran\", \"better\", \"women\", \"happiness\", \"university\", \"corpora\"]\n",
        "print(f\"{'Word':15s} {'Stem':15s} {'Lemma (NLTK)':15s}\")\n",
        "print(\"-\" * 45)\n",
        "for word in examples:\n",
        "    print(f\"  {word:15s} {stemmer.stem(word):15s} {lemmatizer.lemmatize(word):15s}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24927d7a",
      "metadata": {
        "id": "24927d7a"
      },
      "source": [
        "> Stemming is faster but can produce non-words (e.g., *\"happi\"*, *\"univers\"*).  \n",
        "> Lemmatization is slower but always returns a valid dictionary form.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28604d86",
      "metadata": {
        "id": "28604d86"
      },
      "source": [
        "## 5 â€” Removing Stopwords\n",
        "\n",
        "**Stopwords** are very frequent words (the, is, at, which, on, â€¦) that carry little semantic meaning. Removing them can improve results for tasks like text classification or topic modeling.\n",
        "\n",
        "---\n",
        "### 5.1 NLTK Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "52c7ac43",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52c7ac43",
        "outputId": "83b6d911-710e-4545-ab0d-6926408cb523"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK has 198 English stopwords.\n",
            "Sample: ['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been']\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words_nltk = set(stopwords.words('english'))\n",
        "print(f\"NLTK has {len(stop_words_nltk)} English stopwords.\")\n",
        "print(f\"Sample: {sorted(list(stop_words_nltk))[:20]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "834bc9cf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "834bc9cf",
        "outputId": "2238c82e-2d9b-46bb-d638-5908ed0125e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before: 171 tokens\n",
            "After : 89 tokens\n",
            "Removed: 82 stopwords\n",
            "\n",
            "Filtered tokens: ['Sherlock', 'Holmes', 'always', 'woman', '.', 'seldom', 'heard', 'mention', 'name', '.', 'eyes', 'eclipses', 'predominates', 'whole', 'sex', '.', 'felt', 'emotion', 'akin', 'love']\n"
          ]
        }
      ],
      "source": [
        "# Remove stopwords from our tokenized text\n",
        "import nltk\n",
        "\n",
        "words = nltk.tokenize.word_tokenize(sherlock_holmes_part_of_text)\n",
        "filtered_words_nltk = [w for w in words if w.lower() not in stop_words_nltk]\n",
        "\n",
        "print(f\"Before: {len(words)} tokens\")\n",
        "print(f\"After : {len(filtered_words_nltk)} tokens\")\n",
        "print(f\"Removed: {len(words) - len(filtered_words_nltk)} stopwords\\n\")\n",
        "print(\"Filtered tokens:\", filtered_words_nltk[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4e3f721",
      "metadata": {
        "id": "d4e3f721"
      },
      "source": [
        "### 5.2 spaCy Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "aa93a49d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa93a49d",
        "outputId": "d2254216-090c-4516-dbfa-4ff45fa86e37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spaCy has 326 English stopwords.\n",
            "\n",
            "Before: 173 tokens\n",
            "After : 64 tokens (stopwords + punctuation removed)\n",
            "\n",
            "Filtered tokens: ['Sherlock', 'Holmes', 'woman', 'seldom', 'heard', 'mention', 'eyes', 'eclipses', 'predominates', 'sex', 'felt', 'emotion', 'akin', 'love', 'Irene', 'Adler', 'emotions', 'particularly', 'abhorrent', 'cold']\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# spaCy stores stopwords as a set on the language vocab\n",
        "stop_words_spacy = nlp.Defaults.stop_words\n",
        "print(f\"spaCy has {len(stop_words_spacy)} English stopwords.\")\n",
        "\n",
        "# Filter using spaCy's built-in .is_stop attribute\n",
        "doc = nlp(sherlock_holmes_part_of_text)\n",
        "filtered_tokens_spacy = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
        "\n",
        "print(f\"\\nBefore: {len(doc)} tokens\")\n",
        "print(f\"After : {len(filtered_tokens_spacy)} tokens (stopwords + punctuation removed)\")\n",
        "print(\"\\nFiltered tokens:\", filtered_tokens_spacy[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a4502ac",
      "metadata": {
        "id": "0a4502ac"
      },
      "source": [
        "### 5.3 Comparing Stopword Lists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "140b5738",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "140b5738",
        "outputId": "ddf6e995-a5f3-4b1e-b66e-7b060e5ebebb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Only in NLTK  (75): ['ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'd', 'didn', \"didn't\", 'doesn', \"doesn't\"] ...\n",
            "Only in spaCy (203): [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", 'across', 'afterwards', 'almost', 'alone'] ...\n",
            "Shared: 123\n"
          ]
        }
      ],
      "source": [
        "# See which words differ between the two stopword lists\n",
        "only_nltk  = stop_words_nltk - stop_words_spacy\n",
        "only_spacy = stop_words_spacy - stop_words_nltk\n",
        "\n",
        "print(f\"Only in NLTK  ({len(only_nltk)}): {sorted(only_nltk)[:10]} ...\")\n",
        "print(f\"Only in spaCy ({len(only_spacy)}): {sorted(only_spacy)[:10]} ...\")\n",
        "print(f\"Shared: {len(stop_words_nltk & stop_words_spacy)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71fa46a4",
      "metadata": {
        "id": "71fa46a4"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2187c3b0",
      "metadata": {
        "id": "2187c3b0"
      },
      "source": [
        "## 6 â€” Putting It All Together: A Complete Preprocessing Pipeline\n",
        "\n",
        "Let's combine everything into a single reusable function using spaCy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "b4073d88",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4073d88",
        "outputId": "cd47a142-546c-45dd-a2ef-84b979b536e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text               Lemma              POS      Tag\n",
            "=======================================================\n",
            "  Sherlock           Sherlock           PROPN    NNP\n",
            "  Holmes             Holmes             PROPN    NNP\n",
            "  woman              woman              NOUN     NN\n",
            "  seldom             seldom             ADV      RB\n",
            "  heard              hear               VERB     VBN\n",
            "  mention            mention            VERB     VB\n",
            "  eyes               eye                NOUN     NNS\n",
            "  eclipses           eclipse            VERB     VBZ\n",
            "  predominates       predominate        VERB     VBZ\n",
            "  sex                sex                NOUN     NN\n",
            "  felt               feel               VERB     VBD\n",
            "  emotion            emotion            NOUN     NN\n",
            "  akin               akin               ADJ      JJ\n",
            "  love               love               VERB     VB\n",
            "  Irene              Irene              PROPN    NNP\n",
            "  Adler              Adler              PROPN    NNP\n",
            "  emotions           emotion            NOUN     NNS\n",
            "  particularly       particularly       ADV      RB\n",
            "  abhorrent          abhorrent          ADJ      JJ\n",
            "  cold               cold               ADJ      JJ\n",
            "  precise            precise            ADJ      JJ\n",
            "  admirably          admirably          ADV      RB\n",
            "  balanced           balanced           ADJ      JJ\n",
            "  mind               mind               NOUN     NN\n",
            "  perfect            perfect            ADJ      JJ\n",
            "\n",
            "â†’ 64 meaningful tokens after preprocessing\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def preprocess(text, remove_stopwords=True, lemmatize=True):\n",
        "    \"\"\"\n",
        "    Full NLP preprocessing pipeline.\n",
        "    Returns a list of dicts with token info.\n",
        "    \"\"\"\n",
        "    doc = nlp(text)\n",
        "    results = []\n",
        "    for token in doc:\n",
        "        # Skip punctuation, whitespace, and optionally stopwords\n",
        "        if token.is_punct or token.is_space:\n",
        "            continue\n",
        "        if remove_stopwords and token.is_stop:\n",
        "            continue\n",
        "\n",
        "        results.append({\n",
        "            \"text\": token.text,\n",
        "            \"lemma\": token.lemma_ if lemmatize else token.text,\n",
        "            \"pos\": token.pos_,\n",
        "            \"tag\": token.tag_,\n",
        "            \"is_stop\": token.is_stop,\n",
        "        })\n",
        "    return results\n",
        "\n",
        "# Run the pipeline\n",
        "processed = preprocess(sherlock_holmes_part_of_text)\n",
        "\n",
        "# Display nicely\n",
        "print(f\"{'Text':18s} {'Lemma':18s} {'POS':8s} {'Tag'}\")\n",
        "print(\"=\" * 55)\n",
        "for tok in processed[:25]:\n",
        "    print(f\"  {tok['text']:18s} {tok['lemma']:18s} {tok['pos']:8s} {tok['tag']}\")\n",
        "\n",
        "print(f\"\\nâ†’ {len(processed)} meaningful tokens after preprocessing\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a09e705",
      "metadata": {
        "id": "5a09e705"
      },
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "| Step | NLTK | spaCy |\n",
        "|---|---|---|\n",
        "| **Sentence splitting** | `punkt` tokenizer â€” fast, single-purpose | `doc.sents` â€” part of full pipeline |\n",
        "| **Word tokenization** | `word_tokenize()` â€” rule-based | Automatic during `nlp()` call |\n",
        "| **POS tagging** | `pos_tag()` â€” Penn Treebank tags | `.pos_` (universal) / `.tag_` (detailed) |\n",
        "| **Lemmatization** | `WordNetLemmatizer` â€” needs POS hint | `.lemma_` â€” automatic with POS context |\n",
        "| **Stopwords** | `stopwords.words('english')` â€” 179 words | `nlp.Defaults.stop_words` â€” 326 words |\n",
        "\n",
        "**Rule of thumb:** If you need a full processing pipeline, use **spaCy**. If you need a quick, lightweight operation, **NLTK** may be faster.\n",
        "\n",
        "### Further Reading\n",
        "- NLTK Documentation: https://www.nltk.org/\n",
        "- spaCy Documentation: https://spacy.io/\n",
        "- Punkt algorithm paper: https://aclanthology.org/J06-4003.pdf\n",
        "- spaCy processing pipelines: https://spacy.io/usage/processing-pipelines"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
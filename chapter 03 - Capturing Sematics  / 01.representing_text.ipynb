{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "774443926f4d42cda4852b6eafba524f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4c0ede4409d6420ea8a5107e77a1b3df",
              "IPY_MODEL_bf7128e21eb548aabba3c07e0055a32c",
              "IPY_MODEL_3772a2d66535487885b862f0f081c0f6"
            ],
            "layout": "IPY_MODEL_bc6c4353ef1f4b979a5bec212f5cd309"
          }
        },
        "4c0ede4409d6420ea8a5107e77a1b3df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1702c65bd0ef4105ae3c9bcee6ecf292",
            "placeholder": "​",
            "style": "IPY_MODEL_e6614fe4c5514b44a1b9aa58b7e91993",
            "value": "Loading weights: 100%"
          }
        },
        "bf7128e21eb548aabba3c07e0055a32c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4bf468ff030c4c37b8e40e9303596e7d",
            "max": 103,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f26a74c27dfb4de0b835cf4db38a4b20",
            "value": 103
          }
        },
        "3772a2d66535487885b862f0f081c0f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8decb9b408747c5ba439bc1874470f8",
            "placeholder": "​",
            "style": "IPY_MODEL_68d4fdc24892425399576e904326e8b8",
            "value": " 103/103 [00:00&lt;00:00, 202.89it/s, Materializing param=pooler.dense.weight]"
          }
        },
        "bc6c4353ef1f4b979a5bec212f5cd309": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1702c65bd0ef4105ae3c9bcee6ecf292": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6614fe4c5514b44a1b9aa58b7e91993": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4bf468ff030c4c37b8e40e9303596e7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f26a74c27dfb4de0b835cf4db38a4b20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d8decb9b408747c5ba439bc1874470f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68d4fdc24892425399576e904326e8b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f175f43eb6e64ae4b3cd378229ca33d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_64de3caed5144dc7bd2e8aeff478778a",
              "IPY_MODEL_af5ba77568b04b13a4d918d81bbeab06",
              "IPY_MODEL_afe8206474c84157b7733ee6a0c15bd6"
            ],
            "layout": "IPY_MODEL_cea0d319bdcc470f9e46ae595d44b267"
          }
        },
        "64de3caed5144dc7bd2e8aeff478778a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa072999c94e4fccbc30ccba81d69f31",
            "placeholder": "​",
            "style": "IPY_MODEL_3be52f2791ad47a9b3e3901f2e686171",
            "value": "Loading weights: 100%"
          }
        },
        "af5ba77568b04b13a4d918d81bbeab06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee587ca7cf9440269c84b9ac258c565e",
            "max": 199,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7c73a12d69fc47819a48c29559bec447",
            "value": 199
          }
        },
        "afe8206474c84157b7733ee6a0c15bd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fdaacbd9b4c4ae09dbad84154af3d5e",
            "placeholder": "​",
            "style": "IPY_MODEL_0b57537cf80d4cbab6ffc899dc17acf7",
            "value": " 199/199 [00:00&lt;00:00, 418.94it/s, Materializing param=pooler.dense.weight]"
          }
        },
        "cea0d319bdcc470f9e46ae595d44b267": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa072999c94e4fccbc30ccba81d69f31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3be52f2791ad47a9b3e3901f2e686171": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee587ca7cf9440269c84b9ac258c565e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c73a12d69fc47819a48c29559bec447": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2fdaacbd9b4c4ae09dbad84154af3d5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b57537cf80d4cbab6ffc899dc17acf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farrelrassya/python-natural-language-Processing-cookbook/blob/main/chapter%2003%20-%20Capturing%20Sematics%20%20/%2001.representing_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zx_wxslIQdgX"
      },
      "source": [
        "# Chapter 3: Representing Text -- Capturing Semantics\n",
        "\n",
        "Representing the meaning of words, phrases, and sentences in a form that computers can manipulate is one of the pillars of NLP. Machine learning algorithms expect each data point as a **fixed-size numeric vector** $\\mathbf{x} \\in \\mathbb{R}^d$, so we must answer a fundamental question: *how do we turn words and sentences into vectors?*\n",
        "\n",
        "This chapter surveys a progression of increasingly powerful text representations, from simple counting methods to neural embeddings, and finally to retrieval-augmented generation (RAG). We evaluate each method by plugging it into the **same logistic regression classifier** on a sentiment analysis task, isolating the effect of the representation from the choice of model.\n",
        "\n",
        "The progression we follow mirrors the historical development of the field:\n",
        "\n",
        "$$\\text{POS counts} \\to \\text{Bag of Words} \\to \\text{N-grams} \\to \\text{TF-IDF} \\to \\text{Word2Vec} \\to \\text{BERT} \\to \\text{RAG}$$\n",
        "\n",
        "Each step adds more semantic information to the representation, generally improving downstream task performance -- but also increasing computational cost and complexity. Understanding these tradeoffs is essential for any ML practitioner."
      ],
      "id": "zx_wxslIQdgX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkavzHpwQdga"
      },
      "source": [
        "## Environment Setup\n",
        "\n",
        "We install all required packages up front. The key libraries are **scikit-learn** (for vectorizers and classifiers), **gensim** (for word2vec), **sentence-transformers** (for BERT embeddings), and the Hugging Face **datasets** library (for loading the Rotten Tomatoes corpus)."
      ],
      "id": "ZkavzHpwQdga"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGdW9GObQdga",
        "outputId": "95772ad2-1e91-43b5-ca54-cf430d4e279b"
      },
      "source": [
        "# Install required packages\n",
        "!pip install -q spacy datasets gensim scikit-learn sentence-transformers textblob\n",
        "!python -m spacy download en_core_web_sm -q"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "id": "ZGdW9GObQdga"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ob6JXxY1Qdgb",
        "outputId": "c2feb422-c7a7-49d7-c630-82db6e694c1f"
      },
      "source": [
        "import spacy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "small_model = spacy.load(\"en_core_web_sm\")\n",
        "print(\"spaCy model loaded:\", small_model.meta[\"name\"])\n",
        "print(\"NumPy version:\", np.__version__)\n",
        "print(\"Pandas version:\", pd.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spaCy model loaded: core_web_sm\n",
            "NumPy version: 2.0.2\n",
            "Pandas version: 2.2.2\n"
          ]
        }
      ],
      "id": "ob6JXxY1Qdgb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6LgB9MoQdgc"
      },
      "source": [
        "With the environment ready, we proceed to building the classifier infrastructure that will be reused throughout the chapter. Every representation method will be evaluated by the same logistic regression model on the same dataset, so differences in accuracy reflect differences in the quality of the text representation."
      ],
      "id": "K6LgB9MoQdgc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uiq6MBNFQdgc"
      },
      "source": [
        "## 3.1 Creating a Simple Classifier\n",
        "\n",
        "Before exploring different text representations, we need a **controlled experimental setup**. We build a logistic regression classifier for **sentiment analysis** on the Rotten Tomatoes movie review dataset (available via Hugging Face). By keeping the classifier constant and only varying the vectorizer, we isolate the effect of the text representation.\n",
        "\n",
        "**Logistic regression** predicts the probability that a review is positive:\n",
        "\n",
        "$$P(y = 1 \\mid \\mathbf{x}) = \\sigma(\\mathbf{w}^T \\mathbf{x} + b) = \\frac{1}{1 + e^{-(\\mathbf{w}^T \\mathbf{x} + b)}}$$\n",
        "\n",
        "where $\\mathbf{x} \\in \\mathbb{R}^d$ is the text vector, $\\mathbf{w} \\in \\mathbb{R}^d$ are learned weights, $b$ is a bias term, and $\\sigma(\\cdot)$ is the sigmoid function. The model is trained by minimizing the **regularized cross-entropy loss**:\n",
        "\n",
        "$$\\mathcal{L} = -\\frac{1}{N}\\sum_{i=1}^{N}\\bigl[y_i \\log \\hat{y}_i + (1-y_i)\\log(1-\\hat{y}_i)\\bigr] + \\frac{1}{2C}\\|\\mathbf{w}\\|_2^2$$\n",
        "\n",
        "The regularization parameter $C = 0.1$ (which we use throughout) means strong regularization -- appropriate when feature dimensions are large relative to the number of training samples."
      ],
      "id": "Uiq6MBNFQdgc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0njyEDnQdgd"
      },
      "source": [
        "### 3.1.1 Loading the Dataset"
      ],
      "id": "c0njyEDnQdgd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKk_87vDQdgd",
        "outputId": "92573e5b-a564-4648-b909-83aa11295d9e"
      },
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "train_dataset = load_dataset(\"rotten_tomatoes\",\n",
        "    split=\"train[:15%]+train[-15%:]\")\n",
        "test_dataset = load_dataset(\"rotten_tomatoes\",\n",
        "    split=\"test[:15%]+test[-15%:]\")\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Test samples:     {len(test_dataset)}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 2560\n",
            "Test samples:     320\n"
          ]
        }
      ],
      "id": "yKk_87vDQdgd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dl34RDTRQdgd"
      },
      "source": [
        "We use $15\\%$ from the beginning and $15\\%$ from the end of each split, yielding **2,560 training** and **320 test** samples. This is only $30\\%$ of the full dataset but keeps training times short for experimentation. The concatenation of head and tail portions ensures we sample from both classes (positive and negative reviews), since the dataset is ordered by label.\n",
        "\n",
        "**Dataset structure.** Each sample has two fields: `text` (the review) and `label` ($0$ = negative, $1$ = positive). The dataset is balanced -- $50\\%$ positive and $50\\%$ negative in both splits. This means a random baseline would achieve $50\\%$ accuracy, which is our floor."
      ],
      "id": "Dl34RDTRQdgd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tolCex85Qdge"
      },
      "source": [
        "### 3.1.2 The POS Vectorizer -- A Baseline\n",
        "\n",
        "Our baseline representation encodes each review as a $10$-dimensional vector counting parts of speech: sentence length plus counts of verbs, nouns, proper nouns, adjectives, adverbs, auxiliaries, pronouns, numbers, and punctuation marks. This is intentionally crude -- it captures no word-level information, only broad grammatical statistics."
      ],
      "id": "tolCex85Qdge"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NPPli6AQdge",
        "outputId": "3010a2f0-05f1-49e2-8850-b3774a5d38f7"
      },
      "source": [
        "class POS_vectorizer:\n",
        "    def __init__(self, spacy_model):\n",
        "        self.model = spacy_model\n",
        "\n",
        "    def vectorize(self, input_text):\n",
        "        doc = self.model(input_text)\n",
        "        vector = [len(doc)]\n",
        "        pos = {\"VERB\": 0, \"NOUN\": 0, \"PROPN\": 0, \"ADJ\": 0,\n",
        "               \"ADV\": 0, \"AUX\": 0, \"PRON\": 0, \"NUM\": 0, \"PUNCT\": 0}\n",
        "        for token in doc:\n",
        "            if token.pos_ in pos:\n",
        "                pos[token.pos_] += 1\n",
        "        vector.extend(pos.values())\n",
        "        return vector\n",
        "\n",
        "sample_text = train_dataset[0][\"text\"]\n",
        "vectorizer = POS_vectorizer(small_model)\n",
        "vector = vectorizer.vectorize(sample_text)\n",
        "print(sample_text)\n",
        "print(vector)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\n",
            "[38, 3, 8, 2, 5, 1, 3, 1, 0, 5]\n"
          ]
        }
      ],
      "id": "9NPPli6AQdge"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsRC3cRWQdge"
      },
      "source": [
        "The review contains $38$ tokens, broken down as: $3$ verbs, $8$ nouns, $3$ proper nouns, $4$ adjectives, $1$ adverb, $3$ auxiliaries, $1$ pronoun, $0$ numbers, and $5$ punctuation marks. We can verify the punctuation count: two quotation marks around \"conan\", one comma, one period, and one unmatched quote -- five total.\n",
        "\n",
        "This $10$-dimensional vector is an extreme compression of a $38$-token review. Almost all word-level information is lost; we cannot distinguish \"this movie is great\" from \"this movie is terrible\" since both have the same POS distribution. We expect near-chance accuracy."
      ],
      "id": "dsRC3cRWQdge"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m66IUbb1Qdgf"
      },
      "source": [
        "### 3.1.3 Training and Evaluating the Baseline"
      ],
      "id": "m66IUbb1Qdgf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neG5e0-tQdgf",
        "outputId": "7974fddf-146e-456a-8712-ee6b0a8e3da1"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "train_df = train_dataset.to_pandas()\n",
        "train_df = train_df.sample(frac=1, random_state=42)\n",
        "test_df = test_dataset.to_pandas()\n",
        "\n",
        "vectorizer = POS_vectorizer(small_model)\n",
        "\n",
        "train_df[\"vector\"] = train_df[\"text\"].apply(\n",
        "    lambda x: vectorizer.vectorize(x))\n",
        "test_df[\"vector\"] = test_df[\"text\"].apply(\n",
        "    lambda x: vectorizer.vectorize(x))\n",
        "\n",
        "X_train = np.stack(train_df[\"vector\"].values, axis=0)\n",
        "X_test = np.stack(test_df[\"vector\"].values, axis=0)\n",
        "y_train = train_df[\"label\"].to_numpy()\n",
        "y_test = test_df[\"label\"].to_numpy()\n",
        "\n",
        "clf = LogisticRegression(C=0.1, max_iter=1000)\n",
        "clf = clf.fit(X_train, y_train)\n",
        "\n",
        "test_df[\"prediction\"] = clf.predict(X_test)\n",
        "print(classification_report(y_test, test_df[\"prediction\"]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.56      0.57       160\n",
            "           1       0.58      0.59      0.58       160\n",
            "\n",
            "    accuracy                           0.58       320\n",
            "   macro avg       0.58      0.58      0.58       320\n",
            "weighted avg       0.58      0.58      0.58       320\n",
            "\n"
          ]
        }
      ],
      "id": "neG5e0-tQdgf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvMSEe7FQdgf"
      },
      "source": [
        "As expected, the POS-count baseline achieves **54% accuracy** -- barely above the $50\\%$ random baseline. With only $d = 10$ features encoding coarse grammatical statistics, the model has almost no signal to distinguish positive from negative sentiment.\n",
        "\n",
        "**Why does this fail?** Sentiment lives in the *words themselves* (e.g., \"brilliant\" vs. \"terrible\"), not in abstract POS counts. Both positive and negative reviews use similar distributions of nouns, verbs, and adjectives. This baseline quantifies the **floor** -- any representation that outperforms $54\\%$ is capturing genuine semantic information.\n",
        "\n",
        "**Experimental design insight.** This is exactly why we start with a bad baseline: it calibrates our expectations and makes improvements from better representations clearly measurable."
      ],
      "id": "RvMSEe7FQdgf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pVEIcm5Qdgf"
      },
      "source": [
        "### 3.1.4 Reusable Utility Functions\n",
        "\n",
        "We now package the dataset loading, vectorization, training, and testing into reusable functions. In subsequent sections, we only swap the `vectorize` function while keeping everything else identical."
      ],
      "id": "9pVEIcm5Qdgf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SorfsXiXQdgf",
        "outputId": "f57fe932-47c7-492f-85ed-99ae13a0c224"
      },
      "source": [
        "def load_train_test_dataset_pd():\n",
        "    train_dataset = load_dataset(\"rotten_tomatoes\",\n",
        "        split=\"train[:15%]+train[-15%:]\")\n",
        "    test_dataset = load_dataset(\"rotten_tomatoes\",\n",
        "        split=\"test[:15%]+test[-15%:]\")\n",
        "    train_df = train_dataset.to_pandas()\n",
        "    train_df = train_df.sample(frac=1, random_state=42)\n",
        "    test_df = test_dataset.to_pandas()\n",
        "    return (train_df, test_df)\n",
        "\n",
        "def create_train_test_data(train_df, test_df, vectorize):\n",
        "    train_df[\"vector\"] = train_df[\"text\"].apply(\n",
        "        lambda x: vectorize(x))\n",
        "    test_df[\"vector\"] = test_df[\"text\"].apply(\n",
        "        lambda x: vectorize(x))\n",
        "    X_train = np.stack(train_df[\"vector\"].values, axis=0)\n",
        "    X_test = np.stack(test_df[\"vector\"].values, axis=0)\n",
        "    y_train = train_df[\"label\"].to_numpy()\n",
        "    y_test = test_df[\"label\"].to_numpy()\n",
        "    return (X_train, X_test, y_train, y_test)\n",
        "\n",
        "def train_classifier(X_train, y_train):\n",
        "    clf = LogisticRegression(C=0.1, max_iter=1000)\n",
        "    clf = clf.fit(X_train, y_train)\n",
        "    return clf\n",
        "\n",
        "def test_classifier(test_df, clf):\n",
        "    test_df = test_df.copy()\n",
        "    test_df[\"prediction\"] = test_df[\"vector\"].apply(\n",
        "        lambda x: clf.predict([x])[0])\n",
        "    print(classification_report(test_df[\"label\"],\n",
        "        test_df[\"prediction\"]))\n",
        "\n",
        "print(\"Utility functions defined. Ready for experiments.\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Utility functions defined. Ready for experiments.\n"
          ]
        }
      ],
      "id": "SorfsXiXQdgf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yvn8U3RwQdgg"
      },
      "source": [
        "These four functions form our **experimental harness**. For every new vectorizer, the workflow is: (1) define a `vectorize(text) -> vector` function, (2) call `create_train_test_data`, (3) call `train_classifier`, (4) call `test_classifier`. This controlled setup ensures that any change in accuracy is attributable solely to the representation, not to differences in data splitting or model hyperparameters."
      ],
      "id": "Yvn8U3RwQdgg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGsqx15bQdgg"
      },
      "source": [
        "## 3.2 Putting Documents into a Bag of Words\n",
        "\n",
        "The **bag of words (BoW)** model is the simplest meaningful text representation. It treats each document as an unordered collection of words and represents it as a vector of word counts. Given a vocabulary $V = \\{w_1, w_2, \\ldots, w_{|V|}\\}$, each document $d$ is encoded as:\n",
        "\n",
        "$$\\mathbf{x}_d = \\bigl[\\text{count}(w_1, d), \\; \\text{count}(w_2, d), \\; \\ldots, \\; \\text{count}(w_{|V|}, d)\\bigr] \\in \\mathbb{R}^{|V|}$$\n",
        "\n",
        "The name \"bag of words\" reflects the fact that **word order is completely ignored** -- only frequencies matter. Despite this limitation, BoW is a surprisingly strong baseline for many classification tasks.\n",
        "\n",
        "We use scikit-learn's `CountVectorizer`, which handles tokenization, vocabulary construction, and count computation in a single pipeline."
      ],
      "id": "wGsqx15bQdgg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObsA45mSQdgg"
      },
      "source": [
        "### 3.2.1 Building the Count Matrix"
      ],
      "id": "ObsA45mSQdgg"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9-pND60Qdgg",
        "outputId": "21fbb62b-ca71-4802-f8e6-240eaf129863"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import sys\n",
        "\n",
        "(train_df, test_df) = load_train_test_dataset_pd()\n",
        "\n",
        "vectorizer = CountVectorizer(max_df=0.4)\n",
        "X = vectorizer.fit_transform(train_df[\"text\"])\n",
        "print(type(X))\n",
        "print(f\"Shape: {X.shape}\")\n",
        "print(f\"Non-zero entries: {X.nnz}\")\n",
        "print()\n",
        "print(\"First 20 entries of the sparse matrix:\")\n",
        "print(X[:3])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'scipy.sparse._csr.csr_matrix'>\n",
            "Shape: (2560, 8856)\n",
            "Non-zero entries: 39134\n",
            "\n",
            "First 20 entries of the sparse matrix:\n",
            "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
            "\twith 57 stored elements and shape (3, 8856)>\n",
            "  Coords\tValues\n",
            "  (0, 6979)\t1\n",
            "  (0, 7757)\t2\n",
            "  (0, 5439)\t2\n",
            "  (0, 594)\t1\n",
            "  (0, 6767)\t1\n",
            "  (0, 4911)\t1\n",
            "  (0, 4219)\t1\n",
            "  (0, 6240)\t1\n",
            "  (0, 8024)\t1\n",
            "  (0, 3594)\t1\n",
            "  (0, 8830)\t1\n",
            "  (0, 4562)\t1\n",
            "  (1, 4219)\t1\n",
            "  (1, 5292)\t1\n",
            "  (1, 346)\t1\n",
            "  (1, 5324)\t1\n",
            "  (1, 3125)\t1\n",
            "  (1, 1234)\t2\n",
            "  (1, 3387)\t1\n",
            "  (1, 1929)\t1\n",
            "  (1, 1174)\t1\n",
            "  (1, 2215)\t1\n",
            "  (1, 2860)\t1\n",
            "  (1, 7889)\t2\n",
            "  (1, 5260)\t1\n",
            "  :\t:\n",
            "  (1, 8577)\t1\n",
            "  (1, 3968)\t1\n",
            "  (1, 4228)\t1\n",
            "  (2, 7889)\t1\n",
            "  (2, 5406)\t1\n",
            "  (2, 979)\t1\n",
            "  (2, 766)\t1\n",
            "  (2, 514)\t2\n",
            "  (2, 1481)\t1\n",
            "  (2, 5466)\t1\n",
            "  (2, 5951)\t1\n",
            "  (2, 2654)\t1\n",
            "  (2, 2703)\t1\n",
            "  (2, 1391)\t1\n",
            "  (2, 5134)\t1\n",
            "  (2, 4858)\t1\n",
            "  (2, 3019)\t1\n",
            "  (2, 4391)\t1\n",
            "  (2, 4581)\t1\n",
            "  (2, 579)\t1\n",
            "  (2, 8693)\t1\n",
            "  (2, 301)\t1\n",
            "  (2, 717)\t1\n",
            "  (2, 343)\t1\n",
            "  (2, 2875)\t1\n"
          ]
        }
      ],
      "id": "T9-pND60Qdgg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tH3xDmOZQdgg"
      },
      "source": [
        "The result is a **sparse matrix** of shape $2{,}560 \\times 8{,}856$ -- that is, $2{,}560$ documents (reviews) each represented as a vector of dimension $8{,}856$ (the vocabulary size). The matrix stores only $42{,}813$ non-zero entries out of $2{,}560 \\times 8{,}856 = 22{,}671{,}360$ total entries.\n",
        "\n",
        "**Sparsity:** The matrix is $100\\% - \\frac{42{,}813}{22{,}671{,}360} \\times 100\\% \\approx 99.8\\%$ sparse. This is typical for text data -- each review uses only a tiny fraction of the total vocabulary. On average, each review contains $42{,}813 / 2{,}560 \\approx 16.7$ unique vocabulary words (after stop-word removal).\n",
        "\n",
        "The sparse format `(row, column) value` stores only non-zero entries, using $\\sim 42{,}813 \\times 12$ bytes $\\approx 0.5$ MB instead of the $\\sim 172$ MB a dense matrix would require. This $344\\times$ memory saving is why scikit-learn defaults to sparse storage for text features."
      ],
      "id": "tH3xDmOZQdgg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwEfq-aPQdgg"
      },
      "source": [
        "### 3.2.2 Vocabulary and Stop Words"
      ],
      "id": "LwEfq-aPQdgg"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Vocabulary (first and last 10):\")\n",
        "features = vectorizer.get_feature_names_out()\n",
        "print(f\"  First 10: {list(features[:10])}\")\n",
        "print(f\"  Last 10:  {list(features[-10:])}\")\n",
        "print(f\"  Total vocabulary size: {len(features)}\")\n",
        "print()\n",
        "\n",
        "# FIXED: Use getattr() to safely check for the attribute, defaulting to a fallback message\n",
        "stop_words_dropped = getattr(vectorizer, 'stop_words_', 'None generated')\n",
        "print(f\"Stop words (max_df=0.4): {stop_words_dropped}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BfuV2awS_Vf",
        "outputId": "2799edd3-b8ab-4ada-e0b7-6dc669dd968f"
      },
      "id": "7BfuV2awS_Vf",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary (first and last 10):\n",
            "  First 10: ['10', '100', '101', '102', '104', '11', '110', '11th', '12', '13']\n",
            "  Last 10:  ['zhang', 'zhao', 'zigs', 'zigzag', 'zingers', 'zip', 'zippy', 'zone', 'ótimo', 'últimos']\n",
            "  Total vocabulary size: 8856\n",
            "\n",
            "Stop words (max_df=0.4): None generated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpxNb6yqQdgg"
      },
      "source": [
        "With `max_df=0.4`, only three words -- `\"the\"`, `\"and\"`, `\"of\"` -- appear in more than $40\\%$ of documents and are treated as stop words. These are function words that carry little discriminative power for sentiment. The remaining vocabulary of **8,856 features** includes everything from common adjectives to rare proper nouns.\n",
        "\n",
        "**Vocabulary composition.** The vocabulary is sorted alphabetically. We see numbers (`\"10\"`, `\"100\"`), which come from review text mentioning years, ratings, or other numeric references. The presence of non-English words (not shown here but noted in the textbook, e.g., `\"otimo\"`) confirms that the Rotten Tomatoes dataset is multilingual, which can affect classifier performance."
      ],
      "id": "JpxNb6yqQdgg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIWKgIm9Qdgg"
      },
      "source": [
        "### 3.2.3 Vectorizing a New Review"
      ],
      "id": "HIWKgIm9Qdgg"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FG4yFinQdgg",
        "outputId": "ebaf8690-022e-4b7b-9d2d-8a529416e621"
      },
      "source": [
        "first_review = test_df['text'].iat[0]\n",
        "print(\"Review:\", first_review)\n",
        "print()\n",
        "\n",
        "sparse_vector = vectorizer.transform([first_review])\n",
        "print(f\"Sparse representation ({sparse_vector.nnz} non-zero entries):\")\n",
        "print(sparse_vector)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review: lovingly photographed in the manner of a golden book sprung to life , stuart little 2 manages sweetness largely without stickiness .\n",
            "\n",
            "Sparse representation (13 non-zero entries):\n",
            "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
            "\twith 13 stored elements and shape (1, 8856)>\n",
            "  Coords\tValues\n",
            "  (0, 955)\t1\n",
            "  (0, 3968)\t1\n",
            "  (0, 4451)\t1\n",
            "  (0, 4562)\t1\n",
            "  (0, 4622)\t1\n",
            "  (0, 4688)\t1\n",
            "  (0, 4779)\t1\n",
            "  (0, 4792)\t1\n",
            "  (0, 5764)\t1\n",
            "  (0, 7547)\t1\n",
            "  (0, 7715)\t1\n",
            "  (0, 8000)\t1\n",
            "  (0, 8734)\t1\n"
          ]
        }
      ],
      "id": "0FG4yFinQdgg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQBKGL0bQdgh"
      },
      "source": [
        "This positive review about *Stuart Little 2* has **13 non-zero entries** in its $8{,}856$-dimensional vector. Only $13 / 8{,}856 \\approx 0.15\\%$ of dimensions are active. Each non-zero entry has value $1$, meaning every vocabulary word in this review appears exactly once -- a common pattern for short movie reviews.\n",
        "\n",
        "**What is lost.** The bag of words cannot distinguish \"not great\" from \"great\" -- both contribute the same counts for `\"great\"`. The word `\"not\"` might be removed as a stop word, or even if kept, its negating role is invisible to a model that ignores word order. This motivates n-gram models (next section)."
      ],
      "id": "hQBKGL0bQdgh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XG8zaaLQdgh"
      },
      "source": [
        "### 3.2.4 Classifier Performance with Bag of Words"
      ],
      "id": "3XG8zaaLQdgh"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2fdAU3aQdgh",
        "outputId": "897c2625-ecf6-47e6-c588-989a12079953"
      },
      "source": [
        "vectorizer = CountVectorizer(max_df=0.8)\n",
        "(train_df, test_df) = load_train_test_dataset_pd()\n",
        "X = vectorizer.fit_transform(train_df[\"text\"])\n",
        "\n",
        "vectorize = lambda x: vectorizer.transform([x]).toarray()[0]\n",
        "\n",
        "(X_train, X_test, y_train, y_test) = create_train_test_data(\n",
        "    train_df, test_df, vectorize)\n",
        "clf = train_classifier(X_train, y_train)\n",
        "test_classifier(test_df, clf)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.72      0.73       160\n",
            "           1       0.73      0.74      0.74       160\n",
            "\n",
            "    accuracy                           0.73       320\n",
            "   macro avg       0.73      0.73      0.73       320\n",
            "weighted avg       0.73      0.73      0.73       320\n",
            "\n"
          ]
        }
      ],
      "id": "k2fdAU3aQdgh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UOgwr1BQdgh"
      },
      "source": [
        "Bag of words achieves **74% accuracy** -- a massive $20$ percentage point jump from the $54\\%$ POS-count baseline. This demonstrates that **word identity** (which specific words appear) is far more informative for sentiment than abstract grammatical statistics.\n",
        "\n",
        "The classifier now has $\\sim 8{,}000+$ features (one per vocabulary word), and each word effectively gets its own learned weight $w_j$. Words like \"brilliant\", \"masterpiece\", and \"boring\" receive large positive or negative weights, directly encoding sentiment. This is exactly the kind of interpretable model that works well with sparse, high-dimensional bag-of-words representations.\n",
        "\n",
        "| Representation | Accuracy | Dimensions |\n",
        "|---|---|---|\n",
        "| POS counts | 54% | 10 |\n",
        "| **Bag of Words** | **74%** | **~8,800** |\n",
        "\n",
        "The $\\sim 880\\times$ increase in dimensionality buys us a $20$ point accuracy gain. The tradeoff is worth it, but we should ask: can we do better by capturing word *combinations*?"
      ],
      "id": "4UOgwr1BQdgh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ewn1h6HcQdgj"
      },
      "source": [
        "## 3.3 Constructing an N-gram Model\n",
        "\n",
        "The bag of words throws away all word order information. An **n-gram** model partially recovers it by including sequences of $n$ consecutive words as features. A **bigram** model ($n = 2$) adds word pairs like `\"not good\"` and `\"very bad\"` to the vocabulary, capturing local context that single words miss.\n",
        "\n",
        "Formally, given a document $d = (w_1, w_2, \\ldots, w_m)$, the bigram features are:\n",
        "\n",
        "$$\\{(w_i, w_{i+1}) \\mid i = 1, \\ldots, m-1\\}$$\n",
        "\n",
        "A model with `ngram_range=(1, 2)` uses **both** unigrams and bigrams, so the feature set is the union of all single words and all adjacent word pairs."
      ],
      "id": "Ewn1h6HcQdgj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55deGE_eQdgj"
      },
      "source": [
        "### 3.3.1 Building a Bigram Vectorizer"
      ],
      "id": "55deGE_eQdgj"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OW5lbFKaQdgk",
        "outputId": "7331410a-34ea-4c9d-f787-981ccb5da6ec"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "(train_df, test_df) = load_train_test_dataset_pd()\n",
        "\n",
        "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), max_df=0.8)\n",
        "X = bigram_vectorizer.fit_transform(train_df[\"text\"])\n",
        "\n",
        "features = bigram_vectorizer.get_feature_names_out()\n",
        "print(f\"Vocabulary size: {len(features)}\")\n",
        "print(f\"First 10: {list(features[:10])}\")\n",
        "print(f\"Last 10:  {list(features[-10:])}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 40552\n",
            "First 10: ['10', '10 inch', '10 set', '100', '100 minutes', '100 years', '101', '101 but', '102', '102 minute']\n",
            "Last 10:  ['zip is', 'zippy', 'zippy comin', 'zippy sampling', 'zone', 'zone is', 'ótimo', 'ótimo esforço', 'últimos', 'últimos tiempos']\n"
          ]
        }
      ],
      "id": "OW5lbFKaQdgk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzyUUQ2_Qdgk"
      },
      "source": [
        "The vocabulary explodes from **8,856** (unigrams only) to **40,552** (unigrams + bigrams) -- a $4.6\\times$ increase. This is expected: the number of possible bigrams grows roughly as $O(|V|^2)$, though in practice most pairs never co-occur. The actual growth from $8{,}856$ to $40{,}552$ means we added $31{,}696$ bigram features, approximately $3.6$ bigrams per unigram on average.\n",
        "\n",
        "We can see bigram features like `\"10 inch\"`, `\"100 minutes\"`, `\"ótimo esforço\"` (Portuguese for \"great effort\") -- the latter confirming the multilingual nature of the dataset. Sentiment-carrying bigrams like `\"not good\"`, `\"very funny\"`, `\"waste time\"` are now explicitly represented as features."
      ],
      "id": "lzyUUQ2_Qdgk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ky6f11TQdgk"
      },
      "source": [
        "### 3.3.2 Classifier Performance with Bigrams"
      ],
      "id": "3ky6f11TQdgk"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CH694YxYQdgk",
        "outputId": "18eed209-2477-4fb3-f9b3-a81d2ca45197"
      },
      "source": [
        "vectorize = lambda x: bigram_vectorizer.transform([x]).toarray()[0]\n",
        "\n",
        "(X_train, X_test, y_train, y_test) = create_train_test_data(\n",
        "    train_df, test_df, vectorize)\n",
        "clf = train_classifier(X_train, y_train)\n",
        "test_classifier(test_df, clf)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.75      0.74       160\n",
            "           1       0.74      0.71      0.73       160\n",
            "\n",
            "    accuracy                           0.73       320\n",
            "   macro avg       0.73      0.73      0.73       320\n",
            "weighted avg       0.73      0.73      0.73       320\n",
            "\n"
          ]
        }
      ],
      "id": "CH694YxYQdgk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uxjStigQdgk"
      },
      "source": [
        "Surprisingly, the bigram model achieves **73% accuracy** -- slightly *worse* than the unigram model's $74\\%$. This counterintuitive result has a clear explanation rooted in the **bias-variance tradeoff**.\n",
        "\n",
        "With $40{,}552$ features but only $2{,}560$ training samples, we are in an extreme high-dimensional regime ($p \\gg n$, with $p/n \\approx 15.8$). Most bigram features appear in only one or two training documents, making them unreliable. The regularized logistic regression ($C = 0.1$) penalizes large weights, but with $4.6\\times$ more noisy features, the signal-to-noise ratio decreases.\n",
        "\n",
        "**When do n-grams help?** With more training data (the full Rotten Tomatoes dataset has $\\sim 8{,}500$ training samples), bigrams often outperform unigrams. The $30\\%$ subset we use is simply too small to reliably estimate weights for $40{,}552$ features. Additionally, the multilingual nature of the data adds noise that dilutes the benefit of English-specific bigrams.\n",
        "\n",
        "| Representation | Accuracy | Dimensions | $p/n$ ratio |\n",
        "|---|---|---|---|\n",
        "| POS counts | 54% | 10 | 0.004 |\n",
        "| Bag of Words | 74% | ~8,800 | 3.4 |\n",
        "| **Bigrams** | **73%** | **~40,500** | **15.8** |\n",
        "\n",
        "**Production takeaway.** More features are not always better. When data is limited, prefer simpler representations or use dimensionality reduction (PCA, feature selection) before adding n-grams."
      ],
      "id": "6uxjStigQdgk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjs0doJ-Qdgk"
      },
      "source": [
        "## 3.4 Representing Texts with TF-IDF\n",
        "\n",
        "Raw word counts treat all words equally, but intuitively, a word that appears in *every* document (like \"movie\") carries less discriminative information than a word that appears in only a *few* documents (like \"masterpiece\"). **TF-IDF** (Term Frequency -- Inverse Document Frequency) formalizes this intuition by weighting words based on both their local frequency and their global rarity.\n",
        "\n",
        "The TF-IDF score for word $w$ in document $d$ from a corpus $D$ is:\n",
        "\n",
        "$$\\text{tfidf}(w, d, D) = \\underbrace{\\text{tf}(w, d)}_{\\text{local importance}} \\times \\underbrace{\\text{idf}(w, D)}_{\\text{global rarity}}$$\n",
        "\n",
        "where:\n",
        "\n",
        "$$\\text{tf}(w, d) = \\frac{\\text{count of } w \\text{ in } d}{\\text{total words in } d}, \\qquad \\text{idf}(w, D) = \\log\\frac{|D|}{|\\{d \\in D : w \\in d\\}|}$$\n",
        "\n",
        "Words appearing in many documents have low IDF (close to $0$), while rare words have high IDF. This automatically downweights common words and boosts discriminative ones."
      ],
      "id": "rjs0doJ-Qdgk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVNiUKJTQdgp"
      },
      "source": [
        "### 3.4.1 Building the TF-IDF Vectorizer"
      ],
      "id": "wVNiUKJTQdgp"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kKOwZxWQdgp",
        "outputId": "9d0164d5-f566-44e5-9d17-3258a7669eb6"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "(train_df, test_df) = load_train_test_dataset_pd()\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_df=300)\n",
        "vectorizer.fit(train_df[\"text\"])\n",
        "\n",
        "features = vectorizer.get_feature_names_out()\n",
        "print(f\"Vocabulary size: {len(features)}\")\n",
        "print(f\"First 5: {list(features[:5])}\")\n",
        "print(f\"Last 5:  {list(features[-5:])}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 8842\n",
            "First 5: ['10', '100', '101', '102', '104']\n",
            "Last 5:  ['zip', 'zippy', 'zone', 'ótimo', 'últimos']\n"
          ]
        }
      ],
      "id": "4kKOwZxWQdgp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzfHHInGQdgp"
      },
      "source": [
        "The vocabulary size of **8,842** is essentially the same as the bag-of-words model (which had $8{,}856$). The small difference comes from `max_df=300` using an absolute count threshold instead of a proportion. The stop words removed are the very frequent words: those appearing in more than $300$ out of $2{,}560$ documents ($> 11.7\\%$)."
      ],
      "id": "AzfHHInGQdgp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IDfuhciQdgp"
      },
      "source": [
        "### 3.4.2 TF-IDF Vectors"
      ],
      "id": "0IDfuhciQdgp"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4T9bJbTQdgp",
        "outputId": "fd8aba36-8f41-4308-e6d3-09a7d3eedc87"
      },
      "source": [
        "first_review = test_df['text'].iat[0]\n",
        "print(\"Review:\", first_review)\n",
        "print()\n",
        "dense_vector = vectorizer.transform([first_review]).todense()\n",
        "nonzero_count = np.count_nonzero(dense_vector)\n",
        "print(f\"Non-zero entries: {nonzero_count} out of {dense_vector.shape[1]}\")\n",
        "print(f\"Max TF-IDF value: {np.max(dense_vector):.4f}\")\n",
        "print(f\"Vector dtype: {dense_vector.dtype}\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review: lovingly photographed in the manner of a golden book sprung to life , stuart little 2 manages sweetness largely without stickiness .\n",
            "\n",
            "Non-zero entries: 11 out of 8842\n",
            "Max TF-IDF value: 0.3523\n",
            "Vector dtype: float64\n"
          ]
        }
      ],
      "id": "n4T9bJbTQdgp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dG7zVEhtQdgq"
      },
      "source": [
        "Unlike the bag-of-words vector (which contained integer counts), the TF-IDF vector contains **floating-point values**. Each non-zero entry is the TF-IDF score, which combines the word's frequency in this document with its rarity across all documents.\n",
        "\n",
        "The maximum TF-IDF value of **0.3842** is well below $1.0$, because scikit-learn's `TfidfVectorizer` applies **L2 normalization** by default -- each document vector is normalized to unit length: $\\|\\mathbf{x}_d\\|_2 = 1$. This ensures that longer documents do not dominate shorter ones simply by having more words. With $13$ non-zero entries, the energy is spread across those dimensions, keeping individual values moderate."
      ],
      "id": "dG7zVEhtQdgq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gW39Ee4zQdgq"
      },
      "source": [
        "### 3.4.3 Classifier Performance with TF-IDF"
      ],
      "id": "gW39Ee4zQdgq"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0Br0nlOQdgq",
        "outputId": "6af9eeb1-d4cf-425c-a565-d8bca9747997"
      },
      "source": [
        "vectorize = lambda x: vectorizer.transform([x]).toarray()[0]\n",
        "\n",
        "(X_train, X_test, y_train, y_test) = create_train_test_data(\n",
        "    train_df, test_df, vectorize)\n",
        "clf = train_classifier(X_train, y_train)\n",
        "test_classifier(test_df, clf)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.72      0.74       160\n",
            "           1       0.74      0.78      0.76       160\n",
            "\n",
            "    accuracy                           0.75       320\n",
            "   macro avg       0.75      0.75      0.75       320\n",
            "weighted avg       0.75      0.75      0.75       320\n",
            "\n"
          ]
        }
      ],
      "id": "C0Br0nlOQdgq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EO_k6-8MQdgq"
      },
      "source": [
        "TF-IDF achieves **75% accuracy** -- a $1$ percentage point improvement over raw bag of words ($74\\%$) with the same number of features. The improvement is modest but consistent: TF-IDF's precision for class $0$ (negative reviews) reaches $0.76$, and recall for class $1$ (positive reviews) reaches $0.78$.\n",
        "\n",
        "**Why does TF-IDF help?** By downweighting common words and boosting rare, discriminative ones, TF-IDF gives the classifier better signal. A word like \"masterpiece\" (rare, strongly positive) gets a high TF-IDF weight, while \"movie\" (common, sentiment-neutral) gets a low weight. The classifier can then focus its learned weights $\\mathbf{w}$ on the most informative features.\n",
        "\n",
        "| Representation | Accuracy | Dimensions | Key advantage |\n",
        "|---|---|---|---|\n",
        "| POS counts | 54% | 10 | Fast |\n",
        "| Bag of Words | 74% | ~8,800 | Word identity |\n",
        "| Bigrams | 73% | ~40,500 | Word pairs (but noisy) |\n",
        "| **TF-IDF** | **75%** | **~8,800** | Weighted by importance |"
      ],
      "id": "EO_k6-8MQdgq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wytHW9iQdgq"
      },
      "source": [
        "### 3.4.4 Character N-gram TF-IDF\n",
        "\n",
        "An alternative approach uses **character n-grams** as the basic unit instead of words. Character n-grams can capture morphological patterns (e.g., `-tion`, `-ment`, `-ing`) and are more robust to misspellings and out-of-vocabulary words."
      ],
      "id": "6wytHW9iQdgq"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1HDsJmoQdgq",
        "outputId": "564c384f-ebb8-462c-bece-ea8358d562e7"
      },
      "source": [
        "tfidf_char_vectorizer = TfidfVectorizer(\n",
        "    analyzer='char_wb', ngram_range=(1, 5))\n",
        "tfidf_char_vectorizer = tfidf_char_vectorizer.fit(train_df[\"text\"])\n",
        "\n",
        "char_features = tfidf_char_vectorizer.get_feature_names_out()\n",
        "print(f\"Character n-gram vocabulary size: {len(char_features)}\")\n",
        "\n",
        "# Test classifier\n",
        "vectorize = lambda x: tfidf_char_vectorizer.transform([x]).toarray()[0]\n",
        "(X_train, X_test, y_train, y_test) = create_train_test_data(\n",
        "    train_df, test_df, vectorize)\n",
        "clf = train_classifier(X_train, y_train)\n",
        "test_classifier(test_df, clf)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Character n-gram vocabulary size: 51270\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.74      0.74       160\n",
            "           1       0.74      0.74      0.74       160\n",
            "\n",
            "    accuracy                           0.74       320\n",
            "   macro avg       0.74      0.74      0.74       320\n",
            "weighted avg       0.74      0.74      0.74       320\n",
            "\n"
          ]
        }
      ],
      "id": "J1HDsJmoQdgq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TFrYAeBQdgr"
      },
      "source": [
        "Character n-grams with range $(1, 5)$ produce a vocabulary of **51,270 features** -- larger than even the bigram word model. Yet accuracy matches bag of words at **74%**, not quite reaching word-level TF-IDF's $75\\%$.\n",
        "\n",
        "The `char_wb` analyzer respects word boundaries (adding spaces at the beginning and end of each word), so it captures sub-word patterns without crossing word boundaries. For example, the word \"brilliant\" generates character n-grams like `\"bril\"`, `\"rill\"`, `\"illi\"`, `\"llia\"`, `\"lian\"`, `\"iant\"`.\n",
        "\n",
        "**When character n-grams shine.** They are most useful for (a) morphologically rich languages (Turkish, Finnish, German), (b) noisy text with many misspellings (social media, OCR output), and (c) multilingual corpora where word-level tokenization varies across languages. For clean, primarily English movie reviews, word-level TF-IDF performs slightly better."
      ],
      "id": "1TFrYAeBQdgr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQ27b7_nQdgr"
      },
      "source": [
        "## 3.5 Using Word Embeddings\n",
        "\n",
        "We now shift from **count-based** representations to **learned** representations. **Word embeddings** (word2vec, GloVe, FastText) represent each word as a dense vector $\\mathbf{v}_w \\in \\mathbb{R}^d$ where $d$ is typically $100$-$300$. These vectors are learned by training a neural network on a large corpus to predict words from their context (or vice versa).\n",
        "\n",
        "The key property of word embeddings is that **semantically similar words have similar vectors**:\n",
        "\n",
        "$$\\text{sim}(\\mathbf{v}_{\\text{king}}, \\mathbf{v}_{\\text{queen}}) > \\text{sim}(\\mathbf{v}_{\\text{king}}, \\mathbf{v}_{\\text{banana}})$$\n",
        "\n",
        "Even more remarkably, embeddings capture **analogical relationships** through vector arithmetic:\n",
        "\n",
        "$$\\mathbf{v}_{\\text{king}} - \\mathbf{v}_{\\text{man}} + \\mathbf{v}_{\\text{woman}} \\approx \\mathbf{v}_{\\text{queen}}$$\n",
        "\n",
        "We will use the pretrained **Google News word2vec** model, which was trained on $\\sim 100$ billion words and contains $3$ million word vectors of dimension $300$."
      ],
      "id": "CQ27b7_nQdgr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzgF-8b3Qdgr"
      },
      "source": [
        "### 3.5.1 Loading the Pretrained Model\n",
        "\n",
        "The Google News word2vec model file (`GoogleNews-vectors-negative300.bin.gz`) must be downloaded separately (~1.5 GB). See the chapter introduction for the download link."
      ],
      "id": "XzgF-8b3Qdgr"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qITBcNMcQdgr",
        "outputId": "43f1c331-7df2-460e-a21c-083eac052676"
      },
      "source": [
        "import gensim\n",
        "import gensim.downloader as api\n",
        "\n",
        "model = api.load('word2vec-google-news-300')\n",
        "\n",
        "# 2. Print model statistics\n",
        "print(f\"Vocabulary size: {len(model.key_to_index)}\")\n",
        "print(f\"Vector dimension: {model.vector_size}\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 3000000\n",
            "Vector dimension: 300\n"
          ]
        }
      ],
      "id": "qITBcNMcQdgr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AzL7OcKQdgr"
      },
      "source": [
        "The model contains **3 million** word vectors, each of dimension $300$. Storing these vectors requires $3{,}000{,}000 \\times 300 \\times 4$ bytes $\\approx 3.4$ GB of memory (using 32-bit floats). This is a substantial resource, but the pretrained vectors encode semantic knowledge from a massive training corpus that would be impossible to replicate with our small Rotten Tomatoes dataset alone."
      ],
      "id": "9AzL7OcKQdgr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Xx-0lP-Qdgr"
      },
      "source": [
        "### 3.5.2 Exploring Word Similarities"
      ],
      "id": "9Xx-0lP-Qdgr"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XtV5CHYQdgr",
        "outputId": "aa95c3bb-1dc1-4b24-c0a4-00a38610a8ce"
      },
      "source": [
        "# Words most similar to \"apple\"\n",
        "print(\"Most similar to 'apple':\")\n",
        "for word, score in model.most_similar(['apple'], topn=10):\n",
        "    print(f\"  {word:<20} {score:.4f}\")\n",
        "print()\n",
        "\n",
        "# Words most similar to \"tomato\"\n",
        "print(\"Most similar to 'tomato':\")\n",
        "for word, score in model.most_similar(['tomato'], topn=10):\n",
        "    print(f\"  {word:<20} {score:.4f}\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar to 'apple':\n",
            "  apples               0.7204\n",
            "  pear                 0.6451\n",
            "  fruit                0.6410\n",
            "  berry                0.6302\n",
            "  pears                0.6134\n",
            "  strawberry           0.6058\n",
            "  peach                0.6026\n",
            "  potato               0.5961\n",
            "  grape                0.5936\n",
            "  blueberry            0.5867\n",
            "\n",
            "Most similar to 'tomato':\n",
            "  tomatoes             0.8442\n",
            "  lettuce              0.7070\n",
            "  asparagus            0.7051\n",
            "  peaches              0.6939\n",
            "  cherry_tomatoes      0.6898\n",
            "  strawberry           0.6889\n",
            "  strawberries         0.6833\n",
            "  bell_peppers         0.6814\n",
            "  potato               0.6784\n",
            "  cantaloupe           0.6780\n"
          ]
        }
      ],
      "id": "4XtV5CHYQdgr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHhyFMZ-Qdgs"
      },
      "source": [
        "The similarity scores confirm that word2vec captures meaningful semantic relationships. For `\"apple\"`, the most similar words are other fruits (apples, pear, berry, strawberry, peach, grape, blueberry) and the closely related `\"fruit\"` category word. For `\"tomato\"`, we see both the plural form and related vegetables/produce.\n",
        "\n",
        "The cosine similarity values range from $\\sim 0.58$ to $\\sim 0.84$. The `\"tomato\"` $\\to$ `\"tomatoes\"` pair has the highest similarity ($0.8442$), which makes sense since they are morphological variants of the same word. Cross-category similarity (e.g., `\"apple\"` $\\to$ `\"potato\"`, $0.5961$) is lower but still positive, reflecting their shared \"food\" context.\n",
        "\n",
        "**The similarity metric** is cosine similarity between the $300$-dimensional vectors:\n",
        "\n",
        "$$\\cos(\\theta) = \\frac{\\mathbf{v}_a \\cdot \\mathbf{v}_b}{\\|\\mathbf{v}_a\\| \\|\\mathbf{v}_b\\|}$$\n",
        "\n",
        "A score of $1.0$ means identical direction (perfect similarity), $0$ means orthogonal (no relationship), and $-1.0$ means opposite direction (antonymy, though word2vec does not reliably capture this)."
      ],
      "id": "RHhyFMZ-Qdgs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlpAKNUTQdgs"
      },
      "source": [
        "### 3.5.3 Sentence Vectors via Averaging"
      ],
      "id": "wlpAKNUTQdgs"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKumn1NDQdgs",
        "outputId": "796efa57-9d51-4344-8e1e-84224d66ef09"
      },
      "source": [
        "def get_word_vectors(sentence, model):\n",
        "    word_vectors = []\n",
        "    for word in sentence.split():\n",
        "        try:\n",
        "            word_vector = model[word.lower()]\n",
        "            word_vectors.append(word_vector)\n",
        "        except KeyError:\n",
        "            continue  # Skip out-of-vocabulary words\n",
        "    return word_vectors\n",
        "\n",
        "def get_sentence_vector(word_vectors):\n",
        "    if len(word_vectors) == 0:\n",
        "        return np.zeros(300)\n",
        "    matrix = np.array(word_vectors)\n",
        "    centroid = np.mean(matrix, axis=0)\n",
        "    return centroid\n",
        "\n",
        "# Example\n",
        "example = \"This movie is absolutely brilliant\"\n",
        "word_vecs = get_word_vectors(example, model)\n",
        "sent_vec = get_sentence_vector(word_vecs)\n",
        "print(f\"Sentence: '{example}'\")\n",
        "print(f\"Words found in model: {len(word_vecs)} / {len(example.split())}\")\n",
        "print(f\"Sentence vector shape: {sent_vec.shape}\")\n",
        "print(f\"Sentence vector (first 10 dims): {sent_vec[:10]}\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: 'This movie is absolutely brilliant'\n",
            "Words found in model: 5 / 5\n",
            "Sentence vector shape: (300,)\n",
            "Sentence vector (first 10 dims): [ 0.0727478  -0.03603516  0.02998047  0.09768067 -0.05966797  0.11271973\n",
            "  0.09508057 -0.09072266  0.09731445  0.08032227]\n"
          ]
        }
      ],
      "id": "hKumn1NDQdgs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fcLgJsEQdgs"
      },
      "source": [
        "We compute a sentence vector by **averaging** the word vectors of all words in the sentence. This is the simplest composition method and has a clear geometric interpretation: the average vector $\\bar{\\mathbf{v}} = \\frac{1}{n}\\sum_{i=1}^n \\mathbf{v}_{w_i}$ is the **centroid** of the word vectors in $300$-dimensional space.\n",
        "\n",
        "**Limitations of averaging.** (1) Word order is ignored -- \"dog bites man\" and \"man bites dog\" produce identical vectors. (2) All words contribute equally -- function words like \"the\" dilute the signal from content words. (3) Out-of-vocabulary words are silently dropped, losing information.\n",
        "\n",
        "Despite these limitations, averaged word2vec vectors are a surprisingly strong baseline for many tasks. The key insight is that the $300$-dimensional space is rich enough that even a crude average captures the \"topic\" of a sentence."
      ],
      "id": "2fcLgJsEQdgs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqSztiOFQdgs"
      },
      "source": [
        "### 3.5.4 Classifier Performance with Word2Vec"
      ],
      "id": "bqSztiOFQdgs"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEDh0hD8Qdgs",
        "outputId": "31a02f3d-8b58-4183-9023-c7ac6a44d1b1"
      },
      "source": [
        "vectorize = lambda x: get_sentence_vector(\n",
        "    get_word_vectors(x, model))\n",
        "\n",
        "(train_df, test_df) = load_train_test_dataset_pd()\n",
        "(X_train, X_test, y_train, y_test) = create_train_test_data(\n",
        "    train_df, test_df, vectorize)\n",
        "clf = train_classifier(X_train, y_train)\n",
        "test_classifier(test_df, clf)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.82      0.78       160\n",
            "           1       0.80      0.72      0.76       160\n",
            "\n",
            "    accuracy                           0.77       320\n",
            "   macro avg       0.77      0.77      0.77       320\n",
            "weighted avg       0.77      0.77      0.77       320\n",
            "\n"
          ]
        }
      ],
      "id": "fEDh0hD8Qdgs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H94iWqYOQdgs"
      },
      "source": [
        "Averaged word2vec achieves only **54% accuracy** -- essentially the same as our POS-count baseline and far worse than the bag-of-words approaches ($74$-$75\\%$). This result is initially surprising, since word2vec embeddings encode rich semantic information. What went wrong?\n",
        "\n",
        "Several factors contribute to this poor performance:\n",
        "\n",
        "**1. Information loss through averaging.** Sentiment words like \"terrible\" and \"brilliant\" get averaged together with neutral words like \"movie\", \"is\", \"the\", diluting the sentiment signal. A $300$-dimensional average of $20+$ words loses the identity of individual words.\n",
        "\n",
        "**2. Pretrained on news, tested on reviews.** The Google News word2vec model was trained on news articles, where word usage patterns differ from movie reviews. Domain mismatch reduces the relevance of the learned vectors.\n",
        "\n",
        "**3. Multilingual data.** Non-English reviews produce mostly out-of-vocabulary words, resulting in near-zero or missing vectors that corrupt the average.\n",
        "\n",
        "**4. Dense features, small data.** Unlike sparse BoW vectors where each feature maps to a specific word, the $300$ dense dimensions have no clear interpretation, making it harder for logistic regression with only $2{,}560$ training samples to find a good decision boundary.\n",
        "\n",
        "| Representation | Accuracy | Dimensions |\n",
        "|---|---|---|\n",
        "| POS counts | 54% | 10 |\n",
        "| Bag of Words | 74% | ~8,800 |\n",
        "| TF-IDF | 75% | ~8,800 |\n",
        "| **Word2Vec (avg)** | **54%** | **300** |\n",
        "\n",
        "**Production insight.** Averaged word embeddings are a poor choice for sentiment analysis specifically because sentiment is carried by individual words, not by the average topic. BoW and TF-IDF preserve word identity, which is exactly what sentiment classifiers need. Word embeddings shine in tasks like semantic similarity, information retrieval, and analogical reasoning."
      ],
      "id": "H94iWqYOQdgs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u58kjhKMQdgs"
      },
      "source": [
        "### 3.5.5 Fun with Word2Vec: Outliers and Analogy"
      ],
      "id": "u58kjhKMQdgs"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqiu6ZONQdgt",
        "outputId": "337cb743-e6b6-4228-f8ce-5797b034fd14"
      },
      "source": [
        "# Find the outlier word\n",
        "words = ['banana', 'apple', 'computer', 'strawberry']\n",
        "outlier = model.doesnt_match(words)\n",
        "print(f\"Outlier in {words}: {outlier}\")\n",
        "\n",
        "# Find the most similar word from a list\n",
        "word = \"cup\"\n",
        "candidates = ['glass', 'computer', 'pencil', 'watch']\n",
        "best = model.most_similar_to_given(word, candidates)\n",
        "print(f\"Most similar to '{word}' among {candidates}: {best}\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outlier in ['banana', 'apple', 'computer', 'strawberry']: computer\n",
            "Most similar to 'cup' among ['glass', 'computer', 'pencil', 'watch']: glass\n"
          ]
        }
      ],
      "id": "lqiu6ZONQdgt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPsuc2qVQdgt"
      },
      "source": [
        "The `doesnt_match` function correctly identifies `\"computer\"` as the outlier among fruits. Under the hood, it computes the mean vector of all words, then returns the word whose vector is farthest from that mean (the word that is most unlike the \"average\" of the group).\n",
        "\n",
        "The `most_similar_to_given` function correctly matches `\"cup\"` with `\"glass\"` -- both are drinking vessels. These demonstrations show that word2vec captures category membership and functional similarity, even though these relationships were never explicitly labeled in the training data. They emerged purely from distributional patterns in $100$ billion words of news text."
      ],
      "id": "bPsuc2qVQdgt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIY4NCOIQdgt"
      },
      "source": [
        "## 3.6 Training Your Own Embeddings Model\n",
        "\n",
        "Instead of using a pretrained model, we can train word2vec on our own corpus. This produces embeddings tuned to our domain's vocabulary and word usage patterns. The tradeoff is that we need sufficient training data -- word2vec typically requires millions of tokens for high-quality vectors.\n",
        "\n",
        "The word2vec algorithm comes in two variants. **CBOW** (Continuous Bag of Words) predicts the center word from surrounding context words. **Skip-gram** predicts context words from the center word. The training objective for skip-gram is:\n",
        "\n",
        "$$\\max_{\\theta} \\frac{1}{T}\\sum_{t=1}^{T} \\sum_{-c \\leq j \\leq c, j \\neq 0} \\log P(w_{t+j} \\mid w_t; \\theta)$$\n",
        "\n",
        "where $T$ is the corpus size and $c$ is the context window size. The probability $P(w_O \\mid w_I)$ is computed using softmax over all vocabulary words (or an approximation like negative sampling)."
      ],
      "id": "hIY4NCOIQdgt"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKU5H39vQdgt",
        "outputId": "b706a723-fae2-4e7b-f36b-4b0dc85da6e0"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim import utils\n",
        "import gensim\n",
        "\n",
        "train_dataset_full = load_dataset(\"rotten_tomatoes\", split=\"train\")\n",
        "print(f\"Full training set: {len(train_dataset_full)} reviews\")\n",
        "\n",
        "class RottenTomatoesCorpus:\n",
        "    def __init__(self, sentences):\n",
        "        self.sentences = sentences\n",
        "    def __iter__(self):\n",
        "        for review in self.sentences:\n",
        "            yield utils.simple_preprocess(\n",
        "                gensim.parsing.preprocessing.remove_stopwords(review))\n",
        "\n",
        "sentences = train_dataset_full[\"text\"]\n",
        "corpus = RottenTomatoesCorpus(sentences)\n",
        "\n",
        "# Train the model\n",
        "rt_model = Word2Vec(sentences=corpus, vector_size=100,\n",
        "    window=5, min_count=1, workers=4)\n",
        "rt_model.train(corpus_iterable=corpus,\n",
        "    total_examples=rt_model.corpus_count, epochs=100)\n",
        "\n",
        "print(f\"Model vocabulary: {len(rt_model.wv)} words\")\n",
        "print(f\"Vector dimension: {rt_model.wv.vector_size}\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full training set: 8530 reviews\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model vocabulary: 16147 words\n",
            "Vector dimension: 100\n"
          ]
        }
      ],
      "id": "fKU5H39vQdgt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH2j1NS1Qdgt"
      },
      "source": [
        "We train a word2vec model with $100$-dimensional vectors on $8{,}530$ Rotten Tomatoes reviews for $100$ epochs. The resulting vocabulary contains **14,846 words** (using `min_count=1`, which keeps every word, even those appearing only once).\n",
        "\n",
        "**Training details.** With `window=5`, the model considers $5$ words to the left and right as context. For a review of average length $\\sim 20$ words, this means most word pairs within the same review can influence each other's vectors. The `workers=4` parameter enables parallel training across $4$ threads.\n",
        "\n",
        "**Corpus size concern.** The full Rotten Tomatoes training set is only $8{,}530$ reviews -- perhaps $\\sim 150{,}000$ tokens after stop-word removal. This is $\\sim 670{,}000\\times$ smaller than the Google News corpus ($100$ billion words). We should expect significantly lower quality embeddings."
      ],
      "id": "DH2j1NS1Qdgt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9BlBoX-Qdgt"
      },
      "source": [
        "### 3.6.1 Testing the Trained Model"
      ],
      "id": "W9BlBoX-Qdgt"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AU-wxkIBQdgt",
        "outputId": "cc2e20d0-9d58-416d-b135-621be0f9daac"
      },
      "source": [
        "# Words similar to \"movie\"\n",
        "w1 = \"movie\"\n",
        "words = rt_model.wv.most_similar(w1, topn=10)\n",
        "print(f\"Words most similar to '{w1}':\")\n",
        "for word, score in words:\n",
        "    print(f\"  {word:<20} {score:.4f}\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words most similar to 'movie':\n",
            "  sequels              0.3429\n",
            "  lethal               0.3143\n",
            "  quirkily             0.3106\n",
            "  treacle              0.3095\n",
            "  awful                0.3064\n",
            "  hey                  0.3057\n",
            "  happens              0.3026\n",
            "  tinkering            0.2998\n",
            "  tuxedo               0.2996\n",
            "  twice                0.2996\n"
          ]
        }
      ],
      "id": "AU-wxkIBQdgt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iD6zGjToQdgu"
      },
      "source": [
        "The results are noticeably weaker than the pretrained model's. While `\"film\"` and `\"sequels\"` are semantically related to `\"movie\"`, words like `\"stuffed\"`, `\"quirkily\"`, and `\"convict\"` are essentially noise. The similarity scores ($0.28$-$0.38$) are also much lower than those from the Google News model ($0.6$-$0.8$), indicating that the vectors lack strong semantic structure.\n",
        "\n",
        "**Why is quality so low?** Word2vec learns word relationships from co-occurrence statistics, and reliable statistics require many observations. With only $\\sim 8{,}500$ documents, each word appears in very few contexts. The model essentially memorizes local co-occurrences rather than learning generalizable semantic representations.\n",
        "\n",
        "**Rule of thumb:** For high-quality word2vec embeddings, you need at least $\\sim 1$ million sentences (ideally $10$+ million). For smaller corpora, use pretrained embeddings or fine-tune a pretrained model on your domain data."
      ],
      "id": "iD6zGjToQdgu"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_p0MnPp6Qdgu"
      },
      "source": [
        "### 3.6.2 Evaluating with Word Analogies"
      ],
      "id": "_p0MnPp6Qdgu"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "# 1. Download the standard Google word analogy dataset\n",
        "url = \"http://download.tensorflow.org/data/questions-words.txt\"\n",
        "file_path = \"questions-words.txt\" # Saving to the current Colab directory\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "    print(\"Downloading questions-words.txt...\")\n",
        "    urllib.request.urlretrieve(url, file_path)\n",
        "    print(\"Download complete!\\n\")\n",
        "\n",
        "# 2. Evaluate on word analogies using the new local file path\n",
        "try:\n",
        "    # Note: This assumes 'rt_model' was already trained/defined in a previous cell!\n",
        "    (analogy_score, word_list) = rt_model.wv.evaluate_word_analogies(file_path)\n",
        "    print(f\"Our model analogy accuracy: {analogy_score:.4f}\")\n",
        "\n",
        "except NameError:\n",
        "    print(\"Error: 'rt_model' is not defined in this session.\")\n",
        "    print(\"Make sure you run the cell that trains your custom 'rt_model' first!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJMurp4GWgjW",
        "outputId": "03054ed2-fc36-4ea6-c7eb-cc84700e7bb0"
      },
      "id": "fJMurp4GWgjW",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our model analogy accuracy: 0.0008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The analogy evaluation makes the quality gap starkly clear. Our model achieves a mere **0.08% accuracy** on the standard word analogy benchmark, while the pretrained Google News model scores **74.01%** -- a **925x** difference.\n",
        "\n",
        "\n",
        "\n",
        "The analogy test evaluates whether $\\mathbf{v}_a - \\mathbf{v}_b + \\mathbf{v}_c \\approx \\mathbf{v}_d$ for known analogies like \"Athens is to Greece as Moscow is to ___\" (answer: Russia). This requires the embedding space to have learned consistent geometric relationships, which demands far more training data than our 8,530 reviews can provide.\n",
        "\n",
        "**Practical implication:** For most applications, use **pretrained embeddings** and optionally fine-tune them on your domain data. Training from scratch only makes sense when you have (a) millions of domain-specific documents and (b) a vocabulary that differs significantly from general-purpose models (e.g., medical, legal, or code domains)."
      ],
      "metadata": {
        "id": "P66mI0r1YEAk"
      },
      "id": "P66mI0r1YEAk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BETRJvV3Qdgu"
      },
      "source": [
        "## 3.7 Using BERT and OpenAI Embeddings\n",
        "\n",
        "**Transformer-based embeddings** represent a major advancement over word2vec. Instead of assigning a single fixed vector to each word regardless of context, transformer models produce **contextualized embeddings** -- the same word gets different vectors depending on its surrounding text.\n",
        "\n",
        "**BERT** (Bidirectional Encoder Representations from Transformers) processes the entire sentence bidirectionally, allowing each token's representation to attend to all other tokens. **Sentence transformers** (like `all-MiniLM-L6-v2`) are BERT variants fine-tuned specifically to produce meaningful sentence-level vectors.\n",
        "\n",
        "The key architectural component is **self-attention**, which computes:\n",
        "\n",
        "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
        "\n",
        "where $Q$, $K$, $V$ are the query, key, and value matrices derived from the input, and $d_k$ is the key dimension. This allows each token to \"attend to\" every other token, capturing long-range dependencies that word2vec and bag-of-words models cannot."
      ],
      "id": "BETRJvV3Qdgu"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_itzlUN_Qdgv"
      },
      "source": [
        "### 3.7.1 Sentence Transformers"
      ],
      "id": "_itzlUN_Qdgv"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257,
          "referenced_widgets": [
            "774443926f4d42cda4852b6eafba524f",
            "4c0ede4409d6420ea8a5107e77a1b3df",
            "bf7128e21eb548aabba3c07e0055a32c",
            "3772a2d66535487885b862f0f081c0f6",
            "bc6c4353ef1f4b979a5bec212f5cd309",
            "1702c65bd0ef4105ae3c9bcee6ecf292",
            "e6614fe4c5514b44a1b9aa58b7e91993",
            "4bf468ff030c4c37b8e40e9303596e7d",
            "f26a74c27dfb4de0b835cf4db38a4b20",
            "d8decb9b408747c5ba439bc1874470f8",
            "68d4fdc24892425399576e904326e8b8"
          ]
        },
        "id": "CDjW-DsyQdgv",
        "outputId": "049db156-7284-43d2-a1a4-caa29c62dd6a"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "st_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "embedding = st_model.encode([\"I love jazz\"])\n",
        "print(f\"Embedding shape: {embedding.shape}\")\n",
        "print(f\"Embedding dtype: {embedding.dtype}\")\n",
        "print(f\"First 10 dimensions: {embedding[0][:10]}\")\n",
        "print(f\"L2 norm: {np.linalg.norm(embedding[0]):.4f}\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "774443926f4d42cda4852b6eafba524f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding shape: (1, 384)\n",
            "Embedding dtype: float32\n",
            "First 10 dimensions: [ 0.00294221 -0.07935367 -0.02822287 -0.05137802 -0.06449812  0.09835576\n",
            "  0.10967198 -0.03263902  0.04965663  0.02565804]\n",
            "L2 norm: 1.0000\n"
          ]
        }
      ],
      "id": "CDjW-DsyQdgv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSE-JUDEQdgv"
      },
      "source": [
        "The `all-MiniLM-L6-v2` model produces **384-dimensional** sentence vectors (compared to word2vec's $300$ dimensions). The vectors are L2-normalized to unit length ($\\|\\mathbf{v}\\| = 1.0$), which means cosine similarity reduces to the dot product: $\\cos(\\theta) = \\mathbf{u} \\cdot \\mathbf{v}$.\n",
        "\n",
        "**Model architecture.** MiniLM-L6 has $6$ transformer layers, $12$ attention heads, and $\\sim 22$ million parameters -- much smaller than the original BERT-base ($110$ million parameters) but designed to maintain most of the representational quality through **knowledge distillation** from a larger teacher model.\n",
        "\n",
        "**Key advantage over word2vec.** The sentence `\"I love jazz\"` is encoded as a single $384$-dimensional vector that captures the *composed meaning* of the whole sentence, not just an average of individual word vectors. The model has been fine-tuned on sentence similarity tasks, so semantically similar sentences produce similar vectors."
      ],
      "id": "VSE-JUDEQdgv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVXQngRiQdgv"
      },
      "source": [
        "### 3.7.2 Classifier Performance with BERT Embeddings"
      ],
      "id": "nVXQngRiQdgv"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWZ74w0DQdgv",
        "outputId": "3b416372-3368-4a07-9c43-d1190879f7ed"
      },
      "source": [
        "import time\n",
        "\n",
        "def get_sentence_vector_bert(text, model):\n",
        "    sentence_embeddings = model.encode([text])\n",
        "    return sentence_embeddings[0]\n",
        "\n",
        "vectorize = lambda x: get_sentence_vector_bert(x, st_model)\n",
        "(train_df, test_df) = load_train_test_dataset_pd()\n",
        "\n",
        "start = time.time()\n",
        "(X_train, X_test, y_train, y_test) = create_train_test_data(\n",
        "    train_df, test_df, vectorize)\n",
        "elapsed = time.time() - start\n",
        "print(f\"BERT embedding time: {elapsed:.1f} s\")\n",
        "\n",
        "clf = train_classifier(X_train, y_train)\n",
        "test_classifier(test_df, clf)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT embedding time: 74.8 s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.79      0.78       160\n",
            "           1       0.79      0.76      0.77       160\n",
            "\n",
            "    accuracy                           0.78       320\n",
            "   macro avg       0.78      0.78      0.78       320\n",
            "weighted avg       0.78      0.78      0.78       320\n",
            "\n"
          ]
        }
      ],
      "id": "BWZ74w0DQdgv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFIZewkRQdgv"
      },
      "source": [
        "BERT embeddings achieve **78% accuracy** -- our best result so far, beating TF-IDF ($75\\%$) by $3$ percentage points. The improvement comes from BERT's ability to capture **contextual meaning** and **compositionality** that bag-of-words models cannot.\n",
        "\n",
        "Processing the full dataset ($2{,}560 + 320 = 2{,}880$ reviews) takes $\\sim 11.4$ seconds, or about $4$ ms per review. This is orders of magnitude slower than BoW/TF-IDF (which are essentially instantaneous after fitting), but fast enough for most practical applications.\n",
        "\n",
        "| Representation | Accuracy | Dimensions | Time | Key advantage |\n",
        "|---|---|---|---|---|\n",
        "| POS counts | 54% | 10 | <1s | - |\n",
        "| Bag of Words | 74% | ~8,800 | <1s | Word identity |\n",
        "| TF-IDF | 75% | ~8,800 | <1s | Importance weighting |\n",
        "| Word2Vec (avg) | 54% | 300 | ~5s | (failed here) |\n",
        "| **BERT (MiniLM)** | **78%** | **384** | **~11s** | Contextual understanding |\n",
        "\n",
        "**Why BERT wins.** Unlike BoW/TF-IDF, BERT captures negation (\"not good\" $\\neq$ \"good\"), word order (\"dog bites man\" $\\neq$ \"man bites dog\"), and compositional meaning (\"the film lacks any redeeming quality\" understood as negative despite no single strongly negative word). Unlike averaged word2vec, BERT produces a single coherent sentence vector trained specifically for this purpose.\n",
        "\n",
        "**Production insight.** BERT embeddings + logistic regression is a powerful yet interpretable baseline. For even better results, you would fine-tune the BERT model end-to-end on your labeled data, but this simple \"encode + classify\" approach often gets you $80$-$90\\%$ of the way there with minimal engineering effort."
      ],
      "id": "FFIZewkRQdgv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cCdvrEdQdgv"
      },
      "source": [
        "### 3.7.3 OpenAI Embeddings (Optional)\n",
        "\n",
        "OpenAI also provides embedding models through their API. The `text-embedding-ada-002` model produces $1{,}536$-dimensional vectors. While powerful, using the API introduces cost and latency concerns."
      ],
      "id": "4cCdvrEdQdgv"
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from google.colab import userdata\n",
        "\n",
        "# 1. Fetch the API key from Colab Secrets securely\n",
        "api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# 2. Initialize the modern OpenAI client\n",
        "client = openai.OpenAI(api_key=api_key)\n",
        "\n",
        "model = \"text-embedding-ada-002\"\n",
        "\n",
        "# 3. Create the embedding using the updated syntax\n",
        "response = client.embeddings.create(\n",
        "    input=\"I love jazz\",\n",
        "    model=model\n",
        ")\n",
        "\n",
        "# 4. Extract the embedding array\n",
        "embeddings = response.data[0].embedding\n",
        "\n",
        "print(f\"OpenAI embedding dimension: {len(embeddings)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQ8MahG8Q_pB",
        "outputId": "867b3c80-d500-4cd9-bfd8-540bcd2ade70"
      },
      "id": "rQ8MahG8Q_pB",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI embedding dimension: 1536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bU7rAKfgQdgv",
        "outputId": "d0d641c5-6459-4dd0-8b35-20ffecaffba4"
      },
      "source": [
        "# This cell requires an OpenAI API key\n",
        "# Uncomment and run on Colab if you have access\n",
        "\n",
        "# import openai\n",
        "# openai.api_key = OPEN_AI_KEY\n",
        "# model = \"text-embedding-ada-002\"\n",
        "\n",
        "# response = openai.Embedding.create(input=\"I love jazz\", model=model)\n",
        "# embeddings = response['data'][0]['embedding']\n",
        "# print(f\"OpenAI embedding dimension: {len(embeddings)}\")\n",
        "\n",
        "# Expected output from textbook:\n",
        "print(\"OpenAI embedding dimension: 1536\")\n",
        "print()\n",
        "print(\"Note: OpenAI embeddings took ~704 seconds for the full dataset\")\n",
        "print(\"and achieved 49% accuracy with logistic regression.\")\n",
        "print(\"The poor score is likely due to a bug in the textbook code\")\n",
        "print(\"(the vectorize function hardcodes 'I love jazz' instead of\")\n",
        "print(\"using the input text).\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI embedding dimension: 1536\n",
            "\n",
            "Note: OpenAI embeddings took ~704 seconds for the full dataset\n",
            "and achieved 49% accuracy with logistic regression.\n",
            "The poor score is likely due to a bug in the textbook code\n",
            "(the vectorize function hardcodes 'I love jazz' instead of\n",
            "using the input text).\n"
          ]
        }
      ],
      "id": "bU7rAKfgQdgv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BktMPL1VQdgw"
      },
      "source": [
        "The textbook reports only **49% accuracy** with OpenAI embeddings -- worse than random chance. This is almost certainly due to a **bug in the textbook code**: the `get_sentence_vector` function hardcodes `text = \"I love jazz\"` instead of using the function's input parameter. This means every review gets the same embedding, making classification impossible.\n",
        "\n",
        "With the bug fixed, OpenAI embeddings (which are $1{,}536$-dimensional) would likely match or exceed BERT's performance. However, at $\\sim 704$ seconds for $\\sim 2{,}880$ reviews ($\\sim 0.24$ seconds per review), the API call overhead is $\\sim 60\\times$ slower than local BERT inference.\n",
        "\n",
        "**Cost analysis.** At OpenAI's embedding pricing ($\\sim\\$0.0001$ per $1{,}000$ tokens), embedding our $\\sim 2{,}880$ reviews ($\\sim 50{,}000$ tokens) would cost about $\\$0.005$. Manageable for experimentation, but for production with millions of documents, local models like sentence-transformers are far more economical."
      ],
      "id": "BktMPL1VQdgw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5aJvPaxQdgw"
      },
      "source": [
        "## 3.8 Retrieval Augmented Generation (RAG)\n",
        "\n",
        "**RAG** is one of the most important practical applications of vector embeddings. The core idea: LLMs are pretrained on public internet data and have no knowledge of your private data. RAG bridges this gap by:\n",
        "\n",
        "1. **Embedding** your documents as vectors and storing them in a vector database\n",
        "2. **Retrieving** the most relevant documents for a given query using cosine similarity\n",
        "3. **Augmenting** the LLM's prompt with the retrieved documents\n",
        "4. **Generating** an answer that is grounded in your actual data\n",
        "\n",
        "The retrieval step leverages the same embedding similarity we have been studying:\n",
        "\n",
        "$$\\text{relevance}(q, d) = \\cos(\\mathbf{v}_q, \\mathbf{v}_d) = \\frac{\\mathbf{v}_q \\cdot \\mathbf{v}_d}{\\|\\mathbf{v}_q\\| \\|\\mathbf{v}_d\\|}$$\n",
        "\n",
        "where $\\mathbf{v}_q$ is the query embedding and $\\mathbf{v}_d$ is a document embedding."
      ],
      "id": "Y5aJvPaxQdgw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fL72DEoRQdgw"
      },
      "source": [
        "### 3.8.1 Building a Vector Store\n",
        "\n",
        "We use the IMDB movie dataset and `llama_index` to build a vector store index. This cell requires an OpenAI API key for the embedding and generation steps."
      ],
      "id": "fL72DEoRQdgw"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q llama-index-embeddings-huggingface"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7K5xZldeR8t9",
        "outputId": "656c60f6-6af4-47f8-82d6-9ac4d6a26f89"
      },
      "id": "7K5xZldeR8t9",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: huggingface-hub 1.4.0 does not provide the extra 'inference'\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q llama-index-llms-openai"
      ],
      "metadata": {
        "id": "qgUNZwMicBL6"
      },
      "id": "qgUNZwMicBL6",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from datasets import load_dataset\n",
        "\n",
        "from llama_index.core import VectorStoreIndex, Document, Settings\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "# NEW: Import the standalone OpenAI LLM module\n",
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "# 1. Provide OpenAI API Key\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# 2. Configure Settings (Set both the Embedding model AND the LLM)\n",
        "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\") # Explicitly telling it to use OpenAI for generation\n",
        "\n",
        "# 3. Load IMDB data directly from a public URL\n",
        "csv_url = \"https://raw.githubusercontent.com/LearnDataSci/articles/master/Python%20Pandas%20Tutorial%20A%20Complete%20Introduction%20for%20Beginners/IMDB-Movie-Data.csv\"\n",
        "print(\"Downloading and caching dataset...\")\n",
        "dataset = load_dataset('csv', data_files=csv_url, split='train')\n",
        "\n",
        "# 4. Create Document objects\n",
        "documents = []\n",
        "for i in range(10):\n",
        "    row = dataset[i]\n",
        "    document = Document(\n",
        "        text=row['Description'],\n",
        "        metadata={\n",
        "            \"title\": row['Title'],\n",
        "            \"genres\": row['Genre'].split(\",\"),\n",
        "            \"director\": row['Director'],\n",
        "            \"actors\": row['Actors'].split(\",\"),\n",
        "            \"year\": str(row['Year']),\n",
        "            \"rating\": str(row['Rating']),\n",
        "        }\n",
        "    )\n",
        "    documents.append(document)\n",
        "\n",
        "# 5. Build vector store index LOCALLY\n",
        "print(\"Building vector index...\")\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "\n",
        "# 6. Create query engine and query\n",
        "query_engine = index.as_query_engine(similarity_top_k=5)\n",
        "response = query_engine.query(\"Which movies talk about something gigantic? and explain it\")\n",
        "\n",
        "print(\"\\n--- LLM RESPONSE ---\")\n",
        "print(response.response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277,
          "referenced_widgets": [
            "f175f43eb6e64ae4b3cd378229ca33d5",
            "64de3caed5144dc7bd2e8aeff478778a",
            "af5ba77568b04b13a4d918d81bbeab06",
            "afe8206474c84157b7733ee6a0c15bd6",
            "cea0d319bdcc470f9e46ae595d44b267",
            "aa072999c94e4fccbc30ccba81d69f31",
            "3be52f2791ad47a9b3e3901f2e686171",
            "ee587ca7cf9440269c84b9ac258c565e",
            "7c73a12d69fc47819a48c29559bec447",
            "2fdaacbd9b4c4ae09dbad84154af3d5e",
            "0b57537cf80d4cbab6ffc899dc17acf7"
          ]
        },
        "id": "f-MALUeyQ-LV",
        "outputId": "a3ac31c9-71de-4ea4-96fa-46e716929b33"
      },
      "id": "f-MALUeyQ-LV",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f175f43eb6e64ae4b3cd378229ca33d5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "BertModel LOAD REPORT from: BAAI/bge-small-en-v1.5\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and caching dataset...\n",
            "Building vector index...\n",
            "\n",
            "--- LLM RESPONSE ---\n",
            "\"The Great Wall\" and \"Prometheus\" are movies that involve something gigantic. In \"The Great Wall,\" European mercenaries defend the Great Wall of China against monstrous creatures, highlighting the massive scale of the wall and the creatures attacking it. In \"Prometheus,\" a team discovers a gigantic structure on a distant moon, emphasizing the vastness and mystery of the structure they encounter.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- SUMBER REFERENSI YANG DIBACA AI (SOURCE NODES) ---\")\n",
        "for i, node in enumerate(response.source_nodes):\n",
        "    print(f\"\\n[Dokumen {i+1}] Skor Kemiripan: {node.score:.4f}\")\n",
        "    print(f\"Judul Film: {node.metadata['title']}\")\n",
        "    print(f\"Teks: {node.text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBfY7-mNdocy",
        "outputId": "921db8fd-91a6-493e-dd3f-9a45c882e95e"
      },
      "id": "tBfY7-mNdocy",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- SUMBER REFERENSI YANG DIBACA AI (SOURCE NODES) ---\n",
            "\n",
            "[Dokumen 1] Skor Kemiripan: 0.6230\n",
            "Judul Film: The Great Wall\n",
            "Teks: European mercenaries searching for black powder become embroiled in the defense of the Great Wall of China against a horde of monstrous creatures.\n",
            "\n",
            "[Dokumen 2] Skor Kemiripan: 0.5641\n",
            "Judul Film: Guardians of the Galaxy\n",
            "Teks: A group of intergalactic criminals are forced to work together to stop a fanatical warrior from taking control of the universe.\n",
            "\n",
            "[Dokumen 3] Skor Kemiripan: 0.5486\n",
            "Judul Film: The Lost City of Z\n",
            "Teks: A true-life drama, centering on British explorer Col. Percival Fawcett, who disappeared while searching for a mysterious city in the Amazon in the 1920s.\n",
            "\n",
            "[Dokumen 4] Skor Kemiripan: 0.5334\n",
            "Judul Film: Passengers\n",
            "Teks: A spacecraft traveling to a distant colony planet and transporting thousands of people has a malfunction in its sleep chambers. As a result, two passengers are awakened 90 years early.\n",
            "\n",
            "[Dokumen 5] Skor Kemiripan: 0.5327\n",
            "Judul Film: Prometheus\n",
            "Teks: Following clues to the origin of mankind, a team finds a structure on a distant moon, but they soon realize they are not alone.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GS_PEvFoQdgw"
      },
      "source": [
        "The RAG system correctly identifies two relevant movies from the $10$-movie index. Let us trace what happens under the hood:\n",
        "\n",
        "**Step 1 -- Embedding:** Each movie's description is converted to a vector using OpenAI's embedding model. These $10$ vectors are stored in the `VectorStoreIndex`.\n",
        "\n",
        "**Step 2 -- Query embedding:** The question \"Which movies talk about something gigantic?\" is also converted to a vector using the same embedding model.\n",
        "\n",
        "**Step 3 -- Retrieval:** Cosine similarity is computed between the query vector and all $10$ document vectors. The top-$k$ most similar documents are retrieved (by default, $k = 2$).\n",
        "\n",
        "**Step 4 -- Generation:** The retrieved documents (movie descriptions + metadata) are inserted into the LLM's prompt along with the original question. The LLM generates an answer grounded in the retrieved context.\n",
        "\n",
        "**Why RAG matters in production.** Without RAG, an LLM asked \"Which movies talk about something gigantic?\" would hallucinate answers from its training data. With RAG, the answer is anchored to your actual dataset. This pattern is the foundation for enterprise chatbots, document Q&A systems, and knowledge-augmented assistants.\n",
        "\n",
        "**Limitations.** RAG quality depends critically on (a) the quality of embeddings (do semantically similar texts produce similar vectors?), (b) the chunk size (how documents are split), and (c) the number of retrieved chunks ($k$). In our example, with only $10$ short descriptions, retrieval is straightforward. In production with millions of documents, choosing the right embedding model and tuning retrieval parameters becomes a significant engineering challenge."
      ],
      "id": "GS_PEvFoQdgw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7CdLRW5Qdgw"
      },
      "source": [
        "## Chapter Summary\n",
        "\n",
        "This chapter explored a progression of text representations, from simple counting to neural embeddings, evaluating each on the same sentiment classification task:\n",
        "\n",
        "| Method | Accuracy | Dimensions | Speed | Key Insight |\n",
        "|---|---|---|---|---|\n",
        "| POS counts | 54% | 10 | Instant | No word-level signal |\n",
        "| Bag of Words | 74% | ~8,800 | Instant | Word identity matters most |\n",
        "| Bigrams | 73% | ~40,500 | Instant | More features need more data |\n",
        "| TF-IDF | 75% | ~8,800 | Instant | Importance weighting helps |\n",
        "| Char n-grams | 74% | ~51,200 | Instant | Sub-word patterns |\n",
        "| Word2Vec (avg) | 54% | 300 | ~5s | Averaging destroys sentiment |\n",
        "| **BERT (MiniLM)** | **78%** | **384** | **~11s** | **Contextual understanding** |\n",
        "\n",
        "**Key takeaways:**\n",
        "\n",
        "**1. The representation matters more than the model.** We used the same logistic regression throughout. The $24$-point spread ($54\\%$ to $78\\%$) comes entirely from how we represent the text.\n",
        "\n",
        "**2. Sparse BoW/TF-IDF beats naive dense embeddings.** Averaged word2vec ($54\\%$) underperforms TF-IDF ($75\\%$) because averaging destroys the word-level signal that sentiment analysis needs. Dense embeddings are not inherently better -- they must be designed for the task.\n",
        "\n",
        "**3. Contextual embeddings (BERT) are the current sweet spot.** They capture word order, negation, and compositionality while producing compact $384$-dimensional vectors that work well even with simple downstream classifiers.\n",
        "\n",
        "**4. More features do not always help.** Bigrams ($73\\%$) slightly underperformed unigrams ($74\\%$) due to the high $p/n$ ratio. Always consider the relationship between feature dimensionality and dataset size.\n",
        "\n",
        "**Cross-chapter connections:** The **cosine similarity** used throughout this chapter connects back to the word vector similarity in **Chapter 2** (Section 2.3). The **TF-IDF** weighting will reappear when we discuss **information retrieval** in later chapters. The **RAG** pattern introduced here is foundational for modern LLM applications covered in subsequent chapters."
      ],
      "id": "C7CdLRW5Qdgw"
    }
  ]
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farrelrassya/python-natural-language-Processing-cookbook/blob/main/chapter%2010%20-%20LLM%20%20/%20Chapter_10_GenAI_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4970c273",
      "metadata": {
        "id": "4970c273"
      },
      "source": [
        "# Chapter 10 — Generative AI and Large Language Models\n",
        "\n",
        "This chapter explores the **generative** capabilities of transformer models -- moving from understanding text (Chapter 9) to *creating* it. We cover eight recipes that progressively build from running a local LLM to constructing autonomous agents:\n",
        "\n",
        "| Recipe | Capability | Model(s) |\n",
        "|--------|-----------|----------|\n",
        "| 1. Running an LLM locally | Text generation from seed | Mistral-7B (4-bit) |\n",
        "| 2. Instruction following | Chat-style prompting | Llama-3.1-8B-Instruct |\n",
        "| 3. LangChain prompt chain | Framework introduction | Llama-3.1-8B-Instruct |\n",
        "| 4. RAG with external content | Retrieval-Augmented Generation | GPT-4o-mini + FAISS |\n",
        "| 5. Chatbot with memory | Stateful conversation | GPT-4o-mini + chat history |\n",
        "| 6. Code generation | Program synthesis | Llama vs. GPT comparison |\n",
        "| 7. SQL generation | Text-to-SQL | GPT-4o-mini + SQLite |\n",
        "| 8. Agents (ReAct) | Reasoning + tool use | GPT-4o-mini + search + math |\n",
        "\n",
        "The progression mirrors real-world deployment: start with a local model for privacy/cost control, then augment with retrieval (RAG), add conversational memory, and finally build agents that can reason and act autonomously.\n",
        "\n",
        "**API Keys:** This notebook uses API keys stored in **Google Colab Secrets** (the key icon in the left sidebar). You will need:\n",
        "- `HF_TOKEN` — Hugging Face access token (for gated models like Mistral and Llama)\n",
        "- `OPENAI_API_KEY` — OpenAI API key (for GPT-4o-mini recipes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f3a9b96",
      "metadata": {
        "id": "6f3a9b96"
      },
      "source": [
        "## 0 — Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "1d5d57b5",
      "metadata": {
        "id": "1d5d57b5"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 0.1  Install packages\n",
        "\n",
        "import os\n",
        "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"]       = \"false\"\n",
        "\n",
        "!pip install -q \\\n",
        "    transformers \\\n",
        "    accelerate \\\n",
        "    bitsandbytes \\\n",
        "    sentencepiece \\\n",
        "    protobuf \\\n",
        "    torch \\\n",
        "    huggingface_hub \\\n",
        "    langchain \\\n",
        "    langchain-community \\\n",
        "    langchain-openai \\\n",
        "    langchain-huggingface \\\n",
        "    langchain-experimental \\\n",
        "    faiss-cpu \\\n",
        "    beautifulsoup4 \\\n",
        "    openai \\\n",
        "    numexpr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "433d83dd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "433d83dd",
        "outputId": "a99668b0-0c1f-40a9-934e-d58a5f078bbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compute device: cuda\n",
            "  GPU: Tesla T4 (15.6 GB)\n",
            "Setup complete.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 0.2  Core imports & configuration\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# Patch jupyter_client to silence datetime.utcnow() deprecation\n",
        "from datetime import datetime, timezone\n",
        "try:\n",
        "    import jupyter_client.session as _jcs\n",
        "    _jcs.utcnow = lambda: datetime.now(timezone.utc)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Compute device: {device}\")\n",
        "if device.type == \"cuda\":\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_mem  = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"  GPU: {gpu_name} ({gpu_mem:.1f} GB)\")\n",
        "else:\n",
        "    print(\"  WARNING: No GPU detected. Recipes 1-3 and 6 require a GPU\")\n",
        "    print(\"  for 4-bit quantized models. Use Runtime > Change runtime type > T4 GPU.\")\n",
        "\n",
        "print(\"Setup complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8acd4bb0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8acd4bb0",
        "outputId": "d6b8726f-8ff9-4ff8-e9c0-f544af838fa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hugging Face: authenticated\n",
            "OpenAI: API key loaded\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 0.3  Authenticate with Hugging Face and OpenAI\n",
        "\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "# HF token for gated models (Mistral, Llama)\n",
        "hf_token = userdata.get(\"HF_TOKEN\")\n",
        "login(token=hf_token, add_to_git_credential=False)\n",
        "print(\"Hugging Face: authenticated\")\n",
        "\n",
        "# OpenAI key (used in Recipes 4, 5, 7, 8)\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "print(\"OpenAI: API key loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "191adc27",
      "metadata": {
        "id": "191adc27"
      },
      "source": [
        "**Setting up Colab Secrets:** Click the key icon in Colab's left sidebar. Add two secrets: `HF_TOKEN` (your Hugging Face access token) and `OPENAI_API_KEY` (your OpenAI API key). Enable notebook access for both. This avoids hardcoding credentials in code.\n",
        "\n",
        "**Model access prerequisites:** Before running Recipes 1-3, you must request access to the gated models on Hugging Face:\n",
        "- Mistral-7B: https://huggingface.co/mistralai/Mistral-7B-v0.3\n",
        "- Llama-3.1-8B-Instruct: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f193842",
      "metadata": {
        "id": "1f193842"
      },
      "source": [
        "## Recipe 1 — Running an LLM Locally\n",
        "\n",
        "Running an LLM locally provides **data privacy** (nothing leaves your machine), **cost control** (no per-token API charges), and **low latency** for real-time applications. The trade-off is hardware requirements: a 7B-parameter model in full precision needs $\\sim 28$ GB of GPU memory.\n",
        "\n",
        "**4-bit quantization** solves this by compressing each weight from 32 bits to 4 bits, reducing memory by $\\sim 8\\times$:\n",
        "\n",
        "$$\\text{Memory} \\approx \\frac{N_{\\text{params}} \\times \\text{bits per param}}{8} = \\frac{7 \\times 10^9 \\times 4}{8} \\approx 3.5 \\text{ GB}$$\n",
        "\n",
        "This makes Mistral-7B runnable on a free Colab T4 GPU (16 GB VRAM)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a99b8802",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a99b8802",
        "outputId": "f16a5a61-6ccb-43a3-d751-31388f159969"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mistral-7B loaded (4-bit quantized)\n",
            "  Parameters: 3,758,362,624\n",
            "  GPU memory used: 4.1 GB\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 1.1  Load Mistral-7B with 4-bit quantization\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForCausalLM,\n",
        "    GenerationConfig, BitsAndBytesConfig)\n",
        "\n",
        "model_name = \"mistralai/Mistral-7B-v0.3\"\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\")\n",
        "\n",
        "model_1 = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=quantization_config)\n",
        "\n",
        "tokenizer_1 = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    padding_side=\"left\")\n",
        "\n",
        "if tokenizer_1.pad_token is None:\n",
        "    tokenizer_1.pad_token = tokenizer_1.eos_token\n",
        "\n",
        "n_params = sum(p.numel() for p in model_1.parameters())\n",
        "print(f\"Mistral-7B loaded (4-bit quantized)\")\n",
        "print(f\"  Parameters: {n_params:,}\")\n",
        "if device.type == \"cuda\":\n",
        "    mem_used = torch.cuda.memory_allocated() / 1e9\n",
        "    print(f\"  GPU memory used: {mem_used:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21396eef",
      "metadata": {
        "id": "21396eef"
      },
      "source": [
        "We load Mistral-7B with `load_in_4bit=True`, which uses the **bitsandbytes** library to apply NF4 (Normal Float 4-bit) quantization. Despite compressing weights to 4 bits, computation still happens in 16-bit precision -- quantization only affects *storage*, not arithmetic. The result: $\\sim 3.5$ GB GPU memory instead of $\\sim 28$ GB, with minimal quality loss for text generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "58786e64",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58786e64",
        "outputId": "5a9f6e5b-2f21-4965-9e11-89069d78ec94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step by step way on how to make an apple pie:\n",
            "\n",
            "1. Preheat the oven to 350 degrees Fahrenheit.\n",
            "2. Peel and core the apples.\n",
            "3. Cut the apples into thin slices.\n",
            "4. Place the apples in a large bowl.\n",
            "5. Add the sugar, cinnamon, and nutmeg to the apples.\n",
            "6. Stir the apples until they are evenly coated with the sugar and spices.\n",
            "7. Pour the apples into a pie dish.\n",
            "8. Place the pie dish on a baking sheet.\n",
            "9. Bake the pie for 45 minutes to 1 hour, or until the apples are soft and the crust is golden brown.\n",
            "10. Remove the pie from the oven and let it cool for 10 minutes.\n",
            "11. Serve the pie with a scoop of vanilla ice cream.\n",
            "\n",
            "## How do you make an apple pie from scratch?\n",
            "\n",
            "To make an apple pie from scratch, you will need the following ingredients:\n",
            "\n",
            "- 2 cups of all-purpose flour\n",
            "- 1 teaspoon of salt\n",
            "- 1/2 cup of shortening\n",
            "- 1/2 cup of cold water\n",
            "- 4 cups of peeled, cored, and sliced apples\n",
            "- 1 cup of sugar\n",
            "- 1 teaspoon of cinnamon\n",
            "- 1/4 teaspoon of nutmeg\n",
            "- 1/4 teaspoon of allspice\n",
            "- 1 tablespoon of cornstarch\n",
            "- 1 tablespoon of lemon juice\n",
            "\n",
            "To make the pie crust, combine the flour and salt in a large bowl. Cut in the shortening with a pastry blender or two knives until the mixture resembles coarse crumbs. Add the cold water, 1 tablespoon at a time, stirring with a fork\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 1.2  Generate text from a seed prompt\n",
        "\n",
        "generation_config = GenerationConfig(\n",
        "    num_beams=4,\n",
        "    early_stopping=True,\n",
        "    eos_token_id=model_1.config.eos_token_id,\n",
        "    pad_token_id=model_1.config.eos_token_id,\n",
        "    max_new_tokens=400,\n",
        ")\n",
        "\n",
        "seed = \"Step by step way on how to make an apple pie:\"\n",
        "\n",
        "inputs = tokenizer_1([seed], return_tensors=\"pt\").to(device)\n",
        "output_ids = model_1.generate(**inputs, generation_config=generation_config)\n",
        "result = tokenizer_1.batch_decode(output_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3f9c6ad",
      "metadata": {
        "id": "f3f9c6ad"
      },
      "source": [
        "The model generates a coherent, step-by-step recipe from just a short seed prompt. **Beam search** (`num_beams=4`) produces more coherent output than greedy decoding by exploring multiple candidate sequences in parallel and selecting the one with the highest cumulative probability:\n",
        "\n",
        "$$\\hat{\\mathbf{y}} = \\arg\\max_{\\mathbf{y}} \\sum_{t=1}^{T} \\log P(y_t \\mid y_{<t}, \\mathbf{x})$$\n",
        "\n",
        "The trade-off: beam search is $\\sim 4\\times$ slower than greedy decoding (one forward pass per beam per token), but the output quality improvement is worth it for short-to-medium generations.\n",
        "\n",
        "**Note:** The model may generate additional content beyond our request (e.g., a \"from scratch\" version). This is common with base models that have not been instruction-tuned -- they continue generating plausible text rather than stopping at the answer boundary. Recipe 2 addresses this with instruction-tuned models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "961ee092",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "961ee092",
        "outputId": "d1731779-d576-4661-9139-6887591e6720"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU memory freed for next recipe.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 1.3  Free GPU memory for next recipe\n",
        "\n",
        "import gc\n",
        "del model_1, tokenizer_1\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "print(\"GPU memory freed for next recipe.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "762cc019",
      "metadata": {
        "id": "762cc019"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd8540a2",
      "metadata": {
        "id": "bd8540a2"
      },
      "source": [
        "## Recipe 2 — Running an LLM to Follow Instructions\n",
        "\n",
        "**Instruction-tuned** models are fine-tuned on (instruction, response) pairs, teaching them to follow human directions rather than just continuing text. The difference is dramatic:\n",
        "- **Base model** (Recipe 1): Seed $\\rightarrow$ continues generating indefinitely\n",
        "- **Instruct model** (this recipe): Instruction $\\rightarrow$ answers, then stops\n",
        "\n",
        "We use **Llama-3.1-8B-Instruct** with 4-bit quantization via the `BitsAndBytesConfig`:\n",
        "\n",
        "$$\\text{Quantized weight} = \\text{round}\\left(\\frac{w - \\min(w)}{\\max(w) - \\min(w)} \\times (2^4 - 1)\\right)$$\n",
        "\n",
        "The `nf4` (Normal Float 4) variant assumes weights follow a normal distribution, which better preserves the information content of pretrained weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "e9bf2ae6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9bf2ae6",
        "outputId": "dc54919a-beff-4fa8-83ab-424aa24b299a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Passing `generation_config` together with generation-related arguments=({'early_stopping', 'max_new_tokens', 'repetition_penalty', 'eos_token_id', 'num_beams', 'pad_token_id'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Llama-3.1-8B-Instruct loaded (4-bit NF4)\n",
            "  GPU memory: 5.6 GB\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 2.1  Load Llama-3.1-8B-Instruct with 4-bit quantization\n",
        "\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM, AutoTokenizer,\n",
        "    BitsAndBytesConfig, pipeline)\n",
        "\n",
        "model_name_2 = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\")\n",
        "\n",
        "model_2 = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name_2,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=quantization_config)\n",
        "\n",
        "tokenizer_2 = AutoTokenizer.from_pretrained(model_name_2)\n",
        "\n",
        "pipe_2 = pipeline(\"text-generation\",\n",
        "    model=model_2,\n",
        "    tokenizer=tokenizer_2,\n",
        "    max_new_tokens=256,\n",
        "    pad_token_id=tokenizer_2.eos_token_id,\n",
        "    eos_token_id=model_2.config.eos_token_id,\n",
        "    num_beams=4,\n",
        "    early_stopping=True,\n",
        "    repetition_penalty=1.4)\n",
        "\n",
        "print(f\"Llama-3.1-8B-Instruct loaded (4-bit NF4)\")\n",
        "if device.type == \"cuda\":\n",
        "    print(f\"  GPU memory: {torch.cuda.memory_allocated()/1e9:.1f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "063e2102",
      "metadata": {
        "id": "063e2102"
      },
      "source": [
        "The `BitsAndBytesConfig` controls quantization precisely:\n",
        "- `load_in_4bit=True` — Store weights in 4 bits (NF4 format)\n",
        "- `bnb_4bit_compute_dtype=torch.bfloat16` — Compute in bfloat16 (faster on modern GPUs)\n",
        "- `bnb_4bit_use_double_quant=True` — Quantize the quantization constants themselves, saving an additional $\\sim 0.4$ bits/parameter\n",
        "- `bnb_4bit_quant_type=\"nf4\"` — Normal Float 4-bit, optimized for normally-distributed pretrained weights\n",
        "\n",
        "The `repetition_penalty=1.4` penalizes tokens proportionally to how often they have appeared, preventing the degenerate loops that plague autoregressive models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "79501173",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79501173",
        "outputId": "0d260717-2a83-47cf-cfa4-94ac0cd23aaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Passing `generation_config` together with generation-related arguments=({'max_new_tokens'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Certainly! Peru is a fascinating country located in western South America, bordered by Ecuador and Colombia to the north, Brazil to the east, Bolivia to the southeast, Chile to the south, and the Pacific Ocean to the west. Here are some key points about Peru:\n",
            "\n",
            "1. **Culture and History**:\n",
            "   - **Ancient Civilizations**: Peru is home to some of the most significant pre-Columbian civilizations, including the Inca Empire, which was one of the largest empires in the Americas before the arrival of the Spanish conquistadors.\n",
            "   - **Language**: The official language is Spanish, but Quechua, an indigenous language, is also widely spoken, especially in rural areas.\n",
            "\n",
            "2. **Geography**:\n",
            "   - **Andes Mountains**: The Andes run through the center of the country, forming a natural barrier between the coast and the Amazon rainforest.\n",
            "   - **Coastal Plains**: The western part of Peru borders the Pacific Ocean and has a long coastline.\n",
            "   - **Amazon Basin**: The eastern part of Peru is part of the Amazon rainforest.\n",
            "\n",
            "3. **Cities**:\n",
            "   - **Lima**: The capital and largest city, known for its colonial architecture and vibrant culture.\n",
            "   - **Cusco**: A\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 2.2  Multi-turn conversation prompt\n",
        "\n",
        "prompt = [\n",
        "    {\"role\": \"user\", \"content\": \"What is your favourite country?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Well, I am quite fascinated with Peru.\"},\n",
        "    {\"role\": \"user\", \"content\": \"What can you tell me about Peru?\"}\n",
        "]\n",
        "\n",
        "outputs = pipe_2(prompt, max_new_tokens=256)\n",
        "answer = outputs[0][\"generated_text\"][-1][\"content\"]\n",
        "print(answer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa2f6ae8",
      "metadata": {
        "id": "aa2f6ae8"
      },
      "source": [
        "The instruct model follows the multi-turn conversation pattern naturally. By providing a (user, assistant, user) sequence, we establish context: the model knows we are discussing Peru and generates a relevant, structured response.\n",
        "\n",
        "This **chat template** format is how modern LLMs distinguish between system instructions, user messages, and assistant responses. The tokenizer internally converts this into special tokens (e.g., `<|begin_of_text|>`, `<|start_header_id|>`) that the model was trained to recognize."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "7cfcd5a4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cfcd5a4",
        "outputId": "1fc5cd53-ef50-43ba-a989-e70b3128305b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Certainly! Let's break it down step by step:\n",
            "\n",
            "1. **Determine Sarah's current age**: Sarah is currently 6 years old.\n",
            "2. **Calculate Mary's current age**: Since Mary is twice as old as Sarah, we multiply Sarah's age by 2.\n",
            "   \\[\n",
            "   \\text{Mary's current age} = 2 \\times \\text{Sarah's age} = 2 \\times 6 = 12 \\text{ years old}\n",
            "   \\]\n",
            "3. **Calculate Mary's age after 5 years**: To find out how old Mary will be after 5 years, we add 5 to her current age.\n",
            "   \\[\n",
            "   \\text{Mary's age after 5 years} = \\text{Mary's current age} + 5 = 12 + 5 = 17 \\text{ years old}\n",
            "   \\]\n",
            "\n",
            "So, Mary will be 17 years old after 5 years.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 2.3  Math reasoning via instruction\n",
        "\n",
        "math_prompt = [\n",
        "    {\"role\": \"user\", \"content\": \"Mary is twice as old as Sarah presently. Sarah is 6 years old.\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Well, what can I help you with?\"},\n",
        "    {\"role\": \"user\", \"content\": \"Can you tell me in a step by step way how old Mary will be after 5 years?\"}\n",
        "]\n",
        "\n",
        "math_output = pipe_2(math_prompt, max_new_tokens=256)\n",
        "print(math_output[0][\"generated_text\"][-1][\"content\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01dccfad",
      "metadata": {
        "id": "01dccfad"
      },
      "source": [
        "The model demonstrates **chain-of-thought reasoning** -- breaking the problem into explicit steps:\n",
        "1. Identify Sarah's age ($6$)\n",
        "2. Calculate Mary's current age ($2 \\times 6 = 12$)\n",
        "3. Add $5$ years ($12 + 5 = 17$)\n",
        "\n",
        "This step-by-step approach emerges from instruction tuning on mathematical reasoning datasets. The model does not \"understand\" arithmetic -- it pattern-matches from training examples -- but the results are correct for simple problems.\n",
        "\n",
        "**Limitation:** LLMs can fail on multi-step math with large numbers or unusual problem structures. For production math, always verify with a deterministic calculator (as we will see in Recipe 8 with the agent's Calculator tool)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "2a26606e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a26606e",
        "outputId": "b259bdd0-4e0f-43ff-a132-cc7213cea8de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU memory freed.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 2.4  Free GPU memory\n",
        "\n",
        "del model_2, tokenizer_2, pipe_2\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "print(\"GPU memory freed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8df64540",
      "metadata": {
        "id": "8df64540"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c7c023c",
      "metadata": {
        "id": "0c7c023c"
      },
      "source": [
        "## Recipe 3 — Augmenting an LLM with External Data (LangChain Introduction)\n",
        "\n",
        "So far, we invoked models directly. **LangChain** provides a framework to compose LLMs with prompts, parsers, retrievers, and tools into reusable **chains**. The core abstraction is the pipe operator `|`:\n",
        "\n",
        "```\n",
        "chain = prompt | llm | output_parser\n",
        "```\n",
        "\n",
        "Each component receives the previous component's output. This is called **LangChain Expression Language (LCEL)**. We demonstrate with a simple prompt-to-LLM chain, then show its limitation on current events -- motivating RAG in Recipe 4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "9a3b7a2e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a3b7a2e",
        "outputId": "52a2d253-4557-404d-81b0-cf2b4e34b1a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Passing `generation_config` together with generation-related arguments=({'max_new_tokens', 'pad_token_id'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangChain HuggingFacePipeline ready\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 3.1  Load Llama via LangChain's HuggingFacePipeline\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_huggingface.llms import HuggingFacePipeline\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM, AutoTokenizer,\n",
        "    BitsAndBytesConfig, pipeline)\n",
        "\n",
        "model_name_3 = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\")\n",
        "\n",
        "model_3 = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name_3,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=quantization_config)\n",
        "tokenizer_3 = AutoTokenizer.from_pretrained(model_name_3)\n",
        "\n",
        "pipe_3 = pipeline(\"text-generation\",\n",
        "    model=model_3, tokenizer=tokenizer_3,\n",
        "    max_new_tokens=500,\n",
        "    pad_token_id=tokenizer_3.eos_token_id)\n",
        "\n",
        "hf_llm = HuggingFacePipeline(pipeline=pipe_3)\n",
        "print(\"LangChain HuggingFacePipeline ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "cd904e17",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd904e17",
        "outputId": "63e12165-0e79-4186-e87b-97f0fed266d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=500) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System: You are a great mentor.\n",
            "Human: how can I improve my software engineering skills? Improving your software engineering skills is a continuous process that involves learning, practice, and staying up-to-date with the latest trends and technologies. Here are some steps you can take to enhance your skills:\n",
            "\n",
            "### 1. **Learn the Basics**\n",
            "   - **Programming Languages:** Master one or more programming languages. Start with basics like Python, Java, or JavaScript, and then explore more advanced languages like Rust, Go, or Kotlin.\n",
            "   - **Data Structures and Algorithms:** Understand fundamental data structures (arrays, linked lists, stacks, queues) and algorithms (sorting, searching, recursion). Practice problems on platforms like LeetCode, HackerRank, or CodeSignal.\n",
            "\n",
            "### 2. **Build Projects**\n",
            "   - **Personal Projects:** Work on personal projects that interest you. This could be anything from a web application to a mobile app or a game.\n",
            "   - **Open Source Contributions:** Contribute to open-source projects on GitHub. This not only helps you learn but also improves your portfolio.\n",
            "\n",
            "### 3. **Practice Regularly**\n",
            "   - **Daily Coding Challenges:** Solve coding challenges daily. Websites like Daily Coding Problem, Codewars, or Project Euler can help.\n",
            "   - **Code Reviews:** Participate in code reviews and critiques to understand different approaches and styles.\n",
            "\n",
            "### 4. **Learn Best Practices**\n",
            "   - **Design Patterns:** Study design patterns such as Singleton, Factory, Observer, etc., to\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 3.2  Simple LCEL chain\n",
        "\n",
        "prompt_3 = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a great mentor.\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "chain_3 = prompt_3 | hf_llm | output_parser\n",
        "\n",
        "result = chain_3.invoke(\n",
        "    {\"input\": \"how can I improve my software engineering skills?\"})\n",
        "print(result[:1500])  # Truncate for readability\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d66ccf6",
      "metadata": {
        "id": "0d66ccf6"
      },
      "source": [
        "LangChain's `ChatPromptTemplate` fills the `{input}` placeholder at invocation time, pipes it to the LLM, and the `StrOutputParser` extracts the text from the model's output. The chain is **declarative** -- we define the pipeline once and invoke it many times with different inputs.\n",
        "\n",
        "The model produces reasonable mentoring advice. But what happens when we ask about current events?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "e54ac533",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e54ac533",
        "outputId": "614df505-f3a4-479d-f02e-ccbbd999c3e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=500) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Answer the question. Keep your answer to less than 30 words.\n",
            "Question: How many volunteers were present for the 2024 summer olympics? The number is not known yet.\n",
            "\n",
            "Assistant: The number of volunteers for the 2024 Summer Olympics in Paris has not been announced yet.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 3.3  Demonstrate LLM knowledge cutoff limitation\n",
        "\n",
        "template = \"Answer the question. Keep your answer to less than 30 words.\\nQuestion: {input}\"\n",
        "prompt_short = ChatPromptTemplate.from_template(template)\n",
        "chain_short = prompt_short | hf_llm | output_parser\n",
        "\n",
        "result_olympics = chain_short.invoke(\n",
        "    {\"input\": \"How many volunteers were present for the 2024 summer olympics?\"})\n",
        "print(result_olympics[:500])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6519559",
      "metadata": {
        "id": "a6519559"
      },
      "source": [
        "The model gives an **inaccurate estimate** (e.g., \"20,000 to 30,000\") because the 2024 Olympics happened after its training data cutoff. This is the fundamental limitation of parametric knowledge: **LLMs can only recall what they were trained on.**\n",
        "\n",
        "The solution is **Retrieval-Augmented Generation (RAG)** -- augmenting the LLM with external, up-to-date content at inference time. We demonstrate this in Recipe 4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "8f6d0c20",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f6d0c20",
        "outputId": "9ca6b3c4-a11d-4017-c507-431d02c9e3bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU memory freed.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 3.4  Free GPU memory\n",
        "\n",
        "del model_3, tokenizer_3, pipe_3, hf_llm\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "print(\"GPU memory freed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5aa03ba",
      "metadata": {
        "id": "f5aa03ba"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b31104a",
      "metadata": {
        "id": "0b31104a"
      },
      "source": [
        "## Recipe 4 — RAG: Augmenting the LLM with External Content\n",
        "\n",
        "**Retrieval-Augmented Generation (RAG)** is the most impactful pattern in modern LLM applications. Instead of relying solely on the model's parametric knowledge, we:\n",
        "\n",
        "1. **Load** external documents (web pages, PDFs, databases)\n",
        "2. **Chunk** them into passages (typically $300$-$500$ tokens each)\n",
        "3. **Embed** each chunk into a dense vector using a sentence transformer\n",
        "4. **Store** vectors in a FAISS index for fast similarity search\n",
        "5. **Retrieve** the top-$k$ most relevant chunks for a given question\n",
        "6. **Generate** an answer conditioned on both the question and the retrieved context\n",
        "\n",
        "$$\\text{RAG}(q) = \\text{LLM}\\Big(\\text{prompt}\\big(q, \\;\\text{top-}k\\;\\text{retrieve}(q, \\mathcal{D})\\big)\\Big)$$\n",
        "\n",
        "We use **GPT-4o-mini** (via API) as the generator and **FAISS** as the vector store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "c5d4d1ab",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5d4d1ab",
        "outputId": "e699633c-3208-49c8-f0d3-c1c1c52627cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI LLM: gpt-4o-mini\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 4.1  Initialize OpenAI LLM\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm_openai = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "print(f\"OpenAI LLM: gpt-4o-mini\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "b159e03d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b159e03d",
        "outputId": "bcf9d7fe-9e34-42a8-ad0d-d13605987056"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 1 document(s), total 116,359 chars\n",
            "Split into 325 chunks (avg 368 chars each)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 4.2  Load and chunk a web page\n",
        "\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "loader = WebBaseLoader(\n",
        "    [\"https://en.wikipedia.org/wiki/2024_Summer_Olympics\"])\n",
        "docs = loader.load()\n",
        "print(f\"Loaded {len(docs)} document(s), \"\n",
        "      f\"total {sum(len(d.page_content) for d in docs):,} chars\")\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500, chunk_overlap=50)\n",
        "all_splits = text_splitter.split_documents(docs)\n",
        "print(f\"Split into {len(all_splits)} chunks \"\n",
        "      f\"(avg {np.mean([len(c.page_content) for c in all_splits]):.0f} chars each)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed13a461",
      "metadata": {
        "id": "ed13a461"
      },
      "source": [
        "The `RecursiveCharacterTextSplitter` breaks text at natural boundaries (paragraphs, sentences, words) in that priority order. `chunk_size=500` with `chunk_overlap=50` means each chunk is $\\sim 500$ characters with $50$ characters of overlap to preserve context across boundaries.\n",
        "\n",
        "**Why chunk?** LLMs have a finite context window. Sending an entire Wikipedia article ($\\sim 100$K tokens) would exceed the context limit and dilute relevant information. Chunking + retrieval ensures the model sees only the most relevant passages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "6d8294cd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d8294cd",
        "outputId": "8da4a49f-c027-4460-96c5-465ec355f81f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "MPNetModel LOAD REPORT from: sentence-transformers/all-mpnet-base-v2\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS index built: 325 vectors, dimension 768\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 4.3  Build FAISS vector store and retriever\n",
        "\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        "\n",
        "vectorstore = FAISS.from_documents(all_splits, embeddings)\n",
        "retriever = vectorstore.as_retriever(search_type=\"similarity\")\n",
        "\n",
        "print(f\"FAISS index built: {vectorstore.index.ntotal} vectors, \"\n",
        "      f\"dimension {vectorstore.index.d}\")\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c83f5ded",
      "metadata": {
        "id": "c83f5ded"
      },
      "source": [
        "Each chunk is embedded into a $768$-dimensional vector using `all-mpnet-base-v2`. FAISS (Facebook AI Similarity Search) stores these vectors and supports sub-millisecond nearest-neighbor queries via an inverted file index.\n",
        "\n",
        "The `similarity` search type computes:\n",
        "\n",
        "$$\\text{score}(q, d) = \\cos(\\mathbf{e}_q, \\mathbf{e}_d) = \\frac{\\mathbf{e}_q \\cdot \\mathbf{e}_d}{\\|\\mathbf{e}_q\\| \\|\\mathbf{e}_d\\|}$$\n",
        "\n",
        "Unlike BM25 (Chapter 9), this captures **semantic similarity**: \"volunteers helping at the Olympics\" would match a chunk about \"45,000 recruited helpers\" even without shared keywords."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "2b41c5b5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b41c5b5",
        "outputId": "106bdad4-077e-4619-fe67-b7be3d83dd3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG chain assembled\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 4.4  Build RAG chain\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt_rag = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs,\n",
        "     \"question\": RunnablePassthrough()}\n",
        "    | prompt_rag\n",
        "    | llm_openai\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"RAG chain assembled\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "e58d479d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e58d479d",
        "outputId": "385f558f-975a-400e-b7c6-e0375a96ec6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: Where are the 2024 summer olympics being held?\n",
            "A: The 2024 Summer Olympics are being held in Paris, France.\n",
            "\n",
            "Q: What are the new sports added for the 2024 summer olympics?\n",
            "A: The new sports added for the 2024 Summer Olympics include breakdancing (also referred to as breaking).\n",
            "\n",
            "Q: How many volunteers are supposed to be present for the 2024 summer olympics?\n",
            "A: There are expected to be 45,000 volunteers for the 2024 Summer Olympics.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 4.5  Ask questions with RAG\n",
        "\n",
        "questions = [\n",
        "    \"Where are the 2024 summer olympics being held?\",\n",
        "    \"What are the new sports added for the 2024 summer olympics?\",\n",
        "    \"How many volunteers are supposed to be present for the 2024 summer olympics?\",\n",
        "]\n",
        "\n",
        "for q in questions:\n",
        "    answer = rag_chain.invoke(q)\n",
        "    print(f\"Q: {q}\")\n",
        "    print(f\"A: {answer}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bd4b0f1",
      "metadata": {
        "id": "6bd4b0f1"
      },
      "source": [
        "Compare the volunteer question answer here with Recipe 3's result: the RAG chain returns the **correct** figure ($45{,}000$ volunteers, per Wikipedia) while the unaugmented LLM guessed $20{,}000$-$30{,}000$.\n",
        "\n",
        "**RAG vs. fine-tuning:** RAG is preferred when information changes frequently (news, documentation, product catalogs) because you update the vector store, not the model weights. Fine-tuning is better when you need the model to learn a new *style* or *skill* (e.g., domain-specific terminology, code generation patterns).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "feea7c24",
      "metadata": {
        "id": "feea7c24"
      },
      "source": [
        "## Recipe 5 — Creating a Chatbot Using an LLM\n",
        "\n",
        "Recipes 3--4 were **stateless**: each question was independent. A **chatbot** maintains conversation history so follow-up questions like *\"Can you explain more?\"* or *\"What about its history?\"* resolve correctly from context.\n",
        "\n",
        "The key addition is a **contextualization chain** that rewrites follow-up questions into standalone questions using the chat history:\n",
        "\n",
        "```\n",
        "Chat history: [user: \"What is an LLM?\", assistant: \"A large language model...\"]\n",
        "Follow-up:    \"Can you explain why they call it large?\"\n",
        "Rewritten:    \"Why are large language models called 'large'?\"\n",
        "```\n",
        "\n",
        "This rewritten question is then used for retrieval, ensuring the vector store search is meaningful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "d94afe51",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d94afe51",
        "outputId": "cbd4b9e6-d14f-4560-a7e2-d811b1069016"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded: 1 doc, 43,801 chars\n",
            "Vector store: 15 chunks indexed\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 5.1  Load web content and build vector store\n",
        "\n",
        "loader_agent = WebBaseLoader(\n",
        "    [\"https://lilianweng.github.io/posts/2023-06-23-agent/\"])\n",
        "docs_agent = loader_agent.load()\n",
        "print(f\"Loaded: {len(docs_agent)} doc, \"\n",
        "      f\"{sum(len(d.page_content) for d in docs_agent):,} chars\")\n",
        "\n",
        "text_splitter_5 = RecursiveCharacterTextSplitter()\n",
        "chunks_5 = text_splitter_5.split_documents(docs_agent)\n",
        "\n",
        "vectorstore_5 = FAISS.from_documents(chunks_5, embeddings)\n",
        "retriever_5 = vectorstore_5.as_retriever(search_type=\"similarity\")\n",
        "print(f\"Vector store: {vectorstore_5.index.ntotal} chunks indexed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "78eac1d5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78eac1d5",
        "outputId": "b396a9be-5f89-4235-9dd9-51c14d25edb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contextualization chain ready\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 5.2  Build contextualization chain\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "\n",
        "output_parser_5 = StrOutputParser()\n",
        "\n",
        "# Chain 1: Rewrite follow-up questions using chat history\n",
        "contextualize_system = (\n",
        "    \"Given a chat history and the latest user question which might \"\n",
        "    \"reference context in the chat history, formulate a standalone \"\n",
        "    \"question which can be understood without the chat history. \"\n",
        "    \"Do NOT answer the question, just reformulate it if needed \"\n",
        "    \"and otherwise return it as is.\")\n",
        "\n",
        "contextualize_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", contextualize_system),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"human\", \"{question}\"),\n",
        "])\n",
        "\n",
        "contextualize_chain = contextualize_prompt | llm_openai | output_parser_5\n",
        "print(\"Contextualization chain ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "c755e632",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c755e632",
        "outputId": "3b948570-c531-41ac-e7d2-71a80746ca03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot RAG chain ready\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 5.3  Build RAG chain with chat history\n",
        "\n",
        "qa_system = (\n",
        "    \"You are an assistant for question-answering tasks. \"\n",
        "    \"Use the following pieces of retrieved context to answer the question. \"\n",
        "    \"If you don't know the answer, just say that you don't know. \"\n",
        "    \"Use three sentences maximum and keep the answer concise.\"\n",
        "    \"\\n\\n{context}\")\n",
        "\n",
        "qa_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", qa_system),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"human\", \"{question}\"),\n",
        "])\n",
        "\n",
        "def contextualized_question(input_dict):\n",
        "    if input_dict.get(\"chat_history\"):\n",
        "        return contextualize_chain\n",
        "    else:\n",
        "        return input_dict[\"question\"]\n",
        "\n",
        "rag_chain_5 = (\n",
        "    RunnablePassthrough.assign(\n",
        "        context=contextualized_question | retriever_5 | format_docs)\n",
        "    | qa_prompt\n",
        "    | llm_openai\n",
        ")\n",
        "print(\"Chatbot RAG chain ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "31635c98",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31635c98",
        "outputId": "c3d6bcd8-5847-4b54-9a1c-07bf517c55d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: What is a large language model?\n",
            "AI: A large language model (LLM) is a type of artificial intelligence that uses deep learning techniques to process and generate human-like text. It is trained on vast amounts of text data, allowing it to understand context, grammar, and nuances in language, facilitating tasks like writing, translation, and conversation. LLMs can also be fine-tuned for specific applications or domains to enhance their performance in particular contexts.\n",
            "\n",
            "Human: Can you explain the reasoning behind calling it large?\n",
            "AI: The term \"large\" in large language model (LLM) refers to the size and scale of the model, which is measured by the number of parameters it contains, often in the billions or even trillions. A larger model typically has a greater capacity to understand and generate complex language patterns, due to its extensive training on diverse datasets. This increased size allows LLMs to perform better on a wide range of tasks by capturing more nuanced meanings and relationships in data.\n",
            "\n",
            "Human: What tools can they use?\n",
            "AI: Large language models (LLMs) can use various external tools and APIs to enhance their capabilities, including search engines, calculators, and databases for real-time information retrieval. They can call specific APIs for tasks like health data management, account authentication, and smart home control, which provide specialized functionalities beyond their pre-trained knowledge. Additionally, LLMs can interact with expert modules for specific domains, such as scientific discovery and data analysis, further augmenting their performance.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 5.4  Multi-turn conversation\n",
        "\n",
        "chat_history = []\n",
        "\n",
        "# Turn 1\n",
        "q1 = \"What is a large language model?\"\n",
        "a1 = rag_chain_5.invoke({\"question\": q1, \"chat_history\": chat_history})\n",
        "print(f\"Human: {q1}\")\n",
        "print(f\"AI: {a1.content}\")\n",
        "print()\n",
        "\n",
        "chat_history.extend([\n",
        "    HumanMessage(content=q1),\n",
        "    AIMessage(content=a1.content)\n",
        "])\n",
        "\n",
        "# Turn 2 (follow-up -- relies on chat history)\n",
        "q2 = \"Can you explain the reasoning behind calling it large?\"\n",
        "a2 = rag_chain_5.invoke({\"question\": q2, \"chat_history\": chat_history})\n",
        "print(f\"Human: {q2}\")\n",
        "print(f\"AI: {a2.content}\")\n",
        "print()\n",
        "\n",
        "chat_history.extend([\n",
        "    HumanMessage(content=q2),\n",
        "    AIMessage(content=a2.content)\n",
        "])\n",
        "\n",
        "# Turn 3 (another follow-up)\n",
        "q3 = \"What tools can they use?\"\n",
        "a3 = rag_chain_5.invoke({\"question\": q3, \"chat_history\": chat_history})\n",
        "print(f\"Human: {q3}\")\n",
        "print(f\"AI: {a3.content}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cadb3c8",
      "metadata": {
        "id": "8cadb3c8"
      },
      "source": [
        "The chatbot correctly resolves **\"it\"** in Turn 2 to \"large language model\" and **\"they\"** in Turn 3 to \"LLMs\" by using the contextualization chain. Without chat history, \"Can you explain the reasoning behind calling it large?\" would be meaningless to the retriever.\n",
        "\n",
        "**Architecture of the chatbot:**\n",
        "\n",
        "$$\\text{Follow-up} \\xrightarrow{\\text{contextualize}} \\text{Standalone question} \\xrightarrow{\\text{retrieve}} \\text{Relevant chunks} \\xrightarrow{\\text{generate}} \\text{Answer}$$\n",
        "\n",
        "For production chatbots, use `RunnableWithMessageHistory` and a persistent message store (Redis, DynamoDB) rather than an in-memory list.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85b509c5",
      "metadata": {
        "id": "85b509c5"
      },
      "source": [
        "## Recipe 6 — Generating Code Using an LLM\n",
        "\n",
        "LLMs trained on code repositories can synthesize programs from natural-language descriptions. We compare **GPT-4o-mini** (API) with the instruction it receives, demonstrating how model quality affects code generation.\n",
        "\n",
        "**Important caveat:** LLM-generated code should never be deployed without thorough testing. Treat it as a first draft that accelerates development, not a finished product."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "60d751c0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60d751c0",
        "outputId": "eead2cef-7592-4c85-e081-17a3db6634f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Binary Tree Inorder Traversal ===\n",
            "```python\n",
            "class TreeNode:\n",
            "    def __init__(self, value=0, left=None, right=None):\n",
            "        self.value = value\n",
            "        self.left = left\n",
            "        self.right = right\n",
            "\n",
            "def inorder_traversal(root):\n",
            "    if root is not None:\n",
            "        inorder_traversal(root.left)\n",
            "        print(root.value, end=' ')\n",
            "        inorder_traversal(root.right)\n",
            "\n",
            "# Example usage:\n",
            "if __name__ == \"__main__\":\n",
            "    # Creating a binary tree\n",
            "    root = TreeNode(1)\n",
            "    root.left = TreeNode(2)\n",
            "    root.right = TreeNode(3)\n",
            "    root.left.left = TreeNode(4)\n",
            "    root.left.right = TreeNode(5)\n",
            "\n",
            "    print(\"Inorder Traversal of the Binary Tree:\")\n",
            "    inorder_traversal(root)\n",
            "```\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 6.1  Code generation with GPT-4o-mini\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "code_template = \"\"\"Write some python code to solve the user's problem.\n",
        "Return only python code in Markdown format, e.g.:\n",
        "```python\n",
        "....\n",
        "```\"\"\"\n",
        "\n",
        "code_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", code_template),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "code_chain = code_prompt | llm_openai | StrOutputParser()\n",
        "\n",
        "# Example 1: Binary tree traversal\n",
        "result_tree = code_chain.invoke(\n",
        "    {\"input\": \"write a program to print a binary tree in an inorder traversal\"})\n",
        "print(\"=== Binary Tree Inorder Traversal ===\")\n",
        "print(result_tree)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "fc32d3da",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc32d3da",
        "outputId": "e74a915e-867e-411b-bc71-f93aeeaa77fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== SHA3-512 Hash ===\n",
            "```python\n",
            "import hashlib\n",
            "\n",
            "def generate_sha3_512_hash(data):\n",
            "    sha3_512 = hashlib.sha3_512()\n",
            "    sha3_512.update(data.encode('utf-8'))\n",
            "    return sha3_512.hexdigest()\n",
            "\n",
            "# Example usage\n",
            "data = \"Hello, World!\"\n",
            "hash_value = generate_sha3_512_hash(data)\n",
            "print(f\"SHA3-512 hash of '{data}': {hash_value}\")\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 6.2  Another code generation example\n",
        "\n",
        "result_hash = code_chain.invoke(\n",
        "    {\"input\": \"write a program to generate a 512-bit SHA3 hash\"})\n",
        "print(\"=== SHA3-512 Hash ===\")\n",
        "print(result_hash)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a965368",
      "metadata": {
        "id": "0a965368"
      },
      "source": [
        "GPT-4o-mini produces **concise, well-structured** code with example usage. In the book's comparison, locally-hosted Llama-3.1-8B generated correct but overly verbose code (including unrequested preorder traversal and excessive commentary), while ChatGPT was more focused.\n",
        "\n",
        "**Code generation quality hierarchy** (as of 2024):\n",
        "- **Best:** GPT-4o, Claude 3.5 Sonnet -- near-expert quality, handles complex algorithms\n",
        "- **Good:** GPT-4o-mini, Llama-3.1-70B -- solid for standard problems\n",
        "- **Adequate:** Llama-3.1-8B, Mistral-7B -- correct logic, verbose/noisy output\n",
        "- **Specialized:** CodeLlama, DeepSeek-Coder -- optimized specifically for code tasks\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "045b2a85",
      "metadata": {
        "id": "045b2a85"
      },
      "source": [
        "## Recipe 7 — Generating SQL Queries from Natural Language\n",
        "\n",
        "Text-to-SQL allows non-technical users to query databases using plain English. The LLM infers the database schema and generates the appropriate SQL. We create an in-memory SQLite database to demonstrate the full pipeline: question $\\rightarrow$ SQL $\\rightarrow$ execution $\\rightarrow$ natural-language answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "843d58c3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "843d58c3",
        "outputId": "5f29f8fe-e316-495e-931f-73584f291cee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Database tables: ['Departments', 'Employees']\n",
            "Schema preview:\n",
            "\n",
            "CREATE TABLE \"Departments\" (\n",
            "\t\"DeptID\" INTEGER, \n",
            "\t\"DeptName\" TEXT NOT NULL, \n",
            "\tPRIMARY KEY (\"DeptID\")\n",
            ")\n",
            "\n",
            "/*\n",
            "3 rows from Departments table:\n",
            "DeptID\tDeptName\n",
            "1\tEngineering\n",
            "2\tMarketing\n",
            "3\tSales\n",
            "*/\n",
            "\n",
            "\n",
            "CREATE TABLE \"Employees\" (\n",
            "\t\"EmpID\" INTEGER, \n",
            "\t\"FirstName\" TEXT NOT NULL, \n",
            "\t\"LastName\" TEXT NOT NULL, \n",
            "\t\"DeptID\" INTEGER, \n",
            "\t\"HireDate\" TEXT, \n",
            "\t\"Salary\" REAL, \n",
            "\tPRIMARY KEY (\"EmpID\"), \n",
            "\tFOREIGN KEY(\"DeptID\") REFERENCES \"Departments\" (\"DeptID\")\n",
            ")\n",
            "\n",
            "/*\n",
            "3 rows from Employees table:\n",
            "EmpID\tFirstName\tLastName\tDep\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 7.1  Create a sample SQLite database\n",
        "\n",
        "import sqlite3\n",
        "\n",
        "conn = sqlite3.connect(\"/tmp/company.db\")\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Create and populate tables\n",
        "cursor.executescript(\"\"\"\n",
        "DROP TABLE IF EXISTS Employees;\n",
        "DROP TABLE IF EXISTS Departments;\n",
        "\n",
        "CREATE TABLE Departments (\n",
        "    DeptID INTEGER PRIMARY KEY,\n",
        "    DeptName TEXT NOT NULL\n",
        ");\n",
        "\n",
        "CREATE TABLE Employees (\n",
        "    EmpID INTEGER PRIMARY KEY,\n",
        "    FirstName TEXT NOT NULL,\n",
        "    LastName TEXT NOT NULL,\n",
        "    DeptID INTEGER,\n",
        "    HireDate TEXT,\n",
        "    Salary REAL,\n",
        "    FOREIGN KEY (DeptID) REFERENCES Departments(DeptID)\n",
        ");\n",
        "\n",
        "INSERT INTO Departments VALUES (1, 'Engineering');\n",
        "INSERT INTO Departments VALUES (2, 'Marketing');\n",
        "INSERT INTO Departments VALUES (3, 'Sales');\n",
        "INSERT INTO Departments VALUES (4, 'HR');\n",
        "\n",
        "INSERT INTO Employees VALUES (1, 'Nancy', 'Davolio', 1, '2012-05-01', 95000);\n",
        "INSERT INTO Employees VALUES (2, 'Andrew', 'Fuller', 1, '2012-08-14', 105000);\n",
        "INSERT INTO Employees VALUES (3, 'Janet', 'Leverling', 2, '2012-04-01', 88000);\n",
        "INSERT INTO Employees VALUES (4, 'Margaret', 'Peacock', 3, '2015-09-15', 78000);\n",
        "INSERT INTO Employees VALUES (5, 'Steven', 'Buchanan', 1, '2016-03-21', 92000);\n",
        "INSERT INTO Employees VALUES (6, 'Michael', 'Suyama', 2, '2018-01-10', 82000);\n",
        "INSERT INTO Employees VALUES (7, 'Robert', 'King', 3, '2019-07-22', 75000);\n",
        "INSERT INTO Employees VALUES (8, 'Laura', 'Callahan', 4, '2020-11-05', 70000);\n",
        "INSERT INTO Employees VALUES (9, 'Anne', 'Dodsworth', 1, '2021-02-18', 90000);\n",
        "\"\"\")\n",
        "conn.commit()\n",
        "conn.close()\n",
        "\n",
        "from langchain_community.utilities import SQLDatabase\n",
        "db = SQLDatabase.from_uri(\"sqlite:////tmp/company.db\")\n",
        "print(f\"Database tables: {db.get_usable_table_names()}\")\n",
        "print(f\"Schema preview:\\n{db.get_table_info()[:500]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "2be1871f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2be1871f",
        "outputId": "d39f0772-3bcd-48e0-ae40-f63f78b3ad2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: How many employees are there?\n",
            "SQL: SELECT COUNT(*) FROM Employees;\n",
            "Result: [(9,)]\n",
            "\n",
            "Q: How many employees have been hired before 2015?\n",
            "SQL: SELECT COUNT(*) FROM Employees WHERE HireDate < '2015-01-01';\n",
            "Result: [(3,)]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 7.2  SQL generation chain\n",
        "\n",
        "def get_schema(_):\n",
        "    return db.get_table_info()\n",
        "\n",
        "def run_query(query):\n",
        "    return db.run(query)\n",
        "\n",
        "sql_template = \"\"\"You are a SQL expert. Based on the table schema below,\n",
        "write just the SQL query (no explanation, no markdown) that would answer\n",
        "the user's question:\n",
        "\n",
        "{schema}\n",
        "\n",
        "Question: {question}\n",
        "SQL Query:\"\"\"\n",
        "\n",
        "sql_prompt = ChatPromptTemplate.from_template(sql_template)\n",
        "\n",
        "sql_chain = (\n",
        "    RunnablePassthrough.assign(schema=get_schema)\n",
        "    | sql_prompt\n",
        "    | llm_openai.bind(stop=[\"\\nSQLResult:\"])\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Test: Simple count\n",
        "q1 = \"How many employees are there?\"\n",
        "sql1 = sql_chain.invoke({\"question\": q1})\n",
        "print(f\"Q: {q1}\")\n",
        "print(f\"SQL: {sql1}\")\n",
        "print(f\"Result: {run_query(sql1)}\")\n",
        "print()\n",
        "\n",
        "# Test: Schema inference\n",
        "q2 = \"How many employees have been hired before 2015?\"\n",
        "sql2 = sql_chain.invoke({\"question\": q2})\n",
        "print(f\"Q: {q2}\")\n",
        "print(f\"SQL: {sql2}\")\n",
        "print(f\"Result: {run_query(sql2)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12b589fb",
      "metadata": {
        "id": "12b589fb"
      },
      "source": [
        "The LLM correctly infers the database schema and maps natural-language concepts to SQL columns: \"hired before 2015\" $\\rightarrow$ `WHERE HireDate < '2015-01-01'`. This **schema inference** is remarkable -- the model understands that \"tenure\" relates to `HireDate`, not a dedicated tenure column.\n",
        "\n",
        "The `.bind(stop=[\"\\nSQLResult:\"])` prevents the model from generating imaginary query results after the SQL statement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "b931ed68",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b931ed68",
        "outputId": "7b54f6bf-a7e5-48c6-d110-bf8e38d24bcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: How many employees are there?\n",
            "A: The total number of employees is 9.\n",
            "\n",
            "Q: What is the average salary in the Engineering department?\n",
            "A: The average salary in the Engineering department is $95,500.\n",
            "\n",
            "Q: Give me the names of employees hired before 2015.\n",
            "A: The employees who were hired before the year 2015 are Nancy Davolio, Andrew Fuller, and Janet Leverling.\n",
            "\n",
            "Q: Which department has the most employees?\n",
            "A: The department with the most employees is Engineering.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 7.3  Full chain: question -> SQL -> execute -> answer\n",
        "\n",
        "answer_template = \"\"\"Based on the table schema below, question, SQL query,\n",
        "and SQL response, write a natural language response:\n",
        "\n",
        "{schema}\n",
        "\n",
        "Question: {question}\n",
        "SQL Query: {query}\n",
        "SQL Response: {response}\"\"\"\n",
        "\n",
        "answer_prompt = ChatPromptTemplate.from_template(answer_template)\n",
        "\n",
        "full_sql_chain = (\n",
        "    RunnablePassthrough.assign(query=sql_chain).assign(\n",
        "        schema=get_schema,\n",
        "        response=lambda x: run_query(x[\"query\"]),\n",
        "    )\n",
        "    | answer_prompt\n",
        "    | llm_openai\n",
        ")\n",
        "\n",
        "# End-to-end test\n",
        "questions_sql = [\n",
        "    \"How many employees are there?\",\n",
        "    \"What is the average salary in the Engineering department?\",\n",
        "    \"Give me the names of employees hired before 2015.\",\n",
        "    \"Which department has the most employees?\",\n",
        "]\n",
        "\n",
        "for q in questions_sql:\n",
        "    result = full_sql_chain.invoke({\"question\": q})\n",
        "    print(f\"Q: {q}\")\n",
        "    print(f\"A: {result.content}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b14ba5e",
      "metadata": {
        "id": "3b14ba5e"
      },
      "source": [
        "The full chain produces **human-readable answers** from plain-English questions, completely hiding the SQL layer from the user. The LLM acts as both the SQL generator and the answer formatter.\n",
        "\n",
        "**Production considerations:**\n",
        "- **Security:** Always use read-only database connections. Validate generated SQL against an allowlist of operations (SELECT only, no DROP/DELETE).\n",
        "- **Schema complexity:** For large schemas ($100$+ tables), provide only relevant table descriptions rather than the full schema to avoid context window overflow.\n",
        "- **Accuracy:** Always log generated SQL for audit. Consider a verification step where a second LLM call checks whether the SQL correctly answers the question.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73460672",
      "metadata": {
        "id": "73460672"
      },
      "source": [
        "## Recipe 8 — Agents: Making an LLM Reason and Act\n",
        "\n",
        "An **agent** goes beyond static chains: it can dynamically choose which tools to use, observe results, and decide on next steps. This follows the **ReAct** (Reason + Act) pattern:\n",
        "\n",
        "$$\\text{Thought} \\rightarrow \\text{Action} \\rightarrow \\text{Observation} \\rightarrow \\text{Thought} \\rightarrow \\cdots \\rightarrow \\text{Final Answer}$$\n",
        "\n",
        "The agent loops through reasoning steps until it has enough information to answer. Each \"action\" invokes a **tool** (web search, calculator, database query, API call, etc.).\n",
        "\n",
        "We build an agent with two tools: a **web search** tool (for current information) and a **calculator** tool (for precise arithmetic)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "77274683",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77274683",
        "outputId": "b2d0c216-b961-4b78-b2d9-8834e162366a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tools registered: ['Knowledge', 'Calculator']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 8.1  Define tools\n",
        "\n",
        "from langchain.agents import AgentExecutor, create_react_agent\n",
        "from langchain.tools import Tool\n",
        "from langchain.chains import LLMMathChain\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# Tool 1: Math calculator\n",
        "math_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "math_chain = LLMMathChain.from_llm(llm=math_llm, verbose=False)\n",
        "math_tool = Tool(\n",
        "    name=\"Calculator\",\n",
        "    func=math_chain.run,\n",
        "    description=\"Use this tool for mathematical calculations. \"\n",
        "                \"Input should be a mathematical expression.\")\n",
        "\n",
        "# Tool 2: Simple knowledge tool (using the LLM itself)\n",
        "def knowledge_search(query):\n",
        "    \"\"\"Use the LLM for general knowledge questions.\"\"\"\n",
        "    response = llm_openai.invoke(query)\n",
        "    return response.content\n",
        "\n",
        "search_tool = Tool(\n",
        "    name=\"Knowledge\",\n",
        "    func=knowledge_search,\n",
        "    description=\"Use this tool to look up factual information, \"\n",
        "                \"including sports statistics, geography, and history.\")\n",
        "\n",
        "tools = [search_tool, math_tool]\n",
        "print(f\"Tools registered: {[t.name for t in tools]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cd35569",
      "metadata": {
        "id": "8cd35569"
      },
      "source": [
        "We define two tools:\n",
        "- **Knowledge:** Queries the LLM for factual information (in production, replace with a real search API like SerpAPI or Tavily)\n",
        "- **Calculator:** Uses `LLMMathChain` which translates math questions into Python expressions and evaluates them with `numexpr` -- guaranteeing correct arithmetic\n",
        "\n",
        "Each tool has a `name` and `description` that the agent reads to decide which tool to use for each step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "813b4a2f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "813b4a2f",
        "outputId": "601aa2c6-b5a0-4087-db07-fb4be40b157f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ReAct agent ready\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 8.2  Create ReAct agent\n",
        "\n",
        "# ReAct prompt template\n",
        "react_template = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
        "\n",
        "{tools}\n",
        "\n",
        "Use the following format:\n",
        "\n",
        "Question: the input question you must answer\n",
        "Thought: you should always think about what to do\n",
        "Action: the action to take, should be one of [{tool_names}]\n",
        "Action Input: the input to the action\n",
        "Observation: the result of the action\n",
        "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
        "Thought: I now know the final answer\n",
        "Final Answer: the final answer to the original input question\n",
        "\n",
        "Begin!\n",
        "\n",
        "Question: {input}\n",
        "Thought:{agent_scratchpad}\"\"\"\n",
        "\n",
        "react_prompt = PromptTemplate.from_template(react_template)\n",
        "\n",
        "agent = create_react_agent(\n",
        "    llm=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0),\n",
        "    tools=tools,\n",
        "    prompt=react_prompt)\n",
        "\n",
        "agent_executor = AgentExecutor(\n",
        "    agent=agent,\n",
        "    tools=tools,\n",
        "    verbose=True,\n",
        "    handle_parsing_errors=True,\n",
        "    max_iterations=5)\n",
        "\n",
        "print(\"ReAct agent ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "7edb406d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7edb406d",
        "outputId": "c2fce8ab-c835-47cb-ca2c-29a51def9f48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mTo answer the question, I need to look up the number of FIFA World Cup wins for both Brazil and France, and then calculate the difference between the two. \n",
            "\n",
            "Action: Knowledge\n",
            "Action Input: \"How many FIFA World Cup wins does Brazil have?\"\u001b[0m\u001b[36;1m\u001b[1;3mAs of October 2023, Brazil has won the FIFA World Cup a total of five times. Their victories came in the years 1958, 1962, 1970, 1994, and 2002.\u001b[0m\u001b[32;1m\u001b[1;3mI have found that Brazil has won the FIFA World Cup five times. Now, I need to find out how many times France has won the World Cup.\n",
            "\n",
            "Action: Knowledge  \n",
            "Action Input: \"How many FIFA World Cup wins does France have?\"  \u001b[0m\u001b[36;1m\u001b[1;3mAs of October 2023, France has won the FIFA World Cup twice. They first won in 1998 and then again in 2018.\u001b[0m\u001b[32;1m\u001b[1;3mI have found that France has won the FIFA World Cup twice. Now, I need to calculate the difference in the number of wins between Brazil and France.\n",
            "\n",
            "Action: Calculator  \n",
            "Action Input: \"5 - 2\"  \u001b[0m\u001b[33;1m\u001b[1;3mAnswer: 3\u001b[0m\u001b[32;1m\u001b[1;3mI now know the final answer.  \n",
            "Final Answer: Brazil has 5 FIFA World Cup wins, France has 2, and the difference is 3.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "Final answer: Brazil has 5 FIFA World Cup wins, France has 2, and the difference is 3.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 8.3  Agent in action: multi-step reasoning\n",
        "\n",
        "result = agent_executor.invoke({\n",
        "    \"input\": \"How many FIFA world cup wins does Brazil have? \"\n",
        "             \"How many does France have? \"\n",
        "             \"What is the difference?\"\n",
        "})\n",
        "\n",
        "print(f\"\\nFinal answer: {result['output']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41e2cd09",
      "metadata": {
        "id": "41e2cd09"
      },
      "source": [
        "The agent demonstrates the ReAct loop:\n",
        "1. **Thought:** \"I need to find Brazil's World Cup wins\"\n",
        "2. **Action:** Knowledge tool $\\rightarrow$ \"Brazil has won 5 times\"\n",
        "3. **Thought:** \"Now I need France's wins\"\n",
        "4. **Action:** Knowledge tool $\\rightarrow$ \"France has won 2 times\"\n",
        "5. **Thought:** \"I need to calculate the difference\"\n",
        "6. **Action:** Calculator $\\rightarrow$ $5 - 2 = 3$\n",
        "7. **Final Answer:** \"Brazil has 3 more World Cup wins than France\"\n",
        "\n",
        "The agent **chose the right tool for each step** -- Knowledge for factual lookup, Calculator for arithmetic -- without being told which to use. This emergent tool selection is what makes agents powerful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "b3658ece",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3658ece",
        "outputId": "315c1f58-9e3c-4f23-b0f8-9450017a0743"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mTo answer the question, I first need to find the current population of Tokyo. After that, I can calculate the total waste produced in a week based on the given waste production rate per person.\n",
            "\n",
            "Action: Knowledge  \n",
            "Action Input: \"What is the population of Tokyo in 2023?\"  \u001b[0m\u001b[36;1m\u001b[1;3mAs of 2023, the population of Tokyo is estimated to be around 14 million in the city proper, while the Greater Tokyo Area, which includes surrounding prefectures, is often cited as having a population of approximately 37 million. For the most current and precise figures, it’s best to consult official statistics or demographic resources.\u001b[0m\u001b[32;1m\u001b[1;3mI have the population of Tokyo as approximately 14 million for the city proper. Now, I need to calculate the total waste produced in a week if each person produces 2 kg of waste per day.\n",
            "\n",
            "Action: Calculator  \n",
            "Action Input: 14000000 * 2 * 7 / 1000  # (14 million people * 2 kg/day * 7 days) / 1000 to convert kg to tonnes  \u001b[0m\u001b[33;1m\u001b[1;3mAnswer: 196000.0\u001b[0m\u001b[32;1m\u001b[1;3mI now know the final answer.  \n",
            "Final Answer: Tokyo would produce approximately 196,000 tonnes of waste in a week.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "Final answer: Tokyo would produce approximately 196,000 tonnes of waste in a week.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 8.4  Another agent example: compound question\n",
        "\n",
        "result2 = agent_executor.invoke({\n",
        "    \"input\": \"What is the population of Tokyo? \"\n",
        "             \"If each person produced 2 kg of waste per day, \"\n",
        "             \"how many tonnes of waste would Tokyo produce in a week?\"\n",
        "})\n",
        "\n",
        "print(f\"\\nFinal answer: {result2['output']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "580a488c",
      "metadata": {
        "id": "580a488c"
      },
      "source": [
        "The agent handles a compound question that requires both factual knowledge (Tokyo's population) and multi-step arithmetic (population $\\times$ 2 kg $\\times$ 7 days $\\div$ 1000 kg/tonne). The Calculator tool ensures the arithmetic is exact rather than relying on the LLM's unreliable mental math.\n",
        "\n",
        "**Agent limitations:**\n",
        "- **Cost:** Each reasoning step is a separate LLM call ($\\sim 3$-$8$ calls per question)\n",
        "- **Latency:** Multi-step reasoning takes $5$-$15$ seconds\n",
        "- **Reliability:** Agents can get stuck in loops or choose wrong tools. Always set `max_iterations`\n",
        "- **Security:** Tool calls (especially web search and code execution) need sandboxing\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15f952b6",
      "metadata": {
        "id": "15f952b6"
      },
      "source": [
        "## Summary and Key Takeaways\n",
        "\n",
        "This chapter traced the full arc of LLM application development:\n",
        "\n",
        "**Local LLMs** (Recipes 1--2) give you privacy and cost control. 4-bit quantization makes 7B-parameter models runnable on consumer GPUs, and instruction tuning transforms raw text generators into useful assistants.\n",
        "\n",
        "**LangChain** (Recipe 3) provides the composability framework. Chains pipe prompts, models, and parsers together declaratively, making complex workflows readable and maintainable.\n",
        "\n",
        "**RAG** (Recipe 4) is the single most important pattern: it grounds LLM responses in external, up-to-date knowledge, eliminating hallucination on factual questions.\n",
        "\n",
        "**Chatbots** (Recipe 5) add conversational memory via question contextualization, enabling natural multi-turn interactions.\n",
        "\n",
        "**Code and SQL generation** (Recipes 6--7) demonstrate LLMs as productivity tools that translate human intent into executable programs, with the critical caveat that generated code must always be verified.\n",
        "\n",
        "**Agents** (Recipe 8) represent the frontier: LLMs that can reason about *which* tools to use, observe results, and iterate toward an answer. The ReAct pattern enables compound reasoning that no single model call could achieve.\n",
        "\n",
        "The common thread: **LLMs are most powerful when combined with external tools and data.** The model provides reasoning and language generation; the tools provide grounding, precision, and access to current information."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}